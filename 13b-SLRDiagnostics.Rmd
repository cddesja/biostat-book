A previous lecture described how to use statistical inference methods for simple linear regression. What was missing from that lecture was how we evaluate the assumptions. Let’s learn how to check the assumptions of the model via diagnostics to help ensure that we can make valid inferences about the relationship using the simple linear regression model. 

The same example we used in previous lectures, Mercury Content in Fish and Water Acidity, will be used throughout this lecture.Checking assumptions is an important step when doing statistical inference because the estimates (point and interval) and hypothesis testing were developed assuming that the model is correct. If the model is incorrect, then we are at risk for making incorrect conclusions. 

As a review from a previous lecture, the assumptions for simple linear regression are:

The sample should be a random (or representative) sample from the population of interest, to allow us to generalize the results to that population; 

The data should show a linear trend between the X and Y variables. 

The observations should be independent of one another. 

The residuals should be nearly Normal in their distribution. 

The variability of the points around the best-fit line should be roughly constant. 

The remainder of this lecture will discuss the LINE assumptions (which stands for Linearity, Independence, Normality, Equal variance) in more detail.The first of the LINE assumptions is that Y and X have a linear relationship to each other, as shown in the linear regression model equation above. The last three assumptions, though, are all related to the underlying distribution of the errors as specified by the error term in the SLR model above. The simple linear regression model assumes that the random errors are independent values and come from a Normal distribution that has a mean of 0 and some fixed variability, denoted as sigma in the notation. Let’s unpack what we mean by this through an illustration (as presented on the plot above).
 
The plot indicates that Y is linearly related to X, as shown by the line [Linearity assumption]. It further shows that for any given value of X, the values of Y will not all lie exactly on the line: they will be spread out symmetrically above and below the line, with more of them close to the line and fewer of them further away from the line. The differences between the Y values and the line are called the errors. These errors are independent of one another [Independence assumption]. At any given X value, the errors are roughly Normally distributed with mean zero, as shown in the plot by the little Normal curves which are centered on the line [Normality assumption]. The error variance (sigma) is constant all across the X axis, as shown by the little Normal curves all having the same width [Equal variance assumption].

Since the assumptions relate to the population errors, many of the approaches to evaluating these assumptions involve examining the sample estimated errors, or residuals, via graphs. *Note: There was a typo in the transcript. While the transcript has been updated for Spring 2021 (bold and underlined text is the updated information), the audio recording still has the error. This will get corrected once we return back to working in the office.

First up, the linearity assumption. We can check this assumption by creating a scatterplot of the two numerical variables and evaluating the form, that is, assessing the trend of the points. Do the points appear to following a straight line or some other curve or pattern?

Let’s check this assumption using the four plots on the slide. Which plots appear to meet the assumption of linearity? 

If you said Plot A and Plot C, you would be correct. Plot A has a positive linear trend and Plot C has a negative linear trend. It seems reasonable to use a linear regression model to capture the relationship between X and Y.

Plots B and D display a curved relationship between X and Y. The trend is clearly increasing in Plot D but not in a linear way. And the trend is clearly decreasing in Plot B but not in a linear way. This assumption would not be met if we saw either of these scatterplots, and we would need to consider a different model to better capture the relationship between X and Y.Another way we can assess the linearity assumption is by using a residual plot. This plot graphs the residuals from the model (on the y-axis) against the fitted values (i.e., y-hat values) from the model (on the x-axis). 

To check this assumption via a residual plot, we look for random scatter of the points (the author of this lecture tends to think about it as a big ”blob”). If the points are randomly dispersed around the horizontal axis at 0, a linear regression model is appropriate for the data. If the points appear to trend in some way, a different model may be more appropriate.

Looking at the four plots on the slide, we can see that there is no obvious trend for the residual plots A and C, but there is an increasing curvature for residual plot B and a decreasing curvature for residual plot D. This is the same assessment we had made on the previous slide: the linearity assumption appears to be met for Plots A and C but not for B and D. 

It’s helpful to use both the scatterplot and the residual plot to evaluate linearity, because the form might be more pronounced in the residual plot if it’s hard to detect in a scatterplot.Let’s check the linearity assumption for the Mercury Content in Fish and Water Acidity example. The slide displays the scatterplot for the data as well as the residual plot. Using both plots, does it appear that a linear model is appropriate to represent the relationship between average mercury in fish and water acidity? 

The scatterplot shows a general decreasing trend, with no obvious curvature, and the residual plot displays random scatter above and below the horizontal axis, or a big “blob”. It appears that the linearity assumption is met for this example. The next assumption, independence, relates to the how the data were sampled and collected. The observations should be independent of one another. That is, whether one point is above or below the line does not influence whether another point is above or below the line. This assumption would be violated if the observations were related in some way to other observations: for example, if the participants were matched or paired, or if they were related in some way to each other (e.g., members of the same family). Another violation would occur if observations involved repeated measurements: for example, if the participants were measured multiple times on the same outcome (e.g., blood pressure). If the observations are not independent, then methods for correlated data should be used instead.

In the Mercury Content in Fish and Water Acidity example, it is reasonable to assume that the characteristics of one lake do not affect the characteristics of another lake, so we can assume that the 53 lakes are independent of one another.The normality assumption refers to the distribution of the errors or residuals. We can check this assumption by creating a histogram of the residuals and evaluating the shape of the distribution. Do the residuals appear to be approximately Normal or are they skewed?

Let’s check this assumption using the four plots on the slide. Density curves of a Normal distribution overlay the histograms so we can get a better sense, visually, about whether the distributions are approximately Normal. Which plots appear to meet the assumption of normality?

Plots A and D appear to be approximately Normal, while Plots B and C appear to have a right skew to them. If this assumption is not met, then other procedures for carrying out inference for simple linear regression (such as transformation or nonparametric methods) should be used instead. 

Histograms can be useful for identifying a highly asymmetric distribution, but they are not as useful in identifying normality specifically unless the sample size is relatively large. This is primarily due to the subjective nature of deciding the number of bins to have in the histogram. Luckily, there is another type of graph that can help us assess normality of the residuals.The plot that is generally used to evaluate the normality of data is called a QQ plot (also known as a normal probability plot). The idea behind the QQ plot is that if the data follow a Normal distribution, the theoretical percentiles of the Normal distribution and the observed percentiles from the data should be approximately equal. Recall that percentile refers to a value such that p% of the observations lie below that value. For example, in a Normal distribution, only 5% of the observations are below 1.64 standard deviations below the mean. If the percent of the observations in the data that are that far below the mean is much higher (or much lower) than this, then the data may not be normally distributed. For another example, in a Normal distribution, 50% of the observations are below the mean. If the percentage of observations in the data that are below the mean is much different than 50%, then the data may not be normally distributed. Similar statements can be made about all of the other percentiles.

If the data are normally distributed, then a plot of the sample percentiles vs. the theoretical percentiles will be linear. The more closely the points follow a linear pattern, the closer the distribution is to a Normal distribution. A diagonal line is often included in the plot as a visual aid to assess how well the points follow a linear trend. 

Because the data we are interested in for assessing normality is the sample residuals, we create a QQ plot of the residuals. Using the tips for how to evaluate QQ plots, which of the plots on the slide appear to follow a linear trend? Note: Try not to be too picky. Don’t focus too much on small departures from an ideal pattern. We are not looking for exact normality: we are looking for large departures from normality. Pay attention only when you see a consistent departure from the expected pattern. 

Based on the QQ plots on the slide, Plots A and D appear to generally follow a linear trend, which suggests that the residuals are approximately normally distributed. Plots B and C deviate quite a bit from the diagonal line at the top right end of the plot, which suggests some skewness to the distribution of the residuals. This is the same assessment we had made on the previous slide: the normality assumption appears to be met for plots A and D but not for B and C. Let’s evaluate the normality assumption for the Mercury Content in Fish and Water Acidity example. The slide displays the histogram of the residuals as well as the QQ plot of the residuals. Using both plots, does it appear that the residuals are approximately Normal? 

The histogram shows a slight right-skew of the residuals and there is slight curvature in the QQ plot. While the skew is slight, it is probably not severe enough to claim that the assumption is not met. We can conclude that the residuals are approximately Normal.The final assumption we need to evaluate is the equal variance assumption. We assess this assumption by (again) using a residual plot. In particular, we are looking to see if the residuals have consistent variability above and below the horizontal line at 0 as we move across the x-axis. We can think of this as the residuals forming a “horizontal or parallel band” around the 0 line. An example of unequal variance would be if a plot had either a “fanning” or “funneling” trend as we moved across the x-axis. A “fanning” trend would be if the residuals were close to 0 for small fitted values and were more spread out for large fitted values. A “funneling” trend would be the opposite of that; the residuals were spread out for small fitted values but were close to 0 for large fitted values. 

Let’s check this assumption using the four plots on the slide. Which plots appear to meet the assumption of equal variance? 

The residual plot for C appears to be the only one that satisfies the equal variance assumption. There doesn’t appear to be any trend in the residuals, and they appear to be evenly distributed above and below the horizonal line at 0. 

We already know (from an earlier slide) that plots B and D have curvature trends, which makes it difficult to also assess the equal variance assumption. However, the residual plot for A appears to have some fanning going on (that is, the points appear to be a bit more spread out vertically at the right side of the plot than at the left side). So the equal variance assumption does not appear to be met for Plot A. If this assumption is not met, then other procedures for carrying out inference for simple linear regression (such as transformation of X and/or Y data values or weighted least squares linear regression) should be used instead. 

If we put all of the evaluations of the assumptions together, it appears that none of these cases meets all of the assumptions needed for linear regression. B and D fail the linearity assumption, B and C do not appear to have normally distributed residuals, and A does not appear to have equal variances.How does the variability of the residuals appear for the Mercury Content in Fish and Water Acidity example? 

The residuals roughly form a horizontal band around the 0 line. There is no obvious fanning or funneling so it is reasonable to assume that the equal variance assumption is met for this example. 

Putting it all together, for the Mercury Content in Fish and Water Acidity example, all of the assumptions for linear regression appear to be met, so using a SLR model to determine the relationship between average mercury content in fish and water acidity is appropriate. You might be thinking at this point in the lecture that evaluation of the assumptions using graphical diagnostics seems subjective, and you would be right. With time and experience, you will get better at this kind of evaluation. Keep in mind, though, that linear regression is what is called “robust” to small deviations from the assumptions, meaning that the results will be valid as long as the data are “pretty close” to meeting the assumptions. When we check the assumptions, we are not looking for tiny little differences; we are looking for major deviations. Don’t focus on any one or two points. Humans love to try to find patterns in randomness when there isn’t one. Think of it this way: It’s like looking up at the clouds in the sky1. If you look long enough, you start to see images of animals in the random scatter of the clouds. Try to not do this when evaluating the regression diagnostics. Pay attention only when you see a consistent departure from the expected pattern. 

A topic that is not covered in this course is checking for outliers or influential points. Outliers are points that fall far from the cloud or trend of points. They are important to consider because they may have strong influence on the position of the simple linear regression line. There are many measures that have been created to assess which points are potential outliers, or potential influential points, such as Cook’s distance and studentized residuals. These will be covered next semester, in PubH 6451. 

A very common misconception is that the linear regression assumptions are about the predictor variable(s). This is a friendly reminder that the assumptions are about the errors (or the residuals) and not the predictor variables themselves. If the X variable is non-normal and we fit a simple linear regression model, that is totally ok. (The X variable does not even need to be continuous: it could be categorical or even binary. We will see this more in multiple linear regression.) Only the residuals need to be approximately Normal.

Lastly, some violations of the model assumptions are more important than others. Regression inferential methods (i.e., hypothesis tests and confidence intervals) are sensitive to departures from independence and to moderate (but not too tiny) departures from equal variance. But they are fairly robust to departures from normality.

Reference: 
Adapted from https://onlinecourses.science.psu.edu/stat501/book/export/html/910.
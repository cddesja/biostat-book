Thus far in this course, we have discussed statistical methods for comparing means of a numerical variable between two or more groups (using t-tests, confidence intervals, or ANOVA) and for comparing proportions of a binary variable between two groups (using confidence intervals for relative risks or odds ratios). This week, we will discuss hypothesis tests that can be used for comparing proportions of a binary variable between two groups, but which can also be used more generally for comparing any two categorical variables (binary or not) to determine whether or not they are associated. This lecture will focus on a common/popular test for comparing two categorical variables, Pearson’s Chi-square test. Let’s start with an example. A randomized controlled trial was carried out to determine whether a handheld transcranial magnetic stimulation (TMS) device was more effective at treating migraine headaches than a sham control device that did not deliver any stimulation1. Two-hundred adults who suffered from migraine headaches were randomly assigned to use the TMS device or the sham device when they felt a migraine coming on. Two hours later, the participants were asked to record their pain status. 

In this study, there are two binary categorical variables: treatment group (TMS or control) and pain status after two hours (pain-free or not). Similar to a previous lecture, the data can be summarized using a two by two (2x2) table or contingency table, as shown above. 

In this study, 39 out of the 100 participants in the TMS group were pain-free after two hours (or 39% of the TMS group were pain-free). (Recall that this type of calculation is called conditional proportion or conditional probability. You may sometimes hear statisticians use language like “Conditional on treatment group, the probability of being pain-free after two hours is 39%.”) In contrast, 22 out of the 100 in the control group were pain free after two hours (or 22% of the control group were pain free) 

We can see that in this particular study, the chance of being pain-free after two hours is higher in the TMS group (39%) than in the control group (22%).

But, as usual, what we want to know is: Could this large of a difference in pain relief between the two study groups be the result of sampling variability or is this evidence of a real difference? Let’s use statistical inference methods to answer this question.

Reference: 
1Lipton, R.B., et al (2010). Single-pulse transcranial magnetic stimulation for acute treatment of Migraine with aura: A randomized, double-blind, parallel-group, sham-controlled trial, The Lancet Neurology, 9(4), 373-380.
Dataset: Cannon, A.R., Cobb, G.W., Hartlaub, B.A., Legler, J.M., Lock, R.H., Moore, T.L., Rossman, A.J., & Witmer, J.A. (2013) Stat2: Building models for a world of data. New York, NY: W. H. Freeman and Company. (p. 464)
For 2x2 tables, such as those seen thus far, there are a number of analysis approaches that could be used. 

We could frame our research question in terms of relative risk and use the methods for relative risk that were presented in a previous lecture. For example, is the risk of being pain-free different between the TMS group and control group? Or we could formulate the question in terms of odds ratios and use the methods for odds ratios that were presented in a previous lecture (although using odds ratios for anything other than a case-control study doesn’t really make sense). 

In this lecture, we will frame the question as one of association or relationship between variables. For example, “is there an association between study group and pain status OR are the two variables independent of one another?” If there were no association between the variables (i.e., if they were independent), then the probability of being pain-free would not depend on study group. In contrast, if there were an association between the variables, then the probability of being pain-free would depend on study group. 

The advantage of framing the question this way is that it is equally applicable to 2x2 tables, or to 2x3 tables, or to tables of any arbitrary size. 

The limitation of framing the question this way is that it only addresses whether the two variables are associated with each other. It does not address how they are associated, that is, it does not provide an effect direction or size (such as a relative risk or risk difference).

We will discuss two tests that can be used to address the question of whether two categorical variables are associated with each other: Pearson’s Chi-square test (this lecture) and Fisher’s exact test (future lecture). As with any study, we begin with the research question. 

The research question for the TMS and Migraine Pain study is: Is study group (TMS vs. control) associated with migraine headache pain status (pain-free or not) after two hours? Before we can carry out any inferential method, we first need to evaluate the assumptions of the method. The assumptions for Pearson’s Chi-square test are: 

The sample should be a random (or representative) sample from the population, to allow us to generalize the results to that population; 

The observations should be independent of one another. This assumption would be violated if the participants in the study were related in some way to each other (e.g. siblings) or if the observations in one group were related in some way to those in another group, for example, if the participants in the groups were matched or paired. If the observations are paired, then McNemar’s test should be used instead.

In previous lectures, we checked to make sure the sample was “large enough” for the sampling distribution to be approximately Normal. A similar approach is employed to safely utilize the Chi-square test. We have to check to make sure the sample is “large enough” by making sure the expected number of observations in each cell of the table is at least 5 (more to come on expected counts on the next slide). If the sample size is too small, consider using other methods, such as Fisher’s exact test or continuity correction methods, to carry out the hypothesis test. 

If these assumptions are not met, then the results of the test will not be valid. What are expected counts? They are the counts we would expect to see assuming the null hypothesis (of no association) is true. They are computed to reflect identical percent distributions of the “event” for each group that are equal to the overall percent distribution of the “event”. The overall percent for a row or for a column is called a marginal proportion (or a marginal probability) and is found by taking the total for the row or for the column and dividing by the total sample size of the study. [Note: They are called “marginal” because they are located around the edges, or “margins”, of the contingency table.] 

For example, if the overall proportion who have the event is (a+c)/n (suppose this value is 0.28), then we would expect 28% of Group 1 and 28% of Group 2 to have the event. So we would multiply the overall proportion by the sample size of each group to get the expected count of those with the event. We would use a similar method for finding the expected count of those without the event for each group by using the overall proportion who didn’t have the event. 

So when dealing with testing for an association between two groups, we distinguish between observed counts, which are the counts we observed in our sample (a, b, c, and d), and expected counts, which are the counts we would expect if the two variables were independent of one another. In the 2x2 table above, the middle cells display the observed counts in red letters and the formula for calculating the expected counts in the black letters. 

In general, we find the expected counts for the cells of a 2x2 table by calculating the (row total * column total) / total sample size (n).Let’s find the expected counts for the TMS and Migraine Pain study. 

The overall proportion of those who were pain free after two hours was 0.305, or 30.5%, in the sample. If the two variables were independent, we would expect 30.5% of the TMS group and 30.5% of the control group to be pain free. So of the 100 in the TMS group, 0.305*100 = 30.5 of them would be expected to be pain free (assuming the null is true). A similar calculation for the control group yields the same result (because the two groups have equal sample sizes). 

Similarly, we use 0.695 to find the expected counts by group for those who were not pain free. Let’s check the assumptions for the TMS and Migraine Pain example. 

Are the participants a random or representative sample of all adults who suffer from migraines? We don’t have enough information to know for certain if the participants were randomly selected from the population, but it seems reasonable to assume that the researchers would likely have chosen the participants to be as representative as possible of all adult migraine-suffers.

Are the observations independent of one another? Again, we don’t have enough information to know for sure, but for now, it isn’t unreasonable to assume that the participants in each group are independent of each other and that the participants from one group are independent from participants in the other group. (Usually if there was some kind of matching done, it would have been mentioned in the study description.)

Lastly, are all of the expected cell counts greater than 5? Based on the calculations in the previous slide, yes, the sample size condition is met to safely apply the Chi-square distribution for this situation. 

References:
Image: https://www.pinclipart.com/pindetail/owRJbh_file-mw-icon-checkmark-svg-creative-commons-check/The next step is to define the hypotheses for the Chi-square test in terms of the population. Hypotheses when testing an association between two categorical variables tend to be expressed in words rather than through parameter notation (such as those in previous topics). 

Recall that the null hypothesis defines the skeptical perspective or the “nothing is going on” situation. When we are looking to see if two variables are associated, the skeptical perspective is that they aren’t associated at all; they are not related in any way; they are independent of one another.

The alternative hypothesis, on the other hand, defines the competing claim, the thing we are interested in finding evidence for. In this case, the alternative hypothesis is that the two variables are indeed associated with or related to each other and therefore, are not independent of one another.Let’s define the hypotheses for the TMS and Migraine Pain example. 

The null hypothesis is that the two variables of interest, pain status (pain-free or not) and study group (TMS or sham control), are independent of each other in the population of migraine sufferers.

The alternative hypothesis is that the two variables of interest, pain status (pain-free or not) and study group (TMS or sham control), are NOT independent of each other in the population of migraine sufferers;  that is, that there is an association or relationship of some kind between them.We evaluate the claims in the usual way: 
We collect evidence (data), 
We summarize the data using exploratory data analysis (a contingency table and perhaps marginal and conditional probabilities), and
We calculate a test statistic to measure the compatibility between the data and the null hypothesis. 

The test statistic for comparing two categorical variables (given that the assumptions are met) is the Chi-square statistic (pronounced kai, rhymes with eye). It quantifies the discrepancy between the observed counts from a sample and the expected counts based on the null hypothesis, combining the information from all cells of the table. To put it in words, for each cell in the table, it computes the difference between the observed and expected counts, squares that difference, and divides that squared difference by the expected count. For a 2x2 table, there would be four of these values. Then, to get the Chi-square statistic, we would add up all of those ratio values from each of the cells. This test statistic has a Chi-square distribution with degrees of freedom that is determined by the size of the contingency table: (r-1)*(c-1), where r is the number of rows in the table and c is the number of columns in the table. For a 2x2 table, there are two rows and two columns, so the appropriate degrees of freedom would be (2-1)*(2-1) = 1. 

Let’s look at each piece of the Chi-square statistic. If there is a large discrepancy between the observed count and the expected count, the difference in the numerator will be large. This difference is squared to prevent large positive deviations from cancelling large negative deviations when they are summed. Finally, this squared difference is divided by the expected count for that cell as a way of standardizing each cell’s value. Note that large values of the Chi-square statistic correspond to samples that do not agree with the null hypothesis. What does a Chi-square distribution with parameter df look like? 

Since the Chi-square statistic can only take on positive values in both the numerator and denominator, the Chi-square distribution can only take on positive values. Three different Chi-square distributions, for 1, 2 and 4 degrees of freedom, are shown in the figure above. Notice that the Chi-square distribution is not symmetric, but rather is positively skewed. The shape of the Chi-square distribution varies depending on the degrees of freedom. As the degrees of freedom increase, the Chi-square distribution shifts to the right and becomes less peaked.

For a 2x2 table, the degrees of freedom are (2 – 1) times (2 – 1), which equals one, so the distribution with df = 1 is the appropriate one to use. This is shown in blue on the plot above.

For a 3x2 table, the degrees of freedom would be (3-1) times (2-1), which equals two, so the distribution with df = 2 would be the appropriate one to use. This is shown in pink on the plot above.For the TMS and Migraine Pain example, data were collected and summarized via tables earlier in this presentation. We noticed that the conditional proportions for each group were different and that the TMS group had a higher proportion that were pain free after two hours compared to the control group. But do these results provide evidence that there is an association between pain status and study group? 

To answer this question, we compute a Chi-square test statistic using the observed and expected counts. Adding up all of the computed values for each cell gives a Chi-square statistic of 6.817. This test statistic has a Chi-square distribution with 1 degree of freedom. If the null hypothesis were really true and there was no association between the two variables, then the sampling distribution of the Chi-square statistic would follow a Chi-square distribution with 1 degree of freedom, as shown in the plot above. The calculated Chi-square test statistic for the TMS and Migraine Pain study is 6.817. Our test statistic lies in the right tail of this distribution. This value seems fairly unlikely to occur, but how likely or unlikely is it? Let’s quantify this probability. Recall that we quantify how unusual the evidence is compared to what is assumed to be true by computing a p-value. The p-value is the probability that you would obtain a sample difference this “unusual” if the null hypothesis were really true and any observed difference was simply due to sampling variability. To put it another way, it’s the probability of getting our sample result (or one even more extreme) if the null hypothesis were true. As before, what counts as “extreme” depends on the alternative hypothesis. The smaller the p-value, the less consistent or compatible the data are with the null hypothesis. 

Once we have the p-value, we make a conclusion about the strength of evidence we have against the claim by comparing the p-value to the significance level, alpha. If the p-value is less than alpha, we reject the null hypothesis and say we have evidence against the null in favor of the alternative hypothesis. Another way of saying this is that the result is “statistically significant”. Alternatively, if the p-value is greater than alpha, then we do not reject the null hypothesis and we say we lack evidence against the null. This result would not be considered statistically significant. In either case, remember to always state the conclusion in the context of the problem. Let’s evaluate the evidence for the TMS and Migraine Pain example. 

With a Chi-square test statistic of 6.817, and an inherently one-sided test, we are interested in the probability of seeing a test statistic of 6.817 or more “extreme” if the null hypothesis were really true. Therefore, the probability we are looking for is the area in the upper tail of the Chi-square distribution, above the value 6.817. The p-value for this example is 0.009. For a significance level (alpha) set to 0.05, we reject the null hypothesis because the p-value of 0.009 is less than 0.05. Therefore we conclude that there is evidence that pain status (pain free or not) after two hours is associated with study group (TMS or control) in this population of migraine sufferers.

Since we have rejected the null hypothesis, we could go on to say that more participants in the TMS group experienced pain relief after two hours than in the control group (39% vs. 22%). If we wanted an interval estimate for a “plausible range of values for how much more”, we could calculate a confidence interval for the relative risk or the risk difference. Here are a few additional notes about the Chi-square test. 

Although this lecture used a 2 x 2 table, the Chi-square test can be carried out for any r x c table, where r is the number of rows and c is the number of columns. For example, if we had a 3 x 2 table, then there would be 6 values to add up to get the Chi-square statistic, and this Chi-square statistic would have a Chi-square distribution with (3-1)*(2-1) = 2 degrees of freedom. 

As previously mentioned in this lecture, testing for an association, as is done in using the Chi-square test, only helps us answer the question of whether the two variables are associated with each other. It does not address how they are associated, that is, it does not provide an effect direction or size (such as a relative risk or risk difference). We would need to calculate these statistics and their confidence interval to answer the question of how.

It is recommended to not use Chi-square tests for small samples. The typical rule that is that the Chi-square test is acceptable as long as no cell in the table has an expected count less than 1.0 and not more than 20% of cells have expected counts less than 5.0. If this is not met, then we need to use an exact test (to be presented in a future lecture). 

We discussed the Pearson’s Chi-square statistic in this lecture, but there is an ”upgraded” statistic related to the Chi-square statistic called Yate’s continuity correction statistic. [Note: This is being mentioned here only because you might see it within your software.] It tries to correct for bias that can result from using the Chi-square test on a 2x2 table with small samples by introducing a more conservative statistic. However, there is no consensus on whether this correction is needed, and it has been debated for decades. If you do come across a small sample situation, we recommend that you use Fisher’s exact test instead.


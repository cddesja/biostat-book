[["index.html", "Biostatistics (DRAFT) Chapter 1 Introduction 1.1 Cycle of Research", " Biostatistics (DRAFT) Christopher Desjardins, Laura Le, and Ann Brearley 2022-01-27 Chapter 1 Introduction What is biostatistics? How does it fit within the larger field of research? What will you learn from this course? These are the questions that will be addressed in this lecture. Let’s begin with the question “What is statistics?”. There are many definitions, but they all have similar themes. For many people (maybe even you), statistics is seen as a branch of mathematics. However, John Tukey, a famous statistician in the mid-1900’s, stated that “statistics is a science, not a branch of mathematics, but uses mathematical models as essential tools.” Just as engineering, chemistry, and economics use math as a tool but are seen as separate fields from math, so should statistics be distinguished from mathematics. Alan Agresti and Christine Franklin have described statistics as “the art and science of learning from data” (this is also the title of their introductory statistics textbook). The goal of statistics is to translate data into understanding of the real world. At the heart of it all is the word data. Examples of areas in which statistics is used include business, climate change research, manufacturing quality control, government policy, education, finance, and even sports (have you seen the movie Moneyball?). 1.1 Cycle of Research Medical or public health research, like all scientific research, begins with a research question about a population of interest. Biostatistical methods are used to help answer this research question. Let’s describe this process using the Cycle of Research diagram shown here. 1.1.1 Step 1: Population to Sample The scientific Cycle of Research begins with a population. A population is the complete set of individuals who share some common characteristics. The population is the group of people we are interested in learning about. The population of interest in a given study may be U.S. adult males, or it may be Finnish preschoolers, or it may be HIV-positive adults in South Africa. Perhaps we are interested, for example, in knowing how tall ten-year-old American boys are. The population in this case would be all ten-year-old boys in the U.S. When we want to answer a specific question about a particular population, typically we obtain a sample. A sample is a subset of the population of interest. The sample is the group of people we actually study. This is the group of people from whom data are collected. In our example, we might randomly sample 1,000 ten-year-old boys from the U.S. population to serve as our study’s sample. You may wonder, if the goal is to make some conclusions about the population, why not just collect data on the entire population? Samples are often studied instead of entire populations for several reasons. One reason is cost. Often it would be too expensive and/or too time-consuming to collect data from an entire population of interest. For example, perhaps we are interested in the blood pressures of residents of Massachusetts. It would be much too expensive to obtain blood pressure measurements from every resident of the state, so we obtain a sample of residents instead. Take a careful look at this figure. Note that the samples have different numbers of people in them, and different mean (or average) diastolic blood pressure values. Also note that none of the mean values for the samples are the same as the mean blood pressure for the population. We will explore these concepts later. Another reason for using a sample instead of the population is feasibility. Studying the entire population may simply not be possible. For example, it is not possible to determine the effect of a new treatment on an entire population of patients if the treatment is not yet available to the entire population. A third reason is accuracy. Measurements from a study’s sample may be more accurate than measurements from a population. More time and effort can be devoted to carrying out the measurements for the smaller number of people in the sample than is feasible for an entire population. We also can’t measure everyone in the population at the same time in the same way. Measurements may change over time (as we have all seen with our weight). If we have to measure a lot of participants, we may need multiple people or instruments to do it, which can also decrease accuracy. Finally, it may be unethical to study the entire population. If you’re administering a treatment that might be harmful or withholding a treatment that might be beneficial. Step 1 in the Cycle of Research is to design a study in order to obtain a suitable sample from the population or populations of interest. There are a number of ways doing this. These will be discussed in more detail later in this book. 1.1.2 Step 2: Sample to Statistic Once we have a sample, Step 2 in the Cycle of Research is to use the sample data to calculate statistics. A statistic is any number calculated from the sample data. The sample mean is a statistic. The sample variance is another statistic. In our example, we might measure the height of each boy in our study’s sample of 1,000 ten-year-old American boys and then calculate the average (or mean) height. That average height is a statistic. 1.1.3 Step 3: Statistic to Parameter Step 3 in the Cycle of Research is to use sample statistics to estimate population parameters. A parameter is a variable which describes some aspect of the population, but whose true value is unknown. The population mean is a parameter. The population variance is another parameter. In our example, the population parameter of interest is the average height of ALL ten-year-old boys in the U.S. The true value of this parameter is unknown because it is not possible to measure the heights of all ten-year-old American boys at once. Sample statistics can be used as estimates of the unknown population parameters. The sample mean is a good estimate of the population mean. The sample variance is a good estimate of the population variance. In our example, the mean height of the 1,000 ten-year-old boys in our study’s sample is an estimate of the true mean height of ALL ten-year-old boys in the United States. 1.1.4 Step 4: Parameter to Population Step 4 in the Cycle of Research is to use the population parameter estimates we have obtained to infer something about the population we are interested in. Statistical inference is the process of drawing conclusions about a population on the basis of observations from a sample. In our example, we might want to know if ten-year-old American boys are taller now than they were fifty years ago. We could use the estimated true mean height of all ten-year-old American boys we obtained from our sample average in Step 3 and compare it to the value from fifty years ago. If the values were different, we could ask whether that difference is large enough to be statistically significant. Each time a researcher poses a scientific question and the investigation is deemed worthwhile and feasible, the scientific Cycle of Research begins again. The goal of this course is to develop your ability to do four things: To understand the principles behind basic statistical analyses; To choose appropriate statistical analyses for a given scientific context; To carry out statistical analyses using R or SAS and understand the resulting output; and To read and interpret statistical results in the literature of your field of interest. The topics for the course will run from descriptive analyses to inferential analyses, such as confidence intervals, hypothesis tests, and linear regression. "],["variable-types.html", "Chapter 2 Variable Types 2.1 Scales of Measurement 2.2 Summarizing Data", " Chapter 2 Variable Types 2.1 Scales of Measurement There are two broad types of measurement scales: categorical and numerical. Categorical scales are qualitative. Examples of variables measured on a categorical scale include sex, blood type, race, or disease status. The data at an individual level consist of what category that individual falls into (for example: male or female). The data at the study level are the number of study participants in each category (for example: how many men and how many women were included in the study). Numerical scales are quantitative. Examples of variables measured on a numerical scale include weight, height, survival time, or blood pressure. Variables measured on a numerical scale will often have measurement units associated with them. For example, a person’s age might be measured in years, or in months, or, for newborns, in weeks or even days. On a numerical scale, the differences between numbers have meaning. The difference between 32 years and 33 years of age has the same meaning as the difference between 85 years and 86 years of age. All variables measured on numerical scales are therefore “interval variables”. 2.1.1 Categorical Scales Categorical measurement scales can be further divided into nominal scales and ordinal scales. Nominal scales have two or more categories with no natural ordering. For example, religious affiliation could be described on nominal scale using the categories of Catholic, Protestant, Jewish, Muslim, Hindu, Buddhist, Other. The order of listing the categories is irrelevant. Examples of variables measured on a nominal scale include sex, race, blood type, or marital status. A binary scale is a special case of a nominal scale, in which there are only two categories, such as “Male”/”Female” or “Child”/”Adult”. The responses to any question with a “Yes”/”No” answer are measured on a binary scale. Common binary scale measurements in medicine and public health include disease status (“Has the disease”, “Does not have the disease”) and diagnostic test result (“positive”, “negative”). (CHRIS - I don’t know if I buy that binary scales are a special case of a nominal scale. Your examples are ordinal). Ordinal scales have two or more categories that do have a natural ordering. Examples of variables measured on an ordinal scale include Apgar score, tumor stage, Likert-type scale survey questions, or social class. On an ordinal scale, numerical assignments are relative and do not represent any interval relationship between categories. For example, Apgar scores for newborn infants range from 0 to 10, and higher scores indicate better functioning, but the difference between scores of 8 and 9 may not have the same implications as the difference between scores of 2 and 3. A given variable can be measured using a number of different scales. The measurement scale that is used determines what type of variable it is. For example, smoking status could be measured on a binary scale, as “Smoker”/”Non-smoker”. It could be measured on a nominal scale, as “Ex-smoker”, “Current smoker”, “Never smoked”. It could even be measured on an ordinal scale, such as “Never smoked”, “Quit smoking &gt;10 years ago”, “Quit smoking 1-10 years ago”, “Quit smoking within the last year”, “Current smoker”. Smoking status could even be measured on a numeric scale by measuring the number of years a person smoked, or the number of years since they quit smoking. The type of variable, in turn, determines which summary, plotting, and analysis methods are appropriate. Categorical data is typically summarized in tables using the numbers (or counts or frequencies) and proportions (or percentages) of study participants in each category. Appropriate plots for categorical data include bar graphs and pie charts, which help the reader to visualize the number or proportion of study participants in each category. Summary statistics and plots are described in more detail later. We will explore a number of statistical analysis methods that are appropriate for categorical data in this book. These include the use of confidence intervals for estimating a single proportion, methods for comparing proportions in two groups, including confidence intervals for relative risks or odds ratios, and methods for comparing proportions in two or more groups, including Chi-square test, binomial exact test, or Fisher’s exact test. 2.1.2 Numerical Scales Numerical measurement scales can also be further divided, into continuous scales and discrete scales. Continuous scales describe characteristics that can take on any real number value, such as 98.7 or 52.63 or 0.014. Examples of variables that are measured using continuous scales include blood pressure, temperature, age, weight, or height. Discrete scales describe characteristics that can have only integer values. Examples of variables that are measured using discrete scales include the number of children in a family, the number of births in a year, or the number of accidents in a month. Continuous measurement scales can be further divided into interval scales and ratio scales. On an interval scale, the intervals or differences between values have the same meaning throughout the scale (as is true for any numeric scale), but the zero of the scale doesn’t mean “none” of the quantity of interest. For example, temperature is commonly measured on an interval scale (degrees C or degrees F). The difference between 32F and 33F has the same meaning as the difference between 97F and 98F; however, 0˚F does not mean “no temperature”. As a result, it would make no sense to compute or interpret ratios of temperatures. If the high temperature today in Phoenix, Arizona, is 80F and the high in Nome, Alaska, is 40F, it does not mean that Phoenix is twice as hot as Nome. On a ratio scale, both the differences between values and the ratios of values make sense. For example, weight is measured on a ratio scale. The difference between 10 kg and 11 kg has the same meaning as the difference between 35 kg and 36 kg. Furthermore, 0 kg means “no weight” so it makes sense to compute weight ratios. Something that weighs 50 kg is twice as heavy as something that weighs 25 kg. Continuous data is typically summarized in tables using means and standard deviations, or medians and ranges or inter-quartile ranges. If the variable is a ratio variable, reporting the coefficient of variation (CV) is also appropriate. Appropriate plots for continuous data include dot plots, box plots, histograms, and scatterplots (for comparing two continuous variables). Summary statistics and plots are described in more detail later. In this book, we will explore a number of statistical analysis methods that are appropriate for continuous data. We will cover the use of confidence intervals to estimate a mean, confidence intervals or t-tests to compare two means, and analysis of variance (ANOVA) to compare three or more means. We will cover using correlation and simple linear regression to explore relationships between two continuous variables. We will also learn how multiple linear regression is used to model the relationship between a continuous outcome and multiple possible predictors (including treatment or exposure), and to adjust for potential confounding factors. One important note: there are relationships between numerical and categorical scales that can sometimes make it tricky to determine what kind of measurement scale is being used. A variable measured on a numerical scale can be converted to a variable measured on a categorical scale. This may be called “categorizing” the variable. For example, age can be measured on a continuous numerical scale, in years. It can also be categorized by dividing the continuum of years into age group, such as “0-5 years”, “6-10 years”, “11-19 years”, “20-39 years”, “40-59 years”, “60-79 years”, and “80+ years”. Age group, in this case, is an ordinal categorical variable. Age could also be categorized by dividing the continuum into two groups, such as “child” and “adult”. In this case, age group is a binary nominal categorical variable. A variable measured on a categorical scale can be coded numerically. For example, the ordinal variable social class could be coded numerically as 0=Lower class, 1=Working class, 2=Lower middle class, 3=Upper middle class, 4=Upper class. It is important to remember that coding categorical data using numbers does not make the data numeric. The codes simply represent the names of the categories. The nominal variable religious affiliation could be coded numerically as 1=Catholic, 2=Protestant, 3=Jewish, 4=Muslim, 5=Hindu, 6=Buddhist, 7=Other. Remember that coding religious affiliation numerically does not make it into an ordinal variable; there is no inherent ordering to the religious affiliation categories. Let’s look at a real example. The Blood1 dataset in the Stat2Data package contains measurements of three variables related to blood pressure, taken on 500 adults. The variable SystolicBP is the systolic blood pressure measured for each person, in millimeters of mercury (mm Hg). The variable Smoke is the smoking status recorded for each person, labeled a “1” if the individual was a smoker or a “0” if the individual was not a smoker. The variable Overwt is the weight group recorded for each person, labeled a “0” if the individual was in the normal weight range, “1” if the individual was overweight, and 2 if the individual was obese. Table 2.1: First 10 participants in the Blood1 dataset SystolicBP Smoke Overwt 1 133 0 2 2 115 1 0 3 140 1 1 4 132 0 2 5 133 0 1 6 138 0 1 7 133 0 2 8 67 0 0 9 138 0 0 10 130 1 0 The data for the first ten participants in the Blood1 dataset are shown in Table 2.1. Let’s examine these variables one by one. The first variable listed is systolic blood pressure, SystolicBP. This variable is numeric because the values are numbers and have units of measurement. Blood pressure is typically reported to the nearest one millimeter of mercury, as here, but it is nevertheless a continuous variable, since you could have a blood pressure of 138.2 mmHg or 74.5 mmHg. Furthermore, blood pressure is measured on a ratio scale, since a blood pressure of zero mmHg means no pressure, and a blood pressure of 140 mmHg means there is twice as much pressure as a blood pressure of 70 mmHg. The SystolicBP variable in this dataset is therefore a numeric, continuous, ratio variable. The second variable listed is smoking status, Smoke. This variable is categorical, since it tells you which category the person falls into: smoker or non-smoker. The data happen to be coded here as “1” for smoker and “0” for non-smoker, but it is not numeric data. The data could just as well have been coded “Y” for smoker and “N” for nonsmoker, or even as the words “smoker” and “nonsmoker”. This variable is not ordinal. While you may feel that not smoking is better than smoking, there’s no inherent ordering to the two categories. Finally, this variable is binary, since it has only two categories, smoker and non-smoker. The Smoke variable in this dataset is therefore a categorical, binary variable. The third variable listed is weight status, Overwt. This variable is categorical, since it tells you which category the person falls into: normal weight, overweight, or obese. The data happen to be coded here using the numbers “0” for normal, “1” for overweight, and “2” for obese, but they could just as well have been coded “1”, “2” and “3”, or “10”, “50”, and “300”, or “A”, “B”, and “C”, or even as the words, “normal”, “overweight”, and “obese”. This variable is ordinal, since there is an inherent ordering to the categories that matches the numbers: overweight feels like it’s “in between” normal and obese, just as “1” is between “0” and “2”. Note that there is an order to the categories, but not a magnitude: there is no sense that obese (coded “2”) is twice as much weight as overweight (coded “1”); and there is no sense that normal (coded “0”) means “no weight”. This variable is not binary, though, since there are more than two categories. The Overwt variable in this dataset is therefore a categorical, ordinal variable. One further note about the Overwt variable. It is likely that this variable was derived from a body mass index (BMI) measurement by choosing cut-points: for example, perhaps normal was set as BMI values of 24.9 or below, overweight as BMI values of 25.0 to 29.9 and obese as BMI values of 30.0 or more. This is an example of “categorizing” an underlying numeric variable. 2.2 Summarizing Data 2.2.1 Categorical Data Recall that categorical variables are variables that are qualitative, and can be either nominal or ordinal. In the Blood Pressure example, there were two variables in the dataset that were categorical: Overwt and Smoke. We summarize categorical data by: The number of observations in each category, which we find by simply tallying the number of observations in that category. The proportion of observations in each category, which we find my taking the number in each category and dividing by the total number of observations in the study. These proportions will be a value between 0 and 1. The percentage of observations in each category, which is the proportion multiplied by 100, followed by a percentage sign. While not required, it often helps to summarize the data in a table. Let’s go through an example to see how these summary measures work, using the smoking status variable from the Blood Pressure example. Of the 500 participants, 266 were smokers and 234 were nonsmokers. In this dataset, we see that there are more smokers than nonsmokers. It is appropriate to compare two numbers, or counts, when the denominator is the same or very similar. But, for example, if we were comparing the number of smokers between males and females and the total number of males and females was different (for example, 100 males vs. 50 females), it would not be appropriate to compare the counts of smokers between males and females. Rather, it is more appropriate to compare groups using proportions and percentages. In the Blood Pressure dataset, the proportion of smokers is 0.532, or 53.2% of the individuals are smokers, and the proportion of nonsmokers is 0.468, or 46.8% of the individuals are not smokers. (Note: Percentages are more common in reports and articles, and may be reported without the percentage sign next to the number. If this happens, authors will often denote somewhere in the table that the number represents a percent.) Pie charts are one way that we can graphically represent categorical data. Pie charts allow us to visualize the differences between proportions in the various categories. To create the pie chart, we need to know the proportion or percentage in each category. In the Blood Pressure example, we saw that there were about as many smokers as nonsmokers, with slightly more smokers. We can visually see that in the pie chart as well; it looks like the “smoker” piece of the pie has more than the “nonsmoker” piece. However, it’s not exactly clear how much more. ## Insert pie chart Because of this issue with pie charts (the difficulty in comparing slices and estimating percentages), statisticians tend to prefer other graphical methods for categorical data.(CHRIS - There’s perception studies about this) A better graphical summary for categorical data (and the one preferred by many statisticians) is the bar plot. The bar plot can be used with any summary measure: number, proportion, or percentage. Bar plots are often arranged so the categories are on the x- (horizontal) axis and the summary measure is on the y- (vertical) axis; however, the opposite arrangement can be used as well (i.e., categories on y-axis and summary measure on x-axis). As with the pie chart, we can see which categories contain more participants and which contain fewer: the taller bars occur for the categories with more participants (or a higher proportion) and the shorter bars indicate the categories with fewer participants. Unlike the pie chart, the y-axis tells us the number or proportion for each category. As a result, we get both the visual comparison and the actual value. As we did with the pie chart, we can visually see that there are more smokers than nonsmokers. However, we can also see the approximate numbers in each category: there are about 30 more smokers than nonsmokers. library(ggplot2) Blood1$smoke.f &lt;- ifelse(Blood1$Smoke == 0, &quot;Nonsmoker&quot;, &quot;Smoker&quot;) Blood1 |&gt; ggplot(aes(smoke.f)) + geom_bar(fill = &quot;blue&quot;, alpha = .5, col = &quot;blue&quot;) + xlab(&quot;&quot;) + ylab(&quot;Count&quot;) + theme_bw() + theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank()) + scale_y_continuous(breaks = seq(0, 260, 50)) 2.2.2 Numerical Data There are many different ways to summarize numerical data. The most common summaries are measures of center (such as means or medians) and measures of spread (such as standard deviations or interquartile ranges). In addition, we typically want to note any extreme or unusual observations in the data. If we wanted to pick one number to represent a set of data, we’d probably want to pick the value that indicates the center or central tendency of the data values, or the one that is a “typical” or average value. However, we know that there will (most likely) be variability in our data. That is, not all of the values will equal each other. So, the measure of center should not be our only summary measure for numerical data. We are also interested in how spread out, or how varied, our data values are from one another. We do this by computing measures of spread. There are other summary measures that we may compute as well to understand our data, such as percentiles or the minimum or maximum. Finally, we want to note if there are any observations that are very different or unusual from all of the other observations in the sample. 2.2.2.1 Measures of Center 2.2.2.1.1 Mean Some measures of center use the values of the variable. The individual values in the sample are denoted as \\(x\\) with a subscript \\(i\\), where \\(i\\) runs from 1 to the number of observations in our data. For example, the first observation is \\(x_1\\), the second observation is \\(x_2\\), and so on. The mean (or average) value, noted as \\(\\bar{x}\\) (pronounced x-bar), is the result of adding up all the individual values, \\(x_i\\), and then dividing by the number of observations in our sample, denoted by \\(n\\). \\[ \\bar{x} = \\frac{\\Sigma x_i}{n} \\] In our Blood Pressure example, when we add up the blood pressure measurements for all 500 adults and divide by 500, we get a mean value of 145 mm Hg. Note that the mean is calculated using all values in the dataset. If there is one observation or several observations that are much higher or lower than the rest of the data, it will raise or lower the mean more than it would if those observations were similar to the rest of the data. 2.2.2.1.2 Median Sometimes, it makes more sense to summarize the data by the order as opposed to by the value. Why? Because sometimes there are data points that are so different from the rest that they overly influence the mean. For example, let’s say our data points were 1, 2, 3, 4, and 100. If we took the average of these 5 data points, we would get the value 22. But does 22 seem to be typical? Or the center value? It is a lot higher than four of the values and much much lower than the fifth. It doesn’t seem to represent any portion of the data very well. One of the features of numerical data is that you can put it in order from smallest to largest. Let’s take advantage of this additional feature of numerical data to create different summaries for the typical value and the spread of the data. When we order the observations from smallest to largest, there will be a middle value. The median is the value of the middle observation in the dataset. (If there are an even number of observations in the dataset, then the median is the average of the two middle values.) The median is also referred to as the 50th percentile because 50% of the observations are below the median and 50% are above the median. For the Blood Pressure dataset, if we order the observations from smallest to largest, the middle observation would be between the 250\\(^{th}\\) and 251\\(^{st}\\) observations (because there are an even number, 500, of observations in this dataset). When we average these two values, we get a median of 140.5 mmHg, which is similar, but not the same, as what we calculated for the mean. Note that the median is calculated using only the middle value or values. If one observation in the data is much higher or lower than the rest of the data, it will be part of the order of the data, but the value itself is not used to find the median. Therefore, observations that are very different (either higher or lower) than most of the data do not affect the value of the median. 2.2.2.1.3 Other Measures of Center Other measures of center that are encountered in the medince and public health include the trimmed mean and the geometric mean. To calculate a trimmed mean, a specified percentage of the highest and lowest values for that variable are excluded before calculation of the mean. For example, the 10% trimmed mean would exclude the highest 10% and lowest 10% of the observations and then a mean would be calculated based on the included observations (the middle 80% of the data). This makes the trimmed mean less sensitive to those observations that are much higher or lower than the rest of the data. To calculate the geometric mean, the data are first log-transformed, then the mean is calculated on the log scale, and lastly, the mean is transformed back to the original scale. The geometric mean can be useful for summarizing highly skewed data. 2.2.2.2 Measures of Spread 2.2.2.2.1 Variance and Standard Deviation One common measure of the spread of numerical data is the variance (or the related measure, the standard deviation). \\[\\text{Variance}: s^2 = \\frac{\\Sigma(x_i - \\bar{x})^2}{n - 1}\\] The variance, denoted \\(s^2\\), is calculated in several steps. The first step is taking the actual value for each observation, denoted \\(x_i\\), as before, and subtracting the average value, \\(\\bar{x}\\). This difference is called the deviation, \\(x_i – \\bar{x}\\). Values that are far from the mean will give us large deviations, while values close to the mean will give us small deviations. Values that are below the mean will give us negative deviations, while values above the mean will give us positive deviations. However, we need to pull all these deviations into one number. Note that, because we are subtracting the average, some deviations will be negative and some will be positive. If we simply add up these values, the negative and positive deviations will cancel each other out, and the sum will be zero, which is not a useful measure of the spread of the data. Our goal is to quantify the difference from the mean, but we don’t really care if the value is below or above the mean. One way we could emphasize the difference is to square the difference. Then, the focus is on the size of the difference, as opposed to whether the value is above or below the average. (Note that we could also take the absolute value, but it is easier mathematically to work with the squares of the differences.) The second step, therefore, is to add up the squares of the deviations for all observations in the sample. This is sometimes called the sum of squares. The third and last step is to divide the sum of squares by the sample size n - 1, giving a single number called the variance. The variance is a modified average of the squared deviations. We use n - 1 instead of n because it gives a less biased estimate of the variance for all potential participants. The units for variance are the units of the variable squared. Because of this, it is hard to directly interpret the variance value. We typically take the square root of the variance so that the units of measurement match the original data. When we do this, we obtain the standard deviation. The standard deviation can be interpreted as the average deviation of the observations away from the mean. \\[ \\text{Standard Deviation (SD)}: s = \\sqrt{s^2} \\] Note that, like the mean, we use all observations in the data to find the standard deviation. So if one observation is much higher or much lower than the rest of the data, it will influence the value of the variance and standard deviation. Let’s calculate the variance and standard deviation for the Blood Pressure example. \\[s^2 = \\frac{(133 - 145)^2 + (115 - 145)^2 + \\dots + (180 - 145)^2 + (174 - 145)^2}{500 - 1} = 783.72 \\text{ mm Hg}^2\\] We find the deviations between the individual values and the sample mean of 145, square those deviations, add up all the squared deviations, and divide by n - 1, which is 499, and we get a variance of almost 784 mm Hg squared. \\[ s = \\sqrt{783.72} = 27.99 \\text{ mm Hg} \\] Taking the square root, we find that the standard deviation is almost 28 mm Hg. That is, the average deviation away from the mean for the systolic BP variable is roughly 28 mm Hg. 2.2.2.2.2 Interquartile Range Another common measure of the spread of numerical data is the interquartile range, or IQR. This measure, like the median, is based on the order of the observations, not on their values. The interquartile range is calculated from the first and third quartiles of the data and is the range of the middle 50% of the data. The first quartile (or 25th percentile) is the median of the lower half of the data. The third quartile (or 75th percentile) is the median of the upper half of the data. To obtain the interquartile range, we subtract the first quartile from the third quartile. Note that like the median, we order the data from smallest to largest and focus only on the values for the middle half, from Q1 through Q3. So if one observation in the data is much higher or much lower than the rest of the data, it won’t affect the value of the interquartile range. Also note that the exact definition of the median, and thus of the quartiles, varies slightly from one software package to another. For small datasets, the median and IQR given to you by software may differ slightly from the values you obtain by hand, but they will be close. For large datasets, the differences will be negligible. What is the interquartile range for the Blood Pressure data? The first quartile (the 25th percentile) is the average of the 125th and 126th values, which is 130 mm Hg. The third quartile (75th percentile) is the average of the 375th and 376th values, which is 162.5 mm Hg. The interquartile range is the difference between 162.5 and 130, which is 32.5 mmHg. This represents the range of values for the middle 50% of the data. (Note that the IQR for this dataset happens to be a bit bigger than the standard deviation.) Another useful measure of spread is the range. The range is simply the distance between the minimum (the smallest value) and the maximum (the largest value) in the dataset. The minimum, 1st quartile, median, 3rd quartile, and maximum comprise the five-number summary, which is often used as a group to summarize data. It gives a sense of the typical value of the data as well as how the values vary (both the middle 50% of the data as well as the whole data range). Let’s find the range of systolic blood pressure readings from the Blood Pressure example. Smallest 5: 67, 72, 73, 77, 77 Largest 5: 215, 216, 221, 222, 224 Minimum: 67 mm Hg Maximum: 224 mm Hg We see that 67 mm Hg is the smallest value in the dataset, so it is the minimum. The largest value is 224 mm Hg, the maximum. \\[ \\text{Range: } 224 - 67 = 157 \\text{ mm Hg} \\] The range is 157 mm Hg. Putting this all together, for the Blood Pressure dataset, the five-number summary for the systolic blood pressure would be: minimum: 67 mm Hg Q1: 130 mm Hg median: 140.5 mm Hg Q3: 162.5 mm Hg maximum 224 mm Hg 2.2.2.2.3 Other Measures of Spread Other measures of spread include the mean absolute error and the coefficient of variation. The mean absolute error (or MAE) is when you use the absolute difference between each observed value and the mean, instead of the squared difference, as used for the variance. You would then add up all the absolute differences and divide by the number of observations in the sample. The coefficient of variation (or CV) is the sample standard deviation divided by the sample mean, and is usually expressed as a percentage. For example, if the standard deviation is 5 and the mean is 50, then the coefficient of variation is 0.10 or 10%. The sample standard deviation is about 10% as large as the sample mean. The coefficient of variation helps us compare variability for different measurements that have different ranges of values. 2.2.2.3 Other Summary Measures Other summaries which use the order of the data include finding the observation that has a certain percentage of data below it. This is called a percentile. For example, the 10th percentile is the value in the dataset for which 10% of the sample values are below it (and thus the other 90% of the sample values are above it). Note that the first quartile, median, and third quartile are all percentiles. 2.2.2.4 Outliers and Robustness One last important step in describing a set of data is to note observations that are unusually different from the rest of the data. A data value may be much higher than most of the data, or much lower. When a data point doesn’t “fit” with the rest of the data, we call it a potential outlier. As we have mentioned already, extreme or unusual values can overly influence those summary measures based on value, like the mean and the standard deviation, but have less of an effect on those summary measures based on order, like median and IQR. Summary measures that are not as affected by outliers are called robust statistics. 2.2.2.5 Histograms Let’s move on to graphical summaries of numerical data. One way to visualize numerical data is with a histogram. The histogram divides the data into groups (or bins) based on their values and graphs the number or frequency in each bin. The observations are often divided into equal size groups (such as 60-79, 80-99, etc.), but this isn’t required. (CHRIS - A histogram, for visualizing purposes only, turns a numerical variable into a categorical one). For histograms, the data values are plotted on the x-axis and the number of participants for that data value or range are plotted on the y axis. Histograms (for numerical data) may appear similar to bar plots (for categorical data). There are, however, two big distinctions between histograms and bar plots: There is an inherent order to the values on the x axis of a histogram – we create partitions based on the values in their numeric order, and There is no space between the bars on a histogram. In the bar plot, there is a bit of a space between each bar to indicate that they are just different categories and there is not necessarily an order to those categories. In histograms, the numeric order of the values is maintained. The advantage to this type of graph is that you can quickly get a sense of any patterns in the data. If the data look evenly distributed around the middle, we refer to the data as symmetric. If we see that the peak or bulk of the participants are to the left, and the number of participants per group slowly decreases to the right, we say the data are right skewed. If we see that the bulk of the participants are to the right and there’s a slow decrease in the number per group to the left, we say the data are left skewed. The disadvantage is that this type of graph doesn’t give you any summary measures of the data, and doesn’t identify any potential outliers. Blood1 |&gt; ggplot() + geom_histogram(aes(SystolicBP), fill = &quot;blue&quot;, col = &quot;black&quot;, alpha = .5) + xlab(&quot;Systolic Blood Pressure (mm Hg)&quot;) + ylab(&quot;Frequency&quot;) + theme_bw() + theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank()) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Let’s examine the histogram for the Blood Pressure example, shown in Figure ??. The systolic blood pressure observations are centered in the vicinity of 130 mm Hg, and range from about 60 to 240 mm Hg. Over 1/3 of the people appear to have values between 120 and 140 mm Hg (the tallest peak). There are only a few people with systolic blood pressure below 100 mm Hg or above 200 mm Hg. We would say this data has a slight right skew, because the tail of the distribution is to the right of the bulk of the data. 2.2.2.6 Boxplot Another way to visualize numerical data is with a boxplot. Boxplots have also been called box-and-whisker plots (see the box and whiskers in the image shown). To create a boxplot, you use the five-number summary: The smallest observation that is not considered an outlier (if no potential outliers, the minimum) marks the end of the bottom “whisker” as shown above. The first quartile is the bottom edge of the “box”. The median is the line near the middle of the “box”. The third quartile is the top edge of the “box”. The largest observation that is not considered an outlier (if no potential outliers, the maximum) marks the end of the top “whisker”. If there are potential outliers, they are usually plotted as a dot beyond the “whiskers”. For boxplots, we find potential outliers using the 1.5*IQR rule. The 1.5IQR rule states that potential outliers are any observations which are either: greater than 3rd quartile + 1.5IQR, or less than 1st quartile – 1.5*IQR. Blood1 |&gt; ggplot() + geom_boxplot(aes(SystolicBP), fill = &quot;blue&quot;, col = &quot;black&quot;, alpha = .5) + scale_y_continuous(limits = c(-1, 1), breaks = NULL) + xlab(&quot;Systolic Blood Pressure (mm Hg)&quot;) + ylab(&quot;&quot;) + theme_bw() + theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank()) + geom_text(aes(x = 140.5, y = 0.4, label = &quot;Median&quot;), hjust = &quot;left&quot;) + geom_text(aes(x = 130.0, y = 0.4, label = &quot;1st Quartile&quot;, hjust = &quot;left&quot;)) + geom_text(aes(x = 162.2, y = 0.4, label = &quot;3rd Quartile&quot;, hjust = &quot;left&quot;)) + geom_text(aes(x = 81.7, y = 0.4, label = &quot;Smallest non-potential outlier&quot;, hjust = &quot;left&quot;)) + geom_text(aes(x = 210.5, y = 0.4, label = &quot;Largest non-potential outlier&quot;, hjust = &quot;left&quot;)) + coord_flip() For the Blood Pressure example, we can see from the boxplot (Figure ??) that the median is about 140 and the IQR ranges from 130 to about 160. We can also see that there’s a possible right skew, since the median is not in the middle of the box and is closer to the Q1 line. In addition, we can find the potential outliers using the 1.5*IQR rule. Potential outliers would be those values that are: less than 130 – 1.532.2 = 81.7 , or greater than 162.5 + 1.532.2 = 210.5. The smallest value in the dataset that is NOT a potential outlier is 81 mmHg and the largest value that is NOT a potential outlier is 211, so we draw a whisker to those values, and draw a horizontal line at those values. The remaining observations (below 81.25 or above 211.25) are plotted as dots below and above the ends of the whiskers. The advantages of a boxplot are that we can see any potential outliers and get a good sense of center and spread based on order. Side-by-side boxplots are also very useful for quickly comparing two or more datasets. The disadvantage of a boxplot is that we lose some information about the shape of the data. (CHRIS - We could introduce violin plots, sina plots, or overlay points). "],["sampling.html", "Chapter 3 Sampling 3.1 What is selection bias? 3.2 Random Sampling 3.3 Discrete Distributions", " Chapter 3 Sampling As we described in the Cycle of Research, statistical inference is the process of drawing conclusions about a population on the basis of observations from a sample. Put another way, statistical inference is the process of generalizing study results from the study sample to the population of interest. Drawing correct inferences about the population depends on knowing how representative the sample is of that population. A sample is representative of the population if every person or unit in the population of interest has the same chance of being included in the sample. If the chance of being included in the sample differs from person to person, then we are vulnerable to selection bias, which will affect our ability to make accurate inferences about the population. 3.1 What is selection bias? In general, “bias” refers to systematic error or inaccuracy in a result. The result deviates systematically from the truth. There are a number of kinds of bias that can be introduced into a research study, usually unintentionally, at any stage from design through to publication. Selection bias is formally defined as “A systematic tendency to favor the inclusion in a sample of selected subjects with particular characteristics while excluding those with other characteristics” (Pocket Dictionary of Statistics). Selection bias means that some members of a population were more likely to be included in the study than others. For example, if potential study participants were contacted by randomly selecting names from SnapChat, then people who do not have a SnapChat account will be systematically excluded. The resulting study sample is not representative of the population the researchers intended to study. Since the study sample is not representative of the population, the study results cannot be generalized to the population. 3.2 Random Sampling Random sampling methods are used in research studies to minimize this type of bias. Random sampling is a sampling technique for which the probability of each individual being selected into the sample is known. Another term used for this kind of sampling is probability sampling. Three random sampling methods are commonly used in medical or public health studies: simple random sampling, stratified random sampling, and cluster random sampling. 3.2.1 Simple Random Sampling A simple random sample (or SRS) is a random sample where each member has the same chance of being selected, and members are selected independently from each other. In other words, knowing whether other individuals were selected into the sample tells you nothing about whether or not you will be selected into the sample. When simple random sampling is carried out, each potential sample of n individuals from the population is equally likely to be selected. For example, in a simple random sample from the population of Minnesota residents, all residents are equally likely to be chosen, regardless of who they are or where they live. In addition, the probability of a given resident being chosen does not increase (or decrease) if their neighbor is chosen. There are two important things to note about simple random samples: First, taking a simple random sample from a given population doesn’t guarantee that your particular sample will “look like” your population, for the same reason that tossing a fair coin 10 times doesn’t always produce exactly 5 heads and 5 tails. Second, for practical reasons, it is rarely possible to obtain a “true” simple random sample from a population. Instead, it is an ideal that studies try to achieve to the closest degree possible, in order to obtain a sample that is representative of the entire population. Obtaining a simple random sample (SRS) requires first that all possible members of the population of interest are identified and listed. Once that is accomplished, a random sample of size n can be chosen using a number of possible methods. An identification (ID) for each member of the population can written on slips of paper and placed in a hat, and someone can randomly select n slips of paper. This ultra-low-tech method works for small populations, such as children in a school classroom. A table of random numbers can be used to select n items from the population list. This method is useful when you do not have access to a computer. A computer program (for example, random.org) can be used to randomly select n items from the population list. For example, we could have a numbered list of all 51,000 students at the University of Minnesota (our population of interest). We could use a random number generator to give us 1,000 random numbers, between 1 and 51,000. Then we would select the 1,000 students whose numbers on the list correspond to the randomly chosen numbers. These 1,000 students would form our sample. In the health sciences, situations where you can obtain a sample via simple random sampling are quite RARE due to feasibility issues, or sometimes even to desirability issues. Study designs often dictate the nature of sampling. 3.2.2 Stratified Random Sampling In stratified random sampling, the population is divided into subgroups, or “strata”, with similar characteristics, and a simple random sample is selected from each stratum. Common stratification variables include gender, age group, or clinic. Typically, the motivation for doing stratified sampling is to ensure that you have “enough” of a particular subgroup to carry out your analyses of interest. For example, if you were interested in looking at differences in breast cancer tumor characteristics between men and women, you’d do stratified sampling to ensure that you had enough men in the sample to make the sex comparison with reasonable precision. Stratified random sampling requires that information about the stratification variable or variables is available for all potential participants. 3.2.3 Cluster Random Samping In cluster sampling, “interventions” are delivered to groups rather than individuals. The population is divided into clusters and a sample of clusters (random or not) are selected. Cluster sampling is often done in a hierarchical or multistage fashion, with the selected clusters further divided into sub-clusters from which another sample (random or not) is selected. Cluster sampling is often used in epidemiologic studies. For example, if a study is interested in the effect of spraying insecticide to prevent malaria, the sampling is done on houses rather than individuals. Everyone in the house (the cluster) experiences the intervention at the same time, but the outcome of interest is measured at the individual level. Because of this, cluster sampling is an example of non-independent sampling. Clusters are often defined geographically. For example, you might divide a city into neighborhoods, and the neighborhoods into blocks, and the blocks into houses. You would randomly choose a specified number of neighborhoods, and then randomly choose a specified number of blocks within those neighborhoods, and then randomly choose a specified number of houses on each block. The random selections could be simple random samples, or they could be systematic random samples (for example: select every 6th house in the block). Cluster sampling does not require that all members of a population be identified and listed. 3.2.4 Non-Random Sampling In non-random sampling, or non-probability sampling, the probability that a given participant is selected is unknown and may be influenced by selection bias. In convenience sampling, participants who are readily available are enrolled until the desired sample size is reached. That is, researchers just “conveniently” grab participants who are available. Volunteer sampling is a sampling method that relies on participants who choose to respond (for example, online surveys) and are a type of convenience sample. Quota sampling is a non-random version of stratified random sampling. Quota sampling occurs when the sample that is obtained has the same proportions of individuals as the entire population with respect to pre-specified known characteristics or traits. That is, the population is divided into categories with a required quota for each category. Participants are enrolled into the study from each category until the quota for that category is reached. In systematic random sampling, every kth item is chosen. For example, if you were sampling customers in a store, you might approach every 10th person who walked by, or every 20th person. Systematic random sampling should not be used if there are cyclical patterns in the data. For example, selecting hospital admissions data from every seventh day or every 12th month may not be representative of the overall population of admissions and could introduce bias. The problem with non-random samples is that they may be biased. Not all members of the population of interest have the same probability of being included in the study, so the study sample is not representative of the population. The participants who are “readily available” may differ in some ways from the population as a whole. For example, if a study of sleep apnea were done by sampling people who had recently been to a clinic for any reason, then people who had not been sick recently, or people who had no access to medical care, or people who were in hospitals, prisons, or nursing homes, or military personnel on active duty, would not be included in the study. However, in many studies, we actually use one of these approaches to obtain our sample but then view it (and do inference) as if it were an SRS. Often, this non-random sampling is “close enough” to random sampling that we are OK with it. For example, when recruiting for a clinical trial of a new HIV medication, we don’t make a numbered list of all individuals with HIV and then contact them randomly; instead, we approach people attending HIV clinics and ask them to participate. If we want to infer something about the population, we then have to make the argument that this approach approximates random sampling because, for example, the patients attending the clinic during the recruitment period (and who agree to participate) are representative of and have approximately the same characteristics (such as gender, age, socioeconomic status, etc.) as the overall population of HIV-infected individuals. Note that if data were collected on every member of the population of interest, then statistical inference would not be necessary. You do not need to infer from a sample to a population if you already have complete data on the population. Descriptive statistics completely represent the population when you have population data; inferential statistics are unnecessary. 3.3 Discrete Distributions 3.3.1 Random Variables A random variable is an abstract concept that represents the values that could arise from a random process, before they are observed. A random process could be something like tossing a coin or rolling a die, or it could be randomly sampling participants from a population of interest for a survey or a clinical study. We typically use capital letters to represent random variables. For example, the variable smoker, which describes a participant’s smoking status in the Blood Pressure dataset, can be seen as a random variable symbolized by a capital letter \\(Y\\). The smoker random variable can take on only two possible values: 0 if the participant is a nonsmoker, and 1 if the participant is a smoker. Lowercase letters are used to represent the actual observed values. Every person in the Blood Pressure dataset has a measured value for the random variable smoker. The first person in the dataset is a nonsmoker, so their value is 0. We use a lower case letter, \\(y\\) in this case, with a subscript of 1 to indicate that this is the first person in the dataset, followed by the value for that person of 0 since they are a nonsmoker (that is, \\(y_1 = 0\\)). Similarly, person 15 in the Blood Pressure dataset is a smoker, so their value is 1. We note this with a lower case y, then the subscript 15, and set that equal to 1 (that is, \\(y_15 = 1\\)). 3.3.1.1 Discrete Random Variables A discrete random variable is a variable that can take on only a finite (or limited) number of possible values. There are two types of discrete random variables: discrete numerical variables, which have a finite set of whole number values, and categorical variables, which have a finite number of possible categories. The smoker variable in the Blood Pressure dataset, a categorical variable, is a discrete random variable because it can take on only two possible values: smoker (1) or nonsmoker (0). The outcome of a die roll, a discrete numerical variable, is a discrete random variable because we can roll only one of six possible numbers: 1, 2, 3, 4, 5, or 6. 3.3.2 Distributions A distribution describes how often different values for a random variable arise from a random process. There are three types of distributions that are discussed in this book. A sample distribution describes how often each possible value of a variable has occurred in a specific sample. When we graph the sample data for a variable using a barplot or a histogram, we are visualizing the sample distribution. A population distribution describes how often each possible value of a variable occurs in the population of interest. A sampling distribution describes how often each possible value of a sample statistic (such as the sample mean) could occur in all possible samples. Sampling distributions will be discused later. 3.3.2.1 Categorical Distributions Let’s start with distributions for categorical variables. Categorical variables can be summarized graphically using barplots, or in tables using the number and/or proportion in each category. Either kind of summary is a sample distribution, but for clarity we will focus on graphical summaries. The barplot on the left above is a sample distribution for the Overwt variable in the Blood Pressure dataset. Recall that the Blood Pressure dataset contains data on a sample of N = 500 adults from the U.S. population. The plot is a sample distribution because it describes how often different values of the Overwt variable occur in this sample. For each possible category of the variable (normal, overweight, or obese), the plot shows the proportion of participants in the sample who fall into that category. This plot could also have been produced as the number of participants in the sample who fell into each category. The barplot on the right above is a population distribution for weight in the U.S. adult population as determined by the Centers for Disease Control and Prevention (CDC). It is plotted using the same weight categories as in the Blood Pressure dataset. This plot is a population distribution because it describes how often different values of the weight variable occur in the population. For each possible category of the variable (normal, overweight, or obese), the plot shows the proportion of the U.S. adult population that falls into that category. Population distributions always use proportions or probabilities on the y-axis. Note that the overall structure is the same. We have the categories on the x-axis and a measure of quantity on the y-axis. We can see that the proportions in each weight category in the sample data (on the left) are similar to but not exactly the same as the true proportions in the population (on the right). We will learn later how to use data from a sample (as shown on the left) to make inferences about the population (as shown on the right). For now, let’s focus on what information the population distribution gives us. A population distribution gives us several different kinds of information. First, it gives information about overall trends. For example, looking at the population distribution for weight in U.S. adults (above), we see that the normal weight category has the lowest proportion, and the obese weight category has the highest proportion. Second, it gives information about the proportion of the population in a particular category. In our example, we see that the proportion of U.S. adults who are classified as obese is .425, the proportion classified as overweight is 0.311, and the proportion classified as normal weight is 0.264. Finally, it gives information about the proportion of the population in several categories. In our example, if we would like to know the proportion of the U.S. adult population that are not obese, we could add up the proportion that are normal weight and the proportion that are overweight (0.264 + 0.311 = 0.575) and find that 57.5% of U.S. adults are not obese. Proportions obtained from population distributions can also be interpreted as probabilities. Since the proportion of U.S. adults who are obese is 0.425, then if we select a person at random from the U.S. adult population, there is a 42.5% chance (or a probability) that the selected person will be obese. 3.3.2.1.1 Bernoulli Distribution For example, suppose we were interested in a random binary variable \\(X\\), which describes whether a random U.S. adult is obese or not. \\(X\\) takes on the value 1 if the person is obese, and 0 if they are not. The population distribution of the random variable \\(X\\) could be described using a Bernoulli distribution with p = 0.425. A plot of this distribution (above) shows us that the probability of the event, being obese, is 0.425, and the probability of not having the event, not being obese, is 0.575. Theoretical distributions are described using mathematical notation by giving the name of the distribution and the parameters that describe it. The notation is to list the random variable and then a tilde (\\(\\sim\\)) symbol, which indicates that the variable has a given distribution. (The tilde can be read as “is distributed as”.) The tilde is followed by the name of the theoretical distribution followed by parentheses that include the parameter or parameters that define the distribution. The general notation for a Bernoulli distribution, then, would be \\(X \\sim \\text{ Bernoulli(p)}\\), read as “X is distributed Bernoulli with probability p”, where p is the probability of the event of interest. In our example, we would write \\(X \\sim \\text{ Bernoulli(0.425)}\\). The binary obesity variable \\(X\\) is distributed Bernoulli with probability 0.425. 3.3.2.1.2 Discrete Numerical Variables Now, let’s turn to distributions for discrete numerical variables. Discrete numerical variables can be summarized graphically using histograms, or in tables using the number and/or proportion in each category. Either kind of summary is a sample distribution, but for clarity we will focus on graphical summaries. The histogram on the left is a sample distribution for the results of rolling a standard six-sided die 330 times. The plot is a sample distribution because it describes how often different values (1 through 6) of the die roll occurred in this sample. In this particular sample of 330 rolls, the die came up as “1” 57 times, as “2” 488 times, and so on. The histogram on the right is a population distribution for rolling a fair standard six-sided die. This plot is a population distribution because it describes how often different values of a die roll occur in the population. For each possible value of the die roll (1 through 6), the plot shows the probability of obtaining that value. If the die is fair, the probability of any given die roll value is the same: 1 in 6, which gives 0.167 or 16.7%. Note that the overall structure is the same: We have the possible values on the x-axis and a measure of quantity on the y-axis. We can see that the proportions for each die roll value in the sample data (on the left) are similar to but not exactly the same as the true probabilities in the population (on the right). We will learn later how to use data from a sample (as shown on the left) to make inferences about the population (as shown on the right). For now, let’s focus on what information the population distribution gives us. A population distribution gives us several different kinds of information. First, it gives information about overall trends. For example, looking at the population distribution for all possible rolls for a fair die (above), each die roll has an equal probability of happening. This is because we are assuming that the die is a fair die, and each number has an equal chance of happening. Second, it gives information about the probability in a particular category. In our example, there are six different values on the die (1 through 6), and each has an equal chance of being rolled. Therefore, the chance of rolling any one number is 1 out of 6, or 0.167. For example, the probability of rolling a 1 would be 1/6 or 0.167. Finally, it gives information about the probability in several categories. In our example, if we would like to know the probability that a die roll would be less than 4, we would add up the probabilities for all possible die rolls less than 4 (that is, rolls of 1, 2, or 3): 0.167 + 0.167 + 0.167 = 0.5. We would find that the probability of rolling a number less than 4 with a fair six-sided die is 1/2. 3.3.2.1.3 Binomial Distribution One common discrete numerical distribution model is the Binomial distribution, which is used to model the number of times a specific event occurs in multiple attempts. For example, how many times would we get heads if we tossed a coin four times? The Bernoulli distribution, which we discussed earlier, tells us that the probability of obtaining heads on a single coin toss (assuming the coin is fair) is 0.50 or 50%. But what if we tossed the coin more than once? How many times should we expect heads? The Binomial distribution gives probabilities for each possible number of heads we could get if we tossed the coin multiple times. The Binomial distribution is characterized by two parameters: p, which is the probability of the event of interest on any given attempt, and n, which is the total number of attempts. The random variable X gives the number of events (heads, in our example) that occur when a binary outcome (such as obtaining heads on a coin toss) is measured on a number, n, of different independent attempts (such as coin tosses). The possible values of X range from 0 to n: in other words, in n tries, we could obtain anywhere between 0 and n events. The probability of the event (heads, in this case), p, is assumed constant for all attempts. Under these assumptions, the number of events that occur, X, is distributed as Binomial with parameters n and p, where n is the number of attempts and p is the probability of the event. In symbols, X ~ Binomial(n,p). In our example, what would happen if we tossed a fair coin four times? How many heads should we expect? Here, the number of attempts, n, is 4, and the probability of the event (heads), p, is 0.5. We could conceivably obtain anywhere between 0 heads and 4 heads. The probabilities for each of the possible number of heads that could be obtained, 0 through 4, are given in the theoretical distribution plot above. The most likely result, with a probability of 0.375 or 37.5%, is that we will obtain two heads (and two tails) in our four tosses. There is a somewhat lower probability (25%) of obtaining one head, or three heads. There is a very low probability (6.25%) of obtaining no heads, or all four heads. 3.3.2.1.3.1 Example Now let’s look at an example related to public health. Suppose that the (known) prevalence of obesity in the U.S. adult population is p = 0.379 or 37.9%. If we randomly sample four adults from the U.S. population, what is the probability that exactly two of the adults will be obese? What is the probability that two or fewer (i.e. half or less) of the adults will be obese? How likely is it that all four of the adults will be obese? These probabilities can be obtained by using a Binomial distribution for the random variable X, which is the number of the selected U.S. adults who are obese. The Binomial distribution with the number of independent attempts (people) set at n=4 and the probability of the event (obesity) set at p = 0.379 is shown in the plot above. The probability of exactly two of the four people being obese (X = 2) is 0.332 or 33.2%, while the probability of two or fewer being obese is 0.844 or 84.4%. The probability of all four being obese is quite low, at 0.021 or 2%. So… how do we obtain these Binomial probabilities? There are two ways. The first is to use the Binomial formula, and the second is to use statistical software. 3.3.2.1.3.2 Binomial Probabilities by Formula \\[ Pr(X = k) = \\frac{n!}{(n - k)!k!}p^k(1 - p)^{n - k} \\] \\[ \\text{where } n! = n * (n - 1) * ... * 2 * 1 \\] If the random variable X has a Binomial distribution, we can calculate the probability of any number of events, k, given n attempts with probability p, using the Binomial formula given above. For our coin toss example, we would use n = 4, p = 0.5, and then calculate the probabilities one at a time for k = 0 heads, k = 1 head, k = 2 heads, k = 3 heads, and k = 4 heads. This calculation can get tedious and error prone as the number of attempts n increases. A better way is to use statistical software. 3.3.2.1.3.3 Binomial Probabilities by Software Statistical software such as SAS or R allows us to easily calculate Binomial probabilities. The exact syntax will depend on the software used, and will be discussed in the activities, but the information that must be provided to the software is the same for any software. We will need to tell the software the name of the distribution (here, the binomial distribution), the number of attempts or observations (n), the probability of the event of interest (p), and the number of events or “successes” (k) we are interested in. For our coin toss example, we would use the Binomial distribution with n = 4, p = 0.5, and k=the number of heads (“successes”) we wanted the probability for. Let’s work through a few cases using our public health example. Recall that we assumed that the (known) prevalence of obesity in the U.S. adult population is p=0.379 or 37.9%, and that we have obtained a random sample of four adults from this population. What is the probability that exactly two of the four adults are obese? In this case, the number of attempts or observations is the number of adults in our sample, which is four, so n = 4. The probability of the event of interest (being obese) is p = 0.379. The number of events we are interested in is two, so k = 2. This is indicated by the yellow bar in the plot above. We can either insert these values n, p and k into the Binomial formula given previously and calculate the probability, or we can insert them into our software of choice (R or SAS) and it will give us the probability. In either case, the probability that exactly two of the four adults in our sample are obese is 0.332 or 33.2%. What if, instead, we wanted to know the probability that two or fewer of the four adults are obese? The number of attempts or observations is still the number of adults in our sample, which is four, so n = 4. The probability of the event of interest (being obese) is still p = 0.379. Now, however, the number of events we are interested in is two or fewer, so k could be two, or one, or zero. This is indicated by the yellow bars in the plot above. Again, we could use the Binomial formula given previously, and calculate the probability for k = 0, and then the probability for k = 1, and then the probability for k = 2, and then add the three numbers together. Alternatively, we could tell our statistical software that we wanted the probability of k or fewer events (using a slightly different command than the one for the probability of exactly k events). In either case, the probability that two or fewer of the four adults in our sample would be obese is found to be 0.844 or 84.4%. Suppose instead we wanted to know the probability that more than two of the four adults are obese (i.e. that k = 3 or k = 4)? This is indicated by the red bars in the plot above. In this case, we would simply subtract the probability of two or fewer being obese (which is 0.844) from the total probability (which is 1.0), to give 0.156 or 15.6%. Finally, what if we wanted to know the probability that between 1 and 4 of the four adults are obese? In other words, what is the probability that we will have more than 1 but less than 4 obese adults? The number of attempts or observations is still the number of adults in our sample, which is four, so n = 4. The probability of the event of interest (being obese) is still p = 0.379. What is k? Between one and four events means that the number of events (being obese), k, could be two or three. This is indicated by the yellow bars in the plot above. We could use the Binomial formula given previously, and calculate the probability for k=2, and then the probability for k=3, and then add the two numbers together. With software, it gets a bit more complicated here, since most statistical software is set up to calculate the probability of being less or equal to a specified value of k. So in this case, we need to ask the software for the probability of less than or equal to 3 events (i.e. the probability of 3 or 2 or 1 or 0 events), and then ask it for the probability of less than or equal to 1 event (i.e. the probability of 1 or 0 events) and then subtract the two numbers. In this case, the probability of 3 or fewer events (3 or fewer of the four people being obese) is 0.979 or 97.9%. The probability of 1 or fewer events (1 or fewer of the four people being obese) is 0.512 or 51.2%. Therefore, the probability that between 1 and 4 of the four adults being obese is 0.979 – 0.512 which is 0.467 or 46.7%. In either case, we will get the same result: the probability that between 1 and 4 of the four adults in our sample would be obese is 0.467 or 46.7%. We must be very careful with words here. The probability that between 1 and 4 of the adults are obese depends on whether you include the 1 and/or the 4. We could have four possible cases, depending on whether we include 1, include 4, include both, or include neither. The resulting probabilities for the four cases will be different. For example, the plot above shows the case where we include both 1 and 4 as well as the numbers between 1 and 4, as indicated by the yellow bars. As before, we can calculate this probability in two ways. We could use the Binomial formula given previously, and calculate the probabilities for k=1, and k=2, and k=3 and k=4, and then add the four numbers together. With software, we need to ask the software for the probability of less than or equal to 4 events (i.e. the probability of 4 or 3 or 2 or 1 or 0 events), and then ask it for the probability of less than or equal to 0 events (i.e. the probability of 0 events) and then subtract the two numbers. In this case, the probability of 4 or fewer events (4 or fewer of the four people being obese) is 1.000 or 100%. The probability of fewer than 1 events (0 of the four people being obese) is 0.149. Therefore, the probability that at least 1 and at most 4 of the four adults are obese is 1 – 0.149 which is 0.851 or 85.1%. Compare this result to our previous slide, where the probability that between 1 and 4 of the four adults are obese was 0.467, which is slightly more than half the probability that between 1 and 4 adults inclusive are obese. It’s critical when finding probabilities for discrete distributions to pay attention to the words used to describe the values of interest, and determine which values are part of the range of interest. 3.3.2.1.4 Other Discrete Distribution There are many other theoretical distributions relevant to specific kinds of discrete data, including the Hypergeometric distribution, the Poisson distribution, and the Negative Binomial distribution. For example, the Poisson distribution is used to model the distribution of what is called “count data” (for example, the number of births each day at a hospital) and is characterized by a single parameter, lambda (\\(\\lambda\\)), the average rate of the event being counted (for example, the average number of births per day at that hospital). 3.3.3 Continuous Distributions The key difference between a discrete random variable and a continuous random variable is that a continuous random variable can, at least in theory, have infinite possible values. This is because any given range of numbers (say the range from 0 to 1) can be divided into an arbitrarily large number of pieces: we could divide it into tenths, or hundredths, or thousandths, and so on. For example, if we measure systolic blood pressure, we could in principle measure it to the nearest one millimeter of mercury (mm Hg), or to the nearest tenth of a millimeter, or to the nearest hundredth of a millimeter, without limit. ## [1] 133 115 140 132 133 138 Of course, in practice, there are no blood pressure cuffs that can measure a person’s blood pressure to a precision of dozens of decimal places. In addition, even if our measurement device can measure blood pressure to a precision of (say) two decimal places, for practical reasons we may choose to report the measured blood pressure only to the nearest whole number, as shown above for the first six observations for the SystolicBP variable in the Blood Pressure dataset. Nevertheless, we consider blood pressure to be a continuous random variable. Let’s look at continuous distributions. Continuous variables can be summarized graphically using histograms or boxplots, or in tables using the mean and standard deviation (or median and IQR). Either kind of summary is a sample distribution, but for clarity we will focus on graphical summaries. The histogram on the left above is the sample distribution for the SystolicBP variable in the Blood Pressure dataset. Recall that this sample consists of 500 people drawn from the U.S. population. The plot is a sample distribution because it describes how often different values of the SystolicBP variable occur in this sample. The y-axis denotes the number of individuals in our sample that take on certain values. Alternatively, you can construct histograms with “frequency” on the y-axis to denote the proportion of individuals. The plot on the right above shows the population distribution for systolic blood pressure among all U.S. adults. This plot is a population distribution because it describes how often different values of systolic blood pressure occur in the population. The y-axis denotes the proportion of individuals in the population that take on certain values. Unlike the population distribution for a discrete variable, the population distribution for a continuous variable is usually represented using a density curve instead of a histogram. This is because a continuous variable has an infinite number of possible values, so the probability at any one particular value (such as at 147.539872846384 mm Hg) is effectively zero. Therefore, probabilities for continuous variables can only be determined for ranges of values, not for single values. The area under the density curve represents probability. The total area under the density curve is equal to 1.00 or 100% probability. The area under the curve within a specific range of values represents the probability that the variable has a value in that range. In our example, estimating by eye the area under the curve above 200 mm Hg, it appears that the probability that a randomly chosen U.S. adult has a systolic blood pressure above 200 mm Hg is quite small, roughly (very roughly) 0.05 or 5%. Note that the overall structure of the sample distribution and the population distribution is the same: we have the possible values on the x-axis and a measure of quantity on the y-axis. The sample histogram for systolic blood pressure has the proportion in each bin on the y-axis. As previously mentioned, it could also be plotted with the count (or frequency) in each bin on the y-axis. The population density curve, on the other hand, always has probability on the y-axis. We can see that the proportions for each range of systolic blood pressure values in the sample data (on the left) are similar to but not exactly the same as the true probabilities in the population (on the right). We will learn later in this course how to use data from a sample (as shown on the left) to make inferences about the population (as shown on the right). For now, let’s focus on what information the population distribution gives us. A population distribution gives us several different kinds of information. First, it gives information about overall shape and trends. In our example, the population distribution (above) has a single peak (that is, unimodal) and appears slightly skewed to the right. Second, it gives information about the probability for a range of values. In our example, we see that the most common systolic blood pressure value among U.S. adults is where the peak of the curve lies, at approximately 130 mm Hg. We can also see that most U.S. adults have systolic blood pressures between about 90 and 210 mm Hg, and that only a few have values above 210 mm Hg. Unlike for discrete variables, we cannot find the probability for a single value of a continuous variable, since, as discussed earlier, the probability for any single value of a continuous variable is essentially zero. Here is an example: One measure of hypertension (high blood pressure) is systolic blood pressure over 140 mm Hg. We can use the population distribution to find the proportion of U.S. adults who have high blood pressure by finding the area under the density curve above 140 mm Hg. We find that the proportion of U.S. adults with blood pressure above 140 mm Hg is 0.388 or about 39%. Note that, since the probability of having a systolic blood pressure of exactly 140 is essentially zero, the proportion of U.S. adults with blood pressure at or above 140 mm Hg is also 0.388. 3.3.3.1 Normal Distribution One very common theoretical distribution for continuous variables is the Normal distribution. It is also called the Gaussian distribution in honor of Carl Friedrich Gauss who noted this general shape in his work and characterized it, or the “bell curve” because the shape looks like a bell. A Normal distribution is always unimodal (has only one peak) and symmetric. Due to its symmetry, its mean and median are identical. The majority of values are very close to the peak with fewer values further away from the peak. In principle, a Normal distribution has nonzero probability at any value all the way from negative infinity to positive infinity. In practice, Normal distributions are often used to model variables that can only take on positive values, such as systolic blood pressure or serum cholesterol. Each Normal distribution is characterized by two parameters: the population mean, μ (pronounced mu), and the population standard deviation, σ (pronounced sigma). The mean defines the center or peak of the distribution and the standard deviation characterizes its width or spread. The random variable X represents the values that a continuous variable could take on (such as systolic blood pressure values). If this continuous variable is normally distributed, then we would say that the variable, X, is distributed as Normal with parameters μ and σ, where μ is the population mean and σ is the population standard deviation. In symbols, we write X \\(\\sim\\) Norm(\\(\\mu, \\sigma\\)). For all Normal distributions, slightly more than two thirds of their values (68.27% to be exact) lie within plus or minus one standard deviation from the mean, 95.45% of the values lie within plus or minus two standard deviations from the mean, and 99.73% of the values lie within plus or minus three standard deviations of the mean. This is sometimes referred to as the “68 – 95 – 99.7 rule” for Normal distributions. The plot above shows two different Normal distributions. The gold curve is a Normal distribution with a mean of 50 and a standard deviation of 10. The red curve is a Normal distribution with a higher mean of 80 and a larger standard deviation of 20, so it is shifted to the right relative to the gold curve and is wider. Now let’s look at an another example in more detail. Serum cholesterol is an indicator of heart health, and for U.S. women ages 20-34, the population distribution for serum cholesterol looks very similar to a normal distribution. We can model this population distribution by using a Normal distribution with mean cholesterol of 185 mg/dL and standard deviation of 39 mg/dL, as shown in the plot above. We can then use this distribution to answer questions about probabilities. For example, what is the probability that a randomly selected U.S. woman in this age range would have a serum cholesterol level above 240 mg/dL? This probability turns out to be 0.079 or slightly under 8%. Or… what is the probability that a randomly selected U.S. woman in this age range would have a serum cholesterol level below 240 mg/dL? This turns out to be 0.921 or about 92%. Or… what is the probability that a randomly selected U.S. woman in this age range would have a serum cholesterol level between 200 and 240 mg/dL? This turns out to be 0.271 or about 27%. So… how do we obtain these Normal probabilities? There are two ways. The first is to use the formula for the Normal distribution, and the second is to use statistical software. \\[ Pr(X \\leq x) = Pr(X &lt; x) = \\int_{-\\infty}^x \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^2} \\] If the random variable X has a Normal distribution with known mean and standard deviation, we can calculate the probability of X being less than any specific value, x, using the Normal distribution equation given above. This calculation requires knowledge of calculus to do the integration and, like any hand calculation, is prone to error. A better way is to use statistical software. We can use any statistical software package to calculate normal probabilities. It will require that we provide the name of the distribution (here, a Normal distribution), the parameters of the distribution (here, the population mean and the population standard deviation), and the specific value of interest. Software will typically by default give the probability of being less than the specific value of interest. If we want to calculate the probability of being greater than the specific value of interest, or the probability of being between two specific values, then we have to do a bit more work. What if we want to calculate the probability of being above a certain value? Doctors use serum cholesterol levels to monitor heart health, and when the cholesterol level exceeds 240 mg/dL, doctors often start some sort of treatment or intervention, such as medication. What proportion of young women would need some sort of intervention? In other words, what is the probability that a randomly selected U.S. woman between the ages of 20 and 34 would have a serum cholesterol level above 240 mg/dL? Software will give us the probability that a randomly selected woman from this population would have a cholesterol level below 240 mg/dL, which turns out to be 0.921. Since the probability of all events is 1, the probability of having a cholesterol level above 240 mg/dL is simply 1 – 0.921 or 0.079. Alternatively, we could be interested in the probability of being between two values. For example, serum cholesterol levels below 200 mg/dL are considered “healthy”, while serum cholesterol levels between 200 and 240 mg/dL are considered “at risk” – they are not high enough to warrant some sort of medical intervention, but the person’s heart health still needs to be closely monitored. We would like to know the proportion of U.S. women aged 20-34 who have serum cholesterol levels in this “at risk” range of 200-240 mg/dL. How do we calculate this? Let’s look at what what we need and what we can get from software. We want to know the proportion of women with cholesterol levels between 200 and 240 mg/dL. Software can tell us the proportion of women with cholesterol levels at or below a specified value. So software can tell us the proportion at or below 200 mg/dL and the proportion at or below 240 mg/dL. We find the proportion of women with cholesterol levels between 200 and 240 mg/dL by subtracting the proportion with levels below 200 mg/dL (which is 0.650) from the proportion with levels below 240 mg/dL (which is 0.921), giving 0.921 – 0.650 or 0.271. We can say that 27.1% of U.S. women age 20-34 have serum cholesterol levels between 200 and 240 mg/dL. Equivalently, we could say that there is a 27.1% chance that a randomly selected U.S. woman age 20-34 would have a serum cholesterol level in the “at risk” range of 200 – 240 mg/dL. We can also use software to turn the calculation around and find the values associated with a given percentage (or proportion) of women in the population. For example, what if we wanted to know what range of values of serum cholesterol would include 95% of the women in our population of interest? We can find that range using either software or our knowledge of normal distributions. If we use software, we start with the fact that we want the middle 95% of values. This means that the remaining 5% of values would be divided evenly above and below this range. So, the lower end of the range would be the cholesterol level which is above the lowest 2.5% of values. This value is called the 0.025 quantile or the 2.5% percentile of this normal distribution. Similarly, the upper end of the range would be the cholesterol level which is below the highest 2.5% of values, or, equivalently, above the lowest 97.5% of values. This value is called the 0.975 quantile or the 97.5% percentile of this normal distribution. Statistical software can give us any specified quantile of a normal distribution. We must specify the name of the distribution (Normal), the parameters of the distribution (here, the mean of 185 mg/dL and the standard deviation of 39 mg/dL), and the quantiles we are interested in (here the 0.025 and 0.975 quantiles). In our example, software will tell us that the 0.025 quantile is at 107 mg/dL, and the 0.975 quantile is at 263 mg/dL. In this particular case, we can also use our knowledge of the normal distribution to find the values directly, by using the “68 – 95 – 99.7 rule” for normal distributions. This rule tells us that 95% of the values will be within two standard deviations from the mean. In our example, the mean cholesterol is 185 mg/dL, and the standard deviation is 39 mg/dL. So the lower end of the middle 95% of the cholesterol values will be two standard deviations below the mean, at 185 – 239 = 107 mg/dL. Similarly the upper end of the middle 95% of the values will be two standard deviations above the mean, at 185 + 239 = 263 mg/dL. Since all normal distributions are the same except for their mean and standard deviation, it can be convenient to standardize the data and model it using the Standard Normal distribution. We can standardize any continuous random variable X by taking each value and subtracting the mean (this tells us how far away from the mean that value is), and then dividing by the standard deviation (this tells us how many standard deviations away from the mean that value is). The resulting standardized random variable is typically called Z and its values are denoted as z or a “z-score”. Positive z-scores indicate that the value is above the mean, while negative z-scores indicate that the value is below the mean. Larger z-scores indicate that the value is further away from the mean than smaller z-scores. In our serum cholesterol example, if a particular woman has a cholesterol value of 263 mg/dL, then her standardized cholesterol level would be (263 – 185)/39 or a z-score of 2.0. Her cholesterol level is 2 standard deviations above the mean cholesterol level. The Standard Normal distribution can be used to model a standardized continuous variable. The Standard Normal distribution has the same shape as any Normal distribution, but it always has a mean of zero and a standard deviation of 1.0. We write that Z is distributed as Normal with mean 0 and standard deviation 1. In symbols, Z ~ Norm(0, 1). Standardizing variables can come in handy when we don’t have access to software but need to find a probability. Most statistics textbooks contain tables which give the probability of being below Standard Normal values between -3.49 and 3.49 or between -4 and 4. There are also websites and even apps that can find the probabilities for a standard normal value. Let’s use the Standard Normal distribution to address the question: what is the probability that a U.S. woman aged 20-34 has a serum cholesterol level warranting some sort of medical attention or intervention, that is, a cholesterol level above 240 mg/dL? We start by finding the z-score for this value, which is 1.41. In other words, 240 mg/dL is 1.41 standard deviations above the mean value. We can then use software, a table, or a website to find the probability associated with a z-score of 1.41. Since we need the probability of being above 1.41, we can certainly find the probability of being below 1.41 and subtract that value from 1. But we can also take advantage of the symmetry of the Standard Normal distribution around its mean of 0 and find the probability of being below -1.41, since that is the same as the probability of being above 1.41. Using the standard normal table, that probability is 0.0793. The Normal distribution is important in statistics for a number of reasons. One reason is that the sample or population distributions for some continuous variables that we encounter in real life look approximately Normal. Another reason is that the sampling distribution for sample means is approximately Normal if the sample size is large enough. (We will explore sampling distributions in the next unit.) A third (although minor) reason is that the Binomial distribution (which we explored in an earlier lecture) becomes approximately Normal if the sample size is large enough, even though it is a distribution for discrete variables. The Binomial distribution has a mean and standard deviation that depends on the sample size (or the number of attempts), n, and the probability of the event, p, as shown on the slide above. The mean, np, is the expected number of “successes” or events in n attempts. The standard deviation increases as the number of attempts, n, increases. Therefore, as the sample size increases, the mean of the Binomial distribution increases (so the peak of the distribution shifts to the right) and the standard deviation increases (so the distribution broadens). If the sample size, n, is large enough, the Binomial distribution looks nearly Normal, as shown for n=30 and p=0.379 in the plot above. It can therefore be modeled using a Normal distribution using the same mean and standard deviation for that particular Binomial distribution. How large a sample size is “large enough”? This depends on the probability of the event of interest, p. The sample size needs to be large enough that we’d expect to see at least 10 participants in the sample who have the event (np), and at least 10 participants in the sample who don’t have the event of interest (n(1-p)). The fact that the Binomial distribution is approximately Normal for large n used to be helpful in doing statistical calculations, since Normal distribution probabilities were readily available in tables. This is called using the “Normal approximation to the Binomial”. It is becoming less helpful now, since exact Binomial probabilities are easily calculated using statistical software. There are many other theoretical distributions relevant to specific kinds of continuous random variables, including the Exponential distribution, the Lognormal distribution, the Beta distribution, and the Gamma distribution. These distributions will not be discussed further in this course. "],["sampling-distributions.html", "Chapter 4 Sampling Distributions", " Chapter 4 Sampling Distributions In this chapter, we will introduce the abstract concept of sampling distributions and their importance to statistics. In an earlier lecture, we learned about common theoretical distributions, such as the Binomial distribution and the Normal distribution. We also learned about population parameters that define these distributions, such as \\(p\\), \\(\\mu\\), and \\(\\sigma\\). Theoretical distributions and their parameters are useful when we are trying to understand the probability of observing various outcomes in random samples given a known characteristic of the population. For example, suppose for the population of people who suffer from occasional migraine headaches, 60% of them get some relief from taking ibuprofen. 60% is assumed to be a known characteristic (or parameter) of the population. One question we could answer based on this information is, if we have a random sample of 50 people who suffer from occasional migraines, what is the chance that 10 of them will get relief from ibuprofen? We can use the Binomial distribution to answer this question. In this example, we are working from the known population parameter to tell us information about the unknown sample.However, when analyzing data, we are trying to make an inference about the population given the result or estimate from the sample. It essentially is working the opposite way from that described on the previous slide. Using the same example as the previous slide, suppose we are interested in investigating how effective ibuprofen is for people who suffer from occasional migraine headaches. To do this, we go out and obtain a sample from the population and get an estimate of the proportion who get relief after taking ibuprofen. We could then use the results from the sample to make an inference about the population parameter of interest, the proportion who get relief. So the unknown characteristic here is the population parameter and the known part is the result from our sample. Theoretical distributions are used during the process of making an inference but as a way to model the behavior, or distribution, of the sample statistic. What do we mean by this last sentence? This lecture addresses that question, which is the foundation for statistical inference.Suppose we want to know the proportion of people in the United States who have diabetes. One approach would be to survey everybody in the United States and calculate the proportion of them that have diabetes. Suppose this population proportion is 0.093 or [9.3%][CDC2014]. But, suppose that we didn’t know the population proportion (it’s a black box) and that it isn’t possible to collect data from everybody. Instead, we have to obtain a representative sample of people from the population, make measurements on them, and then use that information to infer something about the proportion in the entire population. How can we use information from a single sample to say something about the population? We have to understand a key thing about samples: sampling variability. What is sampling variability? Sampling variability is the term we use to explain what happens when the random sampling process is repeated. It tells us what we intuitively know. If many random samples were collected, the measured quantities, or sample statistics, from those samples will vary from one another. Additionally, the sample statistic we obtain will (most likely) be different from the population quantity, or parameter, of interest. We need to be able to understand the behavior of how the sample statistics vary in order to be able to make an inference about the population parameter. This information about how much a statistic varies from sample to sample is key in helping us know how accurate an estimate is. How can we do this? We can simulate what would happen if random samples of the same size were repeatedly taken.Let’s go back to the Diabetes example to examine the variability of the proportion from sample to sample. Suppose researchers obtain a random sample of 100 people from the United States. They calculate the proportion in their sample (denoted p-hat) who have diabetes and find that 0.110 or 11% of those in their sample have diabetes. Two other researchers do the exact same thing. In their random samples of 100 people, one finds that 7% have diabetes and the other finds 12% have diabetes. Each of these values are an estimate of the true population proportion. Just by chance, these researchers collect samples that have a higher or lower value than the true population value of 0.093, and the sample statistics are each different from one another. This is an example of sampling variability. However, taking only three samples does not give us a very good understanding of the behavior of the sample proportions in repeated sampling. We need to see what happens if we repeat the sampling process many, many more times. If the study with random samples of size n = 100 is repeated 1,000 times (that is, collect 1,000 samples), and the calculated sample proportion for each sample is plotted, a dot plot of the sample proportions would look like this. If we repeat our study over and over indefinitely, the collected sample proportions would form the sampling distribution of sample proportions – defined as the distribution of proportions from all possible samples of this size. In general, whenever a distribution is made up of sample statistics (e.g., means, medians, standard deviations), the distribution is called a sampling distribution of that statistic. What do we notice about the behavior of the sample proportions over repeated random samples? Recall that when we evaluate distributions of numerical data, we look at the shape, the center, and the spread. The shape of the distribution of sample proportions is approximately bell-shaped, with the center being around approximately 0.093 (the population proportion), and the range of values spanning 0.02 to 0.18, with the average deviation away from the mean equaling 0.029. Recall that the average deviation away from the mean is another way of saying standard deviation. When we are describing the sampling distribution, we call the standard deviation of the sample statistics standard error (denoted std. error in the plot).But, what would have happened to the sampling distribution of sample proportions if the samples each had n = 500 instead of n = 100 observations? The dot plot above shows the sampling distribution of that scenario. From this dot plot, we see that the shape is still approximately bell-shaped and the center is still at the population proportion of 0.093. However, compared to the sampling distribution of sample proportions from samples of size 100, the sampling distribution of sample proportions for samples of size 500 has a much smaller spread. The sample proportions on the previous slide ranged from 0.02 to 0.18, and in this case the sample proportions range in values from around 0.06 to about 0.14 and have a standard error of 0.013.To drive the point home, let’s examine one more scenario. What do you think would happen if samples each had n = 1,000 observations, instead of n = 500 or n = 100? The dot plot above shows the sampling distribution of that scenario. Similar to the last two dot plots, we still see that the shape is approximately bell-shaped (but even more bell-shaped than the previous two) and the center is at the population proportion of 0.093. But now, the spread of the sample proportions is even smaller. The sample proportions range in values from around 0.064 to 0.125 and have a standard error of 0.0093.What general behaviors were noticed as we examined the three sampling distributions of sample proportions? The first behavior pertains to shape. As the sample size (n) increased, the sampling distribution of sample proportions looked more bell-shaped. This bell-shaped pattern is observed in the sampling distributions for many statistics (but not all) and will play an important role in the majority of inferential methods we use in this course. The second pattern observed is that the center of the sampling distribution was always located near the population parameter that we are trying to estimate. It would be exactly on the population parameter had we sampled an infinite number of times (which is the precise definition of sampling distributions), but because there were only 1,000 samples in each of the dot plots (which was done for demonstration sake), the center was not always quite equal to the population parameter. This idea of the center of the sampling distribution being equal to the population parameter is important because we want to know we are “hitting” our target–the parameter of interest–on average across all possible samples. That is, our statistic is neither overestimating nor underestimating the value it is trying to estimate; but rather, it is hitting the parameter of interest, on average. Lastly, as the sample size increased, the width or spread of the sampling distribution decreased. That is, as n increased, the sample statistics tended to be closer to the true population parameter value, thus making the variability of the sample statistics smaller. Besides the shape of the distribution, knowing how the statistics vary from sample to sample (i.e., the spread of the sampling distribution) is what we really care about. We use this information in making an inference from the sample to the population because it tells us how precise an estimate is. Let’s review the different distribution types that have been presented thus far: population distributions, sample distributions, and sampling distributions. Recall from a previous lecture that population distribution describes the distribution of a characteristic of that population. It displays the proportion or probability of the values that make up the characteristic. As we learned in this lecture, typically, this distribution is unknown, and information about that distribution (e.g., population parameters; population proportion p) are often what we want to try to estimate. In the Diabetes example, we assumed the population proportion of people in the United States who have diabetes was 0.093. However, what we most likely have access to is a sample of data from the population. If we plot the data from our sample, we create a sample distribution, which is a distribution of a characteristic of the sample. It is similar to the population distribution but it most likely will have different proportions of the values than the population. We use the information from the known and observed sample to help us estimate the unknown population value/parameter of interest. Unlike population and sample distributions, which are distributions of cases or observations, sampling distributions are distributions of statistics, where each “dot” (a.k.a., sample statistic) is aggregate information from a sample of observations and many, many, many ”dots” (a.k.a., sample statistics) are obtained so we can understand the behavior of the ”dots”. While we can simulate what the sampling distribution will look like (as we have in this lecture), we do not observe this distribution directly in real life. This distribution is an abstraction of what would happen if we could mimic the sampling behavior over and over again. It helps us understand the sampling variability of the statistics so that we can make an inference about the population from the sample. While sample distributions are an important part of the data analysis process, sampling distributions are the foundation for statistical inference (as mentioned earlier).The concept of sampling distributions for sample proportions was presented in this lecture. But, any statistic that we calculate from a sample has its own sampling distribution. For example, there is a sampling distribution for sample means (which will be discussed in a future lecture), or for sample relative risks. As we discussed earlier, many statistics have sampling distributions that are bell-shaped, but some statistics have sampling distributions that are not bell-shaped (e.g., sample median, sample standard deviation, sample relative risk, sample odds ratio). References: [CDC2014]: https://www.cdc.gov/media/releases/2014/p0610-diabetes-report.html "],["confidence-intervals.html", "Chapter 5 Confidence Intervals", " Chapter 5 Confidence Intervals A sample proportion is a “point estimate” of a population proportion. In contrast, a confidence interval for a proportion is an “interval estimate” of a population proportion that reflects the precision of the estimate. A confidence interval gives a plausible range for a population proportion based on observed data. Confidence intervals can also be used to make inferences about a population proportion. This lecture will introduce the concept of confidence intervals, focusing on confidence intervals for a proportion. We learned earlier that theoretical distributions and their parameters are useful when we are trying to understand the probability of observing various outcomes in random samples given a known characteristic of the population. For example, if we know the true prevalence of diabetes in the population, we can use the Binomial distribution to say things about random samples from that population. This is an example of the upper arrow in the figure above. However, when analyzing data, we have a single sample in hand that we want to use to say something about the population it came from. For example, if we calculate the proportion of people with diabetes in a sample, what does this tell us about the proportion with diabetes in the entire population? This would be an example of the lower arrow in the figure above: using information about the sample to find out about the population. How do we do this? The key, as we will see, is to use sampling distributions! Suppose we are interested in understanding the prevalence of diagnosed diabetes in younger adults aged 20 - 44 years old in the United States. How would we obtain an estimate of this number? How confident are we that we have indeed estimated accurately the percent of people in this age group with diabetes in the United States? Let’s assume that we have on hand a representative sample of 500 Americans between 20 and 44. Now suppose we ask each one “Have you been diagnosed with diabetes”, and 53 of them say “Yes”. If we divide 53 by 500 we obtain the sample proportion, the proportion of younger adults in our sample who have diabetes, in this case 10.6%. The sample proportion, which we refer to as p-hat, is an estimate of the true proportion, p, of diabetic younger adults in the entire US population. It may be somewhat lower than the true population proportion, or it may be somewhat higher. But how much higher or lower is plausible given our sample? Getting an estimate of 10.6% from a sample of 500 seems pretty unlikely if the true population proportion is, say, 50%. But is this estimate consistent with a true population proportion of, say, 14%? We can use confidence intervals to tell us. A confidence interval is an interval estimate that tells us about the precision of the estimate. Loosely, we can think of it as a range of true population parameter values that are plausible given the observed data. Without worrying for now about how we calculated it, a 95% confidence interval for the true population proportion from our diabetes study is 7.9% to 13.3%. This interval is telling us that true population proportions between 7.9% and 13.3% are relatively plausible given our observed sample proportion of 10.6%. There are a few ways of writing out the confidence interval, including “lower CI value” to “upper CI value” (e.g., 7.9% to 13.3%), or placing parentheses around the two CI values and separating them with a comma (e.g., (7.9%, 13.3%)). Confidence intervals for a proportion can denoted either as proportions or as percents. So what does the 95% mean? Informally, it quantifies how confident we are that the true population proportion falls in the given range. We?ll give a more formal definition shortly. So how did we calculate this confidence interval from the data?Determining a confidence interval requires us to invert or “flip” our thinking, to go from “known” sample to “unknown” population (instead of “known” population to “unknown” sample as we did when we explored sampling distributions). Let’s think about the sampling distribution of sample proportions again. The dot plot above shows the values of the sample proportion, p-hat, calculated for each of 1,000 samples, each with size n=500, from a population that has a known population proportion, p, of 0.093 or 9.3%. Previously, we saw that the sampling distribution for p-hat (the sample proportion) is approximately Normal (a.k.a. bell-shaped) with a mean equal to the true population proportion of 0.093 and a standard error (i.e., standard deviation of the sampling distribution) equal to 0.013, which mathematically works out to be the square root of \\(\\frac{p(1-p)}{n}\\). It is important to recall that the standard error (denoted as SE) is just the “fancy” name denoting a special case of the standard deviation (denoted as SD). The standard error is the standard deviation when applied to a sampling distribution for a statistic. But it?s still a standard deviation (period). Using the term “standard error” just alerts us that it describes a sampling distribution and not the population or the sample observations (our data), which would both be described with the term “standard deviation”. Since the sample size, n, is big here, the observed sample proportions are pretty close to being a Normal distribution. We can see that this is the case by observing the Normal curve that is laid over the dots in the plot and noting how similar they are. This Normal curve has the same mean and standard deviation (a.k.a. standard error) as the sampling distribution; that is, it has a mean of 0.093 and a standard deviation of 0.013. So, using this Normal approximation, what can we say about the likely values of p-hat? Using the 68-95-99.7 rule of Normal distributions, roughly 68% of the sample proportions will fall within one standard deviation (a.k.a. one standard error) of the mean of 0.093, and about 95% of them will fall within two standard deviations (a.k.a. two standard errors) of the mean of 0.093. Building on the “within 1 or 2 SE from the mean” idea, let’s see what the cutoff values for these sample proportions would be. The dot plot on the previous slide has been removed from the plot and all that remains is the Normal curve with same mean of 0.093, the true population proportion, and the same standard error of 0.013. This is depicted in the top curve on the slide. If we wanted the middle 68% of the sample proportions, we would take p, the mean of the sampling distribution for sample proportions, plus or minus 1 * SE. So the middle 68% of the sample proportions are between 0.080 and 0.106. Similarly, the middle approximately 95% of the sample proportions are between 0.067 and 0.119, which is p plus or minus 2 * SE. In fact, we can use the properties of Normal distributions to tell us how many standard errors above and below 0.093 we have to go to contain any arbitrary percentage of the data. For example, it turns out that a range of plus or minus 1.64 standard errors from the mean contains approximately 90% of the data, so in our case, about 90% of the sample proportions should fall between 0.072 and 0.114. Recall that we can standardize data by calculating ?z-scores? that tell us how many standard deviations the observation is from the mean. If the data are statistics, such as sample proportions, then the z-score tells us how many standard errors away from the mean the sample proportion is. The same sampling distribution for the sample proportion is shown in the lower curve on the slide, in standardized form with the z-score on the horizontal axis. Note that the product of the value of the z-score and the standard error is known as the margin of error. Using what we know about where values of p-hat are likely to fall given the true value of p, we can ?flip? our thinking and say something about where p might lie based on the observed value of p-hat. That is, we have just seen that 95% of the sample proportions will lie within ~2SE of the true proportion. Flipping this around tells us that for any given sample proportion, there is a 95% chance that the true proportion will lie within ~2SE of it (and a 5% chance that the true proportion will be further than ~2SE from it). This leads us to the formula for the confidence interval of a proportion: p-hat +/- its margin of error. The margin of error consists of two parts: the z-value, which is determined by the degree of confidence we want, and the standard error of p-hat. Notice that we?ve dressed up the z-value term a little by adding the subscript 1 ? alpha/2. This subscript indicates the appropriate z-value corresponding to a specified level of confidence. For instance, for a 95% confidence interval, alpha equals 1 ? 0.95 or 0.05. Therefore, 1 ? alpha/2 equals 0.975 which tells us that our z-value should be the number that exceeds 97.5% of the observations in a Standard Normal distribution. It turns out that this number is about 1.96. Next we move on to the second component, the standard error of the sample proportion, p-hat. It turns out that the formula for this standard error involves the true population parameter, p. However, this is a problem since p is unknown! What do we do? We simply replace p with its estimate, p-hat. This results in an estimated standard error rather than the true standard error, which uses p. For our example, the sample proportion of young adults with diagnosed diabetes was 53/500 = 0.106, so p-hat = 0.106 and the estimated SE(p-hat) = 0.014 (Notice that the estimated SE of p-hat is slightly different than the true SE of p-hat on slide 5.) For a 95% confidence interval, alpha = 0.05 and the z-value is 1.96. This gives a margin of error of 1.96*(0.014) = 0.027. Our 95% confidence interval for the true population proportion of young adults who have diabetes is therefore 0.106 +/- 0.027, which gives (0.079, 0.133). Now, how do we interpret this range? We can say things like: ?I am 95% confident that the interval between 0.079 and 0.133 contains the true proportion of US young adults with diabetes.? Or ?A plausible range of values for the true proportion of US young adults with diabetes is 0.079 to 0.133.? But these interpretations are somewhat ambiguous. What does ?95% confident? mean? How do we define ?plausible?? Next, we will define precisely what we mean by a 95% (or any %) confidence interval.This plot shows our sample proportion, 0.106 (black dot) and the true population proportion, 0.093 (solid red vertical line). We see that our sample proportion, while not the same as the population proportion, is close. In this case, the 95% confidence interval (or CI) constructed from our one sample contains the true population proportion. How often will this be the case?Let’s say we were able to obtain 50 different samples of size n=500 from the same population. From each sample, we calculate a confidence interval for the true population proportion. The plot above shows 95% confidence intervals for 50 hypothetical random samples taken from our US young adult population where the true proportion of diabetes is 0.093 or 9.3%. We see that the sample proportions, p-hat, vary a bit from sample to sample as indicated by the dots in the center of each interval. For 47 of the samples, the calculated 95% confidence interval contains the true population proportion of 0.093. But for three of the samples, the calculated confidence interval misses the true population proportion (as indicated by the red horizontal lines in the plot above). And, lo and behold, 47 out of 50 is 94%, which is very close to the 95% confidence level we used to calculate the confidence intervals. This isn?t a coincidence. If we repeated the study and obtained 1,000 samples of size n=500, instead of 50 samples, we would get 1,000 slightly different point estimates and 1,000 slightly different confidence intervals, due to sampling variability. Those calculated confidence intervals would contain the true population proportion of 0.093 in about 950 (or 95%) of the samples, but would miss the true value in about 50 (or 5%) of the samples. This plot tells us how to precisely define a 95% confidence interval: A 95% confidence interval is an interval such that, when estimated on repeated samples, approximately 95% of those estimated intervals will contain the true population parameter and approximately 5% will miss it. 90% confidence intervals will “hit” 90% of the time and “miss” 10% of the time; 99% confidence intervals will hit 99% of the time and miss 1% of the time, and so on.What if we wanted to be more than 95% confident? If we wished to be 99% confident, then our confidence interval would need to be wider, so that 99% of the time it would “catch” the true population value and only 1% of the time it would “miss” it. A confidence interval is kind of like a butterfly net: a wider net has a better chance of catching the butterfly. In our imaginary study, a 99% confidence interval for the true proportion of US young adults with diabetes is 0.071 to 0.141. What if we wished to be less than 95% confident? If we wished to be 90% confident, then our confidence interval would need to be narrower, so that it only “catches” the true population value 90% of the time. An interval with a lower level of confidence will produce a confidence interval that is narrower than one with a higher level of confidence. Recall that the z-value in the confidence interval calculation is determined by the desired confidence level. The z-values for three common confidence levels are: 1.645 for a 90% confidence level, 1.96 for a 95% confidence level, and 2.575 for a 99% confidence level. Increasing the confidence level is a double-edged sword. On one hand, we can feel more confident that our interval will contain the truth. But on the other hand, our interval will be wider, so it gives us less information about where the true value lies. Conversely, decreasing the confidence level makes it more likely that we will “miss” the truth, but in exchange our intervals will be narrower. So, where does all this leave us when we are staring at one confidence interval calculated from the one sample we have on hand? Since we don?t know the true population parameter, we don’t know whether this particular interval has “hit” or “missed” the true value. But, since we know that confidence intervals will “hit” the true value a relatively high percentage of the time (a percentage of the time that we determine by fixing the confidence level), we take a sort of “leap of faith” and view the interval as a plausible range for the true population parameter. We have mentioned several times the idea of “flipping” our thinking when it comes to confidence intervals. To recap, the “flipping” part comes when instead of thinking about the variation among the proportions from many samples when we “know” the population proportion, we are thinking about estimating an “unknown” population proportion from a single sample. Let’s connect the concept of the sampling distribution for a proportion (when it is approximately Normal) to the concept of confidence intervals. The same image of multiple confidence intervals estimated from multiple samples is presented as from a previous slide. But this time, the sampling distribution for a proportion with mean 0.093 and standard error of 0.013 is overlaid on top of the confidence intervals. The dark blue vertical lines at 0.068 and 0.118 (that is, at ~2SE below and above the mean) denote the middle 95% of the sample proportions in the sampling distribution. That is, 95% of the sample proportions fall within ~2 standard errors of the mean, the population proportion, but 5% of the sample proportions are outside of this region. So while most of the sample proportions are “near” the population proportion, just by chance we might get a sample proportion that is “far away” from the population proportion. Notice that the 47 sample proportions (blue dots) that are within the middle 95% of the sampling distribution have a confidence interval that contains the population proportion. In contrast, the 3 sample proportions (red dots) that are not within this region have a confidence interval that doesn’t contain the population proportion. To put it all together now, based on the sampling distribution of sample proportions, we know that for 95% of the sample proportions (the ones in the middle of the sampling distribution) the true population proportion is going to be within ~2SE of that sample proportion, and for the other 5% of the sample proportions (the ones in the tails), the true population proportion is going to be further than ~2SE from that sample proportion. So if we put an interval of +/- ~2SE around”any”observed sample proportion, then 95% of the time, this interval will contain the true population proportion.A confidence interval method is a “recipe” for calculating a confidence interval based on observed data. And just as in cooking, there are different recipes for making the same dish, and each recipe has its own strengths and weaknesses. The Wald method is the one we have just described. It generally works fairly well, but becomes less accurate when the sample size is small. The modified Wald method involves a small tweak to the standard confidence interval formula that improves its behavior in small samples. For more information on this method, see the following website: https://www.graphpad.com/support/faq/the-modified-wald-method-for-computing-the-confidence-interval-of-a-proportion/. Because they are calculated by adding and subtracting the margin of error from the sample proportion, both the Wald and modified Wald methods generate symmetric confidence intervals. This can be a problem if the sample size is very small or the estimated proportion is close to 0 or 1, because then the confidence interval may include values below zero or above 1, which are impossible values for a population proportion. An alternative confidence interval recipe that avoids this problem is the Clopper-Pearson Exact Binomial method. The word “exact” refers to the fact that this method does not rely on the Normal distribution approximation; instead, it uses the fact that the sampling distribution of the sample proportion is a re-scaled version of the Binomial distribution. Confidence intervals estimated using the Clopper-Pearson method may not be symmetric, but will never contain values outside the range of 0 to 1. Like most exact methods, Clopper-Pearson confidence intervals usually require a computer to calculate; they cannot be easily hand-calculated like Wald confidence intervals. See the following website if you are interested in the formula: https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/PASS/Confidence_Intervals_for_One_Proportion.pdf. No matter which recipe is used, as the size of the sample increases, the confidence interval gets narrower, which means that the estimate is more precise. Recall that the process of using a sample proportion to estimate a population proportion is an example of statistical inference. Statistical inference for a proportion relies on several assumptions. We assume that the sample is a random (or representative) sample from the population of interest. In this way, we are able to generalize the results from this sample to the population it came from. This assumption would be violated, for example, if a sample was obtained in a way that under-represented people of color. The prevalence of diabetes is higher in people of color than in whites, so the results from a non-representative sample would not be generalizable to the U.S. young adult population as a whole. We assume that the observations are independent. This assumption would be violated, for example, if siblings were included in the sample. Siblings may share genetic and environmental risk factors for diabetes and so might be more similar to each other than two people chosen at random would be. If we are using the Wald method, we assume that the sample is “large enough” for the sampling distribution of sample proportions to be approximately Normal. For proportions, “large enough” requires at least 5 events (or “successes”) and at least 5 non-events (or “failures”). In our example, this assumption would be met if our sample contained at least 5 people with diabetes and at least 5 people without diabetes. If the sample size is too small, consider using other methods, such as the exact binomial method, to compute confidence intervals. If these assumptions are violated, the confidence intervals we calculate may give us faulty information about the true population proportion. For instance, intervals might be too narrow, suggesting a more precise estimate than we actually have, or they might be centered at the wrong place, and hence more likely to “miss” the true population proportion. "],["bootstrapping.html", "Chapter 6 Bootstrapping", " Chapter 6 Bootstrapping In this chapter, we will explore a technique known as bootstrapping, which can be used to obtain confidence intervals for parameters (such as medians) for which the statistic does not have a sampling distribution that is Normally distributed. Bootstrapping uses the sample itself as a “stand-in” for the population, and repeatedly samples from the sample to create an approximate sampling distribution for the statistic of interest. This approximate sampling distribution can then be used to estimate a confidence interval for the population parameter. In earlier lectures, we explored the behavior of the sampling distributions for sample proportions and for sample means. However, any statistic that we calculate from a sample will have a sampling distribution. For example, there is a sampling distribution for sample medians, and a sampling distribution for sample relative risks. The sampling distribution for a statistic simply describes how that particular statistic (such as the median or the relative risk) varies from sample to sample. As a general rule, the sampling distributions of other statistics besides means and proportions are usually NOT Normal and the Central Limit Theorem does NOT apply to them. Some of the other statistics have known parametric sampling distributions (e.g., sample slope), but others have sampling distributions that aren’t known. But we can still create confidence intervals for the population parameter even if we do not know anything about the sampling distribution for the sample statistic. Let’s explore how to get a confidence interval for other statistics, focusing on the median as the “other” statistic in this lecture. Let’s start by thinking about a sample. We have seen that, if we take a random (representative) sample from a population, the summary statistic from that sample will be a good estimate of the population parameter. If our sample is “large enough”, the shape and variability of the sample will also be similar to the population. Let’s look at an example. On the left, we see the population distribution for systolic blood pressure (SBP) in US adults*. On the right, we see a dotplot of the SBP from the Blood Pressure dataset, which contains a sample of 500 people. While the sample distribution isn’t exactly the same as the population distribution, the shape, center, and variability are similar: we see a few people with SBP below 100 mmHg, a peak around 140 mmHg, and then a more gradual decrease until about 200 mmHg, with a few above 200 mmHg. Suppose that we didn’t have information about the population and we only had information from the 500 individuals in the Blood Pressure dataset. The median SBP in this sample is 140.5 mmHg. This is a point estimate of the population median SBP. If we wanted to better estimate the population median SBP, how can we obtain a confidence interval for this value? Reference: *These SBP measurements were obtained from 5209 people as part of the Framingham Heart Study (http://www.framinghamheartstudy.org). This population may not be exactly like the entire US population in all respects. Add citation for the data for the sample, if different. Confidence intervals for a mean or a proportion are based on the assumption that the sampling distribution for the statistic (mean or proportion) is Normally or approximately Normally distributed. For other statistics, such as the median, we cannot make that assumption. If we could only take a very large number of samples repeatedly from the population of interest, calculate the statistic of interest for each sample (e.g., sample median), and obtain the sampling distribution for the statistic, we could understand the sampling variability behavior of the statistics. Unfortunately, in most cases we don’t have complete data from the population of interest. (If we did, we wouldn’t need to do statistical inference.) But we do know that a single sample can be a good approximation of the population under certain circumstances: if it is representative of the population, if the observations are independent and measured accurately, and if the sample size is “large enough”. If our sample is a good approximation of the population, we should be able to take a very large number of samples from our sample (a.k.a. our “approximate” population) to approximate the sampling distribution! Bradley Efron introduced what is called “the bootstrap” in 1979, when he formalized the idea of using repeated sampling from the sample itself to approximate the sampling distribution1. The idea behind the bootstrap is simple: we use the sample to approximate the unknown population, and then sample repeatedly from it to obtain an approximate sampling distribution for our statistic. Our bootstrap or approximate sampling distribution will probably not have exactly the same center as the true sampling distribution (or the population), since it will be centered at the sample value. It will also probably not have exactly the same variability as the true sampling distribution, but it will be close, provided the original sample is large enough. “Large enough” for bootstrap confidence intervals means sample sizes larger than about n=50. For smaller samples, the bootstrap distribution may underestimate the true variability, leading to confidence intervals that are too narrow and “miss” the true value more often than they should. References: 1 A very accessible (to non-statisticians) description of the bootstrap can be found in Bradley Efron and Robert Tibshirani (1991) “Statistical Data Analysis in the Computer Age”, Science 253(5018), pp390-395. A somewhat more statistical description of the bootstrap can be found in Brad Efron and Rob Tibshirani (1986) “Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy”, Statistical Science 1(1), pp 54-77. So how exactly is this done? (Note that this is carried out by computer software, not by hand!) First the computer takes a sample of size n from the original sample (Note: n is the same sample size as the original sample). This new sample is called a “bootstrap sample”. We sample with replacement, which means that once we have randomly chosen an observation, we put it back in the dataset so that it could be chosen again. This means that in a given bootstrap sample, some observations in the sample will be represented more than once and others may not be present at all. This will result in the bootstrap samples differing slightly from each other, in ways that reflect the variability in the original sample. (If we sampled without replacement, there would be only one way to get a bootstrap sample of size n from an original sample of size n, which wouldn’t give us any information about variability.) We then calculate the statistic of interest (in our example, the median) for the bootstrap sample. The computer repeats the process of taking a bootstrap sample and calculating the statistic of interest thousands or tens of thousands of times. (This takes next to no time with modern computing power.) This gives us a “bootstrap distribution” or approximate sampling distribution for the statistic of interest. There are a number of ways of obtaining a confidence interval from the bootstrap distribution. A common one is to use the percentiles of the distribution directly. Using this approach, a 95% confidence interval for the median would span the middle 95% of the bootstrapped medians, from the 2.5% value to the 97.5% value. Note: Bootstrapping is an example of a nonparametric statistical method. Nonparametric methods rely only on the sample to make inference about the population. They do not require that we know the distribution (and parameters for that distribution) of the population or of the true sampling distribution. Let’s look at an example. As we said a few slides ago, the median SBP level in our Blood Pressure dataset is 140.5 mmHg. This is a point estimate of the median SBP level in the population of all US adults. How can we determine how precise our estimate is? How can we obtain an interval estimate, a confidence interval, for the true population median SBP level? Using bootstrapping, we sample with replacement repeatedly from our one original sample of size n=500. Three of those bootstrap samples are presented at the top of the slide. As you can see, each of the bootstrap samples have a slightly different distribution and a different median. We are mimicking the process of repeated sampling from the stand-in or “approximate” population (a.k.a. our sample) to build up the sampling distribution of the median. If we obtain 10,000 bootstrap samples of size n= 500 and obtain the median for each sample, we end up with the plot shown at the bottom of the slide. This bootstrap distribution of the sample median has a center (mean) at 140.3 and standard error of 1.35.To find a 95% confidence interval, we have the computer put the 10,000 sample medians in order from smallest to largest, and find the lower and upper values of the middle 95% of the median SBP values. This corresponds to the 2.5th percentile, or 251 values above the minimum value, and the 97.5th percentile, which is 251 values below the maximum value. We obtain a 95% confidence interval for the population median SBP of 138 to 142 mmHg. We interpret this confidence interval the same way as we did for estimating the population proportion and the population mean. While the bootstrap technique was designed to obtain confidence intervals when the form of the sampling distribution of the sample statistic is unknown, the bootstrap can be used to find confidence intervals for any parameter, including means and proportions. Let’s see what happens if we calculate a bootstrap confidence interval for the mean SBP in our example. The sample mean from the Blood Pressure example is 144.95 mmHg. When we calculate the mean instead of the median for each of our 10,000 bootstrap samples, we obtain the bootstrap distribution for the sample mean shown here. This time, the bootstrap distribution looks quite Normally distributed. This is what we would expect, since we know from the Central Limit Theorem that the sampling distribution for a sample mean is Normal. The bootstrap distribution mirrors the sampling distribution in shape and spread, but is centered on the sample mean instead of the population mean. If we order the 10,000 sample means and find the 2.5th percentile and the 97.5th percentile, we obtain a bootstrapped 95% confidence interval for the population mean SBP of 142.56 to 147.44 mmHg. How does this result compare with a 95% confidence interval calculated the usual way, using the Central Limit Theorem? The sample size is 500, the sample mean is 144.95 mmHg, and the sample standard deviation is 27.99 mmHg. Our degrees of freedom are 499, which results in a t-value of 1.965. Putting these values into the formula, a 95% CI is xbar +/- tSE = 144.95 +/- (1.965)(27.99/sqrt(500)) = 144.95 +/- 2.46 . We end up with a 95% confidence interval that ranges from 142.49 to 147.41 mmHg, which is nearly identical to the bootstrap CI above. We see that when we use the bootstrap method to calculate a confidence interval for the mean, the resulting interval is almost identical to the interval based on the Central Limit Theorem, which is reassuring! "],["sampling-distribution-of-means.html", "Chapter 7 Sampling Distribution of Means", " Chapter 7 Sampling Distribution of Means Earlier we described the behavior of sampling distributions for a sample proportion, in order to understand how we can use a sample proportion to make inferences about a population proportion. In this lecture, we will describe the behavior of sampling distributions for a sample mean, in order to understand how we can use a sample mean to make inferences about a population mean. Sampling distributions for sample means behave similarly to those for sample proportions, but there are some important differences.Suppose we want to know the average body mass index (BMI) of US adults whose age is between 30 and 60 years old. Suppose that BMI measurements could somehow be collected from all US adults between those age ranges. This plot shows results that might be obtained from such a census*. A plot that shows the measurement results for an entire population is known as a population distribution. The BMI measurements in this census fall between 14 and 57 kg/m2. Half of the adults have BMI values less than the median BMI of 25.13 kg/m2, and the mean BMI was 25.58 kg/m2. The shape of the population distribution of BMI values in US adults appears to be more or less Normal but skewed to the right (or positively skewed), with a long right tail of very high BMI values. References: *These BMI measurements were obtained from 5209 people as part of the Framingham Heart Study (http://www.framinghamheartstudy.org). This population may not be exactly like the entire US population in all respects.It is not usually possible or feasible to obtain measurements on the entire population of interest. Rather, we usually obtain a sample of observations. Suppose we conduct a study to investigate the average BMI of US adults, aged 30-60: we obtain a single sample, a simple random (and therefore representative) sample of n=100 adults from the population shown on the previous slide. A dot plot for the 100 BMI values in our sample is shown here, with one dot for each person in the sample. A plot that shows all of the measurement results from a sample is called a sample distribution. The mean BMI in our sample is 25.47 kg/m2, which is slightly lower than the true population mean of 25.58 kg/m2. What would the sample means look like if we repeated the sampling process a few more times? Suppose we obtain three more random samples of 100 adults from the same population. The sample distributions of BMI values for these three samples are shown here. Each of the sample distributions has a somewhat different shape, a somewhat different mean, and a somewhat different standard deviation, as we would expect due to sampling variability. However, collecting only three samples does not give us a very complete understanding of the behavior of the sample means in repeated sampling. Let’s see what happens if we repeat the sampling process many, many more times. If we repeat the sampling process 1,000 times, collecting samples of size n=100 each time, and we calculate the mean BMI from each sample, a dot plot of all 1,000 sample means would look like this. Recall that true sampling distributions consist of the sample statistics from all possible samples. This distribution of only 1,000 sample means is an approximation of the true sampling distribution for samples of size n=100. We are using this approximate sampling distribution to help us better understand the concept of sampling distributions. What shape does this sampling distribution have? The shape appears very close to Normal (bell-shaped). Surprisingly, even though the underlying population of BMI values in US adults is right-skewed, the sampling distribution of sample means from repeated samples of size n=100 appears to be very close to symmetric. Where is the center of this sampling distribution? The mean of this sampling distribution (25.59 kg/m2) is very close to the true population mean of 25.58 kg/m2. This discrepancy is because we only took 1,000 samples and not an infinite number of samples. If we were able to take an infinite number of samples, the mean of the sampling distribution would exactly equal the true population mean. What about this sampling distribution’s spread? The sample means range from about 24 to 27 kg/m2. The standard deviation (or standard error) of this sampling distribution is 0.42 kg/m2. What would happen if we repeated the sampling process, but this time we used a much larger sample size of n=500 instead? A dot plot of all 1,000 sample means would look like this. How has increasing the sample size from n=100 on the previous slide to n=500 on this slide affected the sampling distribution? The shape of this sampling distribution has not changed a lot. It is still very close to Normal: bell-shaped, unimodal and symmetric. The center of this sampling distribution, given by the mean of 25.58 kg/m2 , which equals the true population mean of 25.58 kg/m2. The biggest difference is in the sampling distribution’s spread. Before, with a sample size of n=100, the sample means ranged from about 24 to 27 and the standard error of the sampling distribution was 0.42 kg/m2. Now, with a sample size of n=500, the sample means range from about 25 to 26 and the SE is 0.19 kg/m2. With a larger sample size, the sampling distribution is narrower. With a larger sample, the sample means from the various samples are all much more closely clustered around the true population mean. Note that the number of samples is identical in both cases; it is only the sample size that differs. Increasing the number of samples that we take does not narrow the sampling distribution, although it does give us a clearer picture of its shape. It is only increasing the size of each sample taken which will narrow the sampling distribution.What would happen if the sample size were very small? Suppose we repeated the sampling process, but this time we used a very small sample size of n=20 instead? A dot plot of all 1,000 sample means would look like this. How has decreasing the sample size from n=100 on a previous slide to n=20 on this slide affected the sampling distribution? The shape of this sampling distribution has changed a bit. Unlike in previous slides, it has a slight right skew, but it’s fairly close to Normal: bell-shaped, unimodal and symmetric. The center of this sampling distribution, given by the mean of 25.61 kg/m2, is again very close to the true population mean of 25.58 kg/m2. The biggest difference is in the sampling distribution’s spread (notice the limits on the x-axis changed from 24 to 27.5 in the two previous slides to 22.5 to 29.5 on this slide). Before, with a sample size of n=100, the sample means ranged from about 24 to 27 and the standard error of the sampling distribution was 0.42 kg/m2. Now, with a sample size of n=20, the sample means range from about 22.5 to 29.5 and the SE is 0.95 kg/m2. With a SMALLER sample size, the sampling distribution is WIDER. With a SMALLER sample, the sample means from the various samples are MORE WIDELY SPREAD AROUND the true population mean. What general behaviors have we noticed as we examined the sampling distributions both for sample proportions and for sample means? The first behavior we observe pertains to shape. As the sample size (n) increases, the sampling distribution both for sample proportions and for sample means looks more bell-shaped, unimodal and symmetric? that is, more Normal in the statistical sense. The second pattern we observe is that the center of the sampling distribution is always located very close to the true population parameter, even for relatively small samples. Lastly, we notice that as the sample size increases, the width or spread of the sampling distribution decreases. That is, as the sample size, n, increases, the sample statistics tend to be closer to the true population parameter value, thus making the variability of the sample statistics smaller. What we have observed in these sampling distributions is actually a fundamental theorem in statistics, the Central Limit Theorem (frequently abbreviated as CLT). The Central Limit Theorem tells us two things. First, the Central Limit Theorem states that if we choose a large enough sample size, n, then the sampling distribution of the sample means will be approximately Normal (unimodal, symmetric and bell-shaped) EVEN IF the underlying population is not Normal at all (for example, multimodal or severely skewed). If the population distribution itself is Normal, then the sampling distribution of the sample means will be Normal for any sample size, even a sample size of n=1. The next logical question is ‘how large of a sample is ’large enough’ for the CLT to hold for the distribution of sample means’? That depends on the shape of the population distribution. If the shape of the population distribution is completely Normal (that is, if the measurement of interest is truly Normally distributed in the population), then a minimum of a sample size of 1 is needed in order for the distribution of sample means to be approximately Normal. In other words, if the population is truly Normal, then the sampling distribution will be Normal, for ANY size of sample. If the shape of the population distribution is not Normal but is approximately symmetric, then one rule of thumb is that a minimum sample size of about 15 is needed in order for the sampling distribution of sample means to be roughly Normal. If the shape of the population distribution is not Normal and not symmetric, but instead is skewed, then one rule of thumb is that a minimum sample size of about 30 is needed in order for the sampling distribution of sample means to be roughly Normal. Keep in mind, though, that the less Normal or the less symmetric the population distribution is, the larger the sample size that will be needed to ensure that the sampling distribution of sample means will be roughly Normal.The second thing the Central Limit Theorem tells us is that the sampling distribution of sample means will be centered at the true population mean, \\(\\mu\\) (mu), and will have a standard error, SE (sometimes called the standard error of the mean, SEM), equal to the population standard deviation, \\(\\sigma\\) (sigma), divided by the square root of the sample size, n.  This applies to the true sampling distribution, which is for all possible samples of size n. As previously mentioned, our example sampling distribution contained only 1,000 samples of size n=100, not all possible samples, so it is a good but not quite perfect approximation of the true sampling distribution. Its sample mean is nearly but not quite equal to the true population mean and its sample standard error is nearly but not quite equal to the population standard deviation divided by the square root of n. In our BMI example, the mean of our approximate sampling distribution after obtaining 1,000 samples of size n=100 (25.59 kg/m2) is very close to the true population mean of 25.58 kg/m2. The true population standard deviation is 4.24 kg/m2 and the sample size is 100, so the standard error, SE, of the sampling distribution should be 0.424 kg/m2, which is quite close to the value we saw in our approximate sampling distribution plot (0.42 kg/m2). An important thing to notice: the SE calculation shows us that as the sample size increases, the standard error decreases, and the sample means will get closer and closer to the population mean. This is consistent with what we observed earlier when we compared the sampling distributions for samples of size n=20, n=100 and n=500. Note that we do not use the Central Limit Theorem to get a better approximation of the population parameter of interest by using lots of samples. In most cases, it isn’t practical or even possible to re-sample from the same population thousands of times. In most cases, we only ever get one sample and we need to make our inferences about the population based on that one single sample. What we DO use the Central Limit Theorem for is to tell us how close our sample statistic is likely to be to the true population parameter, i.e. how well the sample statistic estimates the true population value. The Central Limit Theorem also applies to sample proportions. In this case, the CLT states that if we choose a large enough sample size, the sampling distribution of sample proportions from random samples of size n will be approximately Normal (unimodal, symmetric, and bell-shaped) and will be centered around the true population proportion, p, with a standard error equal to the square root of p*(1-p)/n. Totally optional, but if you are curious: The CLT only applies to means, but it happens to also work for proportions because a proportion is in fact a mean of a Bernoulli distribution of 1’s (e.g. diabetic) and 0’s (e.g. not diabetic). A Bernoulli distribution isn’t even remotely close to Normal in shape (see the earlier lecture on sampling distributions for a sample proportion), but the sampling distribution of sample proportions is nevertheless, per the CLT, approximately Normally distributed. Again, how large is ‘large enough’? Recall that we find the proportion for an event or category of interest. In our example from an earlier lecture, the ‘event’ we were interested in was the proportion of young adults with diagnosed diabetes. One common rule of thumb is that the sampling distribution of sample proportions will be approximately Normal if the number of events or ‘successes’ (n * p) and the number of non-events or ‘failures’ (n * (1-p)) are both at least 10. In our example, this assumption would be met if our sample contained at least 10 people with diabetes and at least 10 people without diabetes. If p, the population proportion, is unknown, then we use the sample proportion, p-hat, to estimate it. As we did earlier for proportions, let’s review the different distribution types that have been presented thus far for means: population distributions, sample distributions, and sampling distributions. The population distribution for our BMI example is shown in the upper left here. This describes the distribution of BMI values in the entire population of US adults. Typically, this distribution is unknown, and information about that distribution (e.g., population parameters such as the mean, mu (\\(\\mu\\))) are often what we want to try to estimate. However, what we most likely have access to is a single sample of data from the population. If we plot the data from our one sample, we create a sample distribution, which is a distribution of a characteristic in the sample. The sample distribution of BMI values for our one sample of size n=100 from US adults is shown in the upper right here. We can use the information from the known and observed sample to help us estimate the unknown population value/parameter of interest. Unlike population and sample distributions, which are distributions of cases or observations, sampling distributions are distributions of statistics, where each ‘dot’ (a.k.a., sample statistic) is aggregate information from a sample of observations and many, many, many ‘dots’ (a.k.a., sample statistics) are obtained so we can understand the behavior of the ‘dots’. The approximate sampling distribution for mean BMI values for samples of size n=100 is shown in the lower center here. While we can simulate what the sampling distribution will look like (as we have in this lecture), we do not observe this distribution directly in real life. This distribution is an abstraction of what would happen if we could repeat the sampling behavior over and over again. It helps us understand the sampling variability of the statistic so that we can make an inference about the population from the sample. While sample distributions are an important part of the data analysis process, sampling distributions are the foundation for statistical inference.We have now explored the behavior of the sampling distributions for sample proportions and for sample means. Both are approximately Normal under certain conditions, as given by the Central Limit Theorem. One key difference is that when we are considering proportions (e.g. the proportion of young adults with diabetes), there are only two parameters needed to describe the behavior of the sampling distribution: the population proportion, p, and the sample size, n. In contrast, when we are considering means (e.g., the mean BMI in US adults), there are three parameters needed to describe the behavior of the sampling distribution: the population mean, mu, the population standard deviation, sigma, and the sample size, n. "],["hypothesis-testing.html", "Chapter 8 Hypothesis Testing", " Chapter 8 Hypothesis Testing Hypothesis testing provides a standardized decision-making process for scientific research. It can be used in many situations: to compare one group’s result to a ‘standard’, to compare two groups to each other, or to compare among many groups. Hypothesis testing is used to compare means, to compare proportions, to compare odds ratios, to compare hazard ratios, and so on. In this lecture, we will introduce the logic behind hypothesis testing. In the next lecture, we will formally present a specific hypothesis test, the one sample t-test, which is used to compare one sample mean to a ‘standard’. To review, statistical inference is the process of concluding something about the value of an unknown population parameter (such as the true mean or true proportion) based on information from a known sample (such as the sample mean or sample proportion). There are two approaches to doing statistical inference. One approach is through estimating the unknown population parameter via confidence intervals using the known result from our sample (which we learned in an earlier lecture). The other approach is through hypothesis testing. So how is hypothesis testing different from using confidence intervals? In hypothesis testing, we first assume some claim about the population parameter and then use the result from our sample to gain insight into that statement. When we use confidence intervals, on the other hand, we make no assumption about what the population parameter is. The purpose of constructing confidence intervals is to estimate the population value from the result of our sample, whereas, the purpose of hypothesis testing is to determine whether there is enough evidence (from the result from our sample) to refute the assumed claim about the population value. You might be thinking, it seems odd to start with an assumed claim about the population value. Why would we start with that? To understand this, we have to understand the logic of hypothesis testing. The logic of hypothesis testing involves three big pieces: Hypotheses, which are statements or claims about the population parameter; Evidence, which involves summarizing sample data to provide evidence about the claims; and Evaluation, which involves making decisions about how usual or unusual the evidence is compared to the claim about the population parameter. A commonly used analogy for the logic of hypothesis testing is the US criminal justice system. Let’s use this analogy to better understand the pieces in the framework. A key principle of the US criminal justice system is ‘A defendant is presumed innocent until proven guilty’. In this statement, there are two competing hypotheses about the ‘truth’: – The defendant is innocent. - The defendant is guilty. The first hypothesis (in this case, innocence) is assumed to be true. This assumed claim about the ‘truth’ is also known as the null hypothesis, denoted by \\(H_0\\), H-sub-zero or H-naught. It is the claim that is to be tested. In general, the null hypothesis describes the default situation (the status quo or the ‘nothing interesting is happening’ situation). It is the thing that we are going to initially assume is true about the population(s). When we are comparing one population to a standard, the null hypothesis is that the value of the parameter of interest in the population is equal to some hypothesized value. When we are comparing two populations (for example, a treatment group and a control group), the null hypothesis is that there isn’t any difference: the parameter has the same value in both populations. (‘Null’ means amounting to nothing, none, absent, insignificant.) In contrast, the second hypothesis (in this case, guilt) is an alternative claim, a claim for which we seek significant evidence. The alternative hypothesis describes what we hope to demonstrate about the population(s) using the data. It is closely aligned with the research question. When we are comparing one population to a standard, the alternative hypothesis is that the value of the parameter of interest in the population is different than the hypothesized value. When we are comparing two populations (for example, a treatment group and a control group), the alternative hypothesis is that there IS a difference: the parameter does NOT have the same value in both populations. These two statements should be made prior to examining any evidence. It would be considered cheating if we used our evidence to determine the hypotheses. We need to start with some assumed ‘truth’ in order to compare the evidence to the claim. Using knowledge of research design and detective skills, evidence is collected and evaluated against the claims (hypotheses) that have been made. In the US justice system, this might involve interviewing witnesses, gathering physical evidence, analyzing forensic evidence, etc. In statistics, our ‘evidence’ is our sample of data, which we may have obtained through a survey, or through a clinical trial, or through some other kind of study. The data are summarized to provide a clearer picture of what evidence we have on hand. References: Image: https://www.kisspng.com/png-magnifying-glass-fingerprint-clip-art-kisspng-1155964/The last piece of the framework is to evaluate how usual or unusual the evidence is compared to the claim about the ‘truth’. From this evaluation, one of two situations could occur: We find that the evidence is unusual (is not likely to occur) compared to what is assumed to be true. We find that the evidence is usual (is likely to occur) compared to what is assumed to be true. In the US justice system, if the evidence is not consistent with (or is unusual compared to) the assumed claim of innocence, then we conclude there is evidence against the claim that the defendant is innocent (i.e., against the null hypothesis) and evidence for the claim that the defendant is guilty (i.e., in support of the alternative hypothesis). On the other hand, if the evidence is consistent with (or not unusual compared to) the assumed claim of innocence, then what we conclude is that there is no evidence for the claim that the defendant is guilty. CAUTION: We need to be careful with our wording with hypothesis testing. If the evidence is consistent with (or not unusual compared to) the assumed claim of innocence, then we do NOT conclude that there is evidence for the claim that the defendant is innocent (i.e., we do NOT ‘accept the null hypothesis’). Lack of evidence against the null hypothesis is not evidence for the null hypothesis. For example, a defendant may indeed have exceeded the speed limit, but if there were no witnesses and no speed measurements, then there won’t be any evidence of it. Lack of evidence doesn’t necessarily mean the defendant is innocent: it just means there isn’t any evidence of guilt. This is why the US justice system verdict is either ‘guilty’ (if there is sufficient evidence against innocence) or ‘not guilty’ (if there isn’t enough evidence against innocence); the verdict is never ‘innocent’. Evidence is used in a similar manner in statistics. We evaluate evidence by calculating the probability of observing the result we obtained given the assumption that the null hypothesis is true. That is, IF we assume the null hypothesis is true, how likely or unlikely would it be to see the result we obtained from our data just by random chance? We compare this value against some threshold to make a conclusion about the population. If the result we obtained from our data is NOT likely to occur if the null hypothesis statement is true, then something else must be going on. In this case, we say we have evidence against the null hypothesis and for the alternative hypothesis. Another common phrase for this is we reject the null hypothesis. On the other hand, if the result we obtained from our data IS likely to occur if the null hypothesis statement is true, then we say we lack evidence against the null hypothesis or for the alternative hypothesis, or we do not reject the null hypothesis. These three pieces are the framework for all hypothesis testing and will be used throughout this course. References: Image: https://www.pixcove.com/scales-balances-fairness-weighing-considering-justice-tilted-slanted-symbol-sign/ "],["hypotheis-testing-of-means.html", "Chapter 9 Hypotheis Testing of Means", " Chapter 9 Hypotheis Testing of Means Now we will formally present how to carry out a hypothesis test, by comparing a single mean to a ‘standard’ value. The context that will be used to formally learn about hypothesis testing is Mercury Content in Fish. High levels of mercury are harmful to ecosystems and humans because it is known to be highly toxic. As a result, many groups (including state health departments) monitor mercury levels in lakes. In particular, they monitor mercury content in fish, as high mercury levels can be a health hazard to humans who consume this food item (especially to women who are pregnant, nursing mothers, and young children). A study was carried out on 53 lakes in Florida to understand the mercury concentration in Florida sport fish. Samples of fish (largemouth bass) were collected from each of 53 lakes in Florida and the average mercury level in the fish for each lake was recorded (in parts per million; ppm). For simplicity’s sake, ‘mercury level’ will be used instead of ‘average mercury level’ for the remainder of this lecture, but remember that the mercury level in a lake is an aggregate measure from ALL of the fish sampled from that lake, and not for ONE fish. Summary statistics and plots of this data are presented above. Recall that when we examine a distribution, we focus on the shape, center, and spread. Multiple summary measures and displays are presented here to give as complete a picture about the data as a researcher might need (and to review previous topics). The mean mercury level in the 53 lakes was 0.53 ppm, and the average deviation away from the mean (the standard deviation, SD) was 0.34 ppm. The data appears to have a right, or positive, skew. References: Study: Lange, T. R., Royals, H. E., &amp; Connor, L. L. (1994). Mercury accumulation in largemouth bass (Micropterus salmoides) in a Florida lake. Archives of Environmental Contamination and Toxicology, 27(4), 466-471. Dataset: Lock, R.H., Lock, P.F., Lock Morgan, K., Lock, E.F., &amp; Lock, D.F. (2013). Statistics: Unlocking the power of data (1st ed.). Hoboken, NJ: John Wiley &amp; Sons, Inc.  Guidelines for ‘safe’ limits of mercury content in fish are set by the Food and Drug Administration (FDA) or its equivalent. The FDA in the US has determined that a ‘safe’ limit for mercury content in fish is 1.0 ppm (whereas, the limit set by the Canadian FDA is 0.5 ppm). Because the data were collected in the US, the researchers were interested in determining whether the average mercury level in largemouth bass in Florida lakes is acceptable by the US FDA standard. That is, is the average mercury level in largemouth bass in Florida lakes less than 1.0 ppm We saw that the study sample had a mean mercury level of 0.53 ppm, which is less than 1.0 ppm, but this may be due to sampling variability. The key question is: Is it a lot lower than what we would expect, taking sampling variability into account, if the population from which this sample was drawn truly had a mean mercury level of 1.0 ppm We will use hypothesis testing to help us answer this question. Before we can carry out a hypothesis test, we need to first evaluate the assumptions about the test. These assumptions are the same as those we’ve seen before, in calculating a confidence interval for a mean. That is: The sample should be a random (or representative) sample from the population, to allow us to generalize the results from this sample to the population it came from; The observations should be independent of one another (otherwise, we would use a different statistical inferential method that took dependent or correlated data into account); and The sampling distribution of sample means should be approximately Normal. How can we check this assumption We can check to see if the conditions for the Central Limit Theorem (CLT) hold. Recall that if the underlying population distribution is approximately Normal (which we can check by plotting the sample distribution), then the sampling distribution of sample means is approximately Normal. OR, even if the underlying population distribution is not Normal, if the sample size is ‘large enough’ (and what is ‘large enough’ depends on how heavily skewed the population distribution is), then the sampling distribution will still be approximately Normal. If this assumption is not met, use other methods for carrying out a hypothesis test for a mean (such as re-randomization tests). If these assumptions are not met, then the results of a hypothesis test will not be valid. Let’s check the assumptions for the Mercury Content in Fish example. Are the lakes a random or representative sample of all Florida lakes Based on the brief description of the dataset, it doesn’t seem that the lakes were randomly selected. Are they nevertheless representative of all lakes in Florida It seems reasonable to argue that the study authors would likely have chosen the 53 lakes to be representative of all lakes in Florida. Are the observations independent of one another Again, without more information, it is reasonable to assume that the mercury content in a given lake is not affected by the mercury level in the other lakes, so the 53 lakes will be independent of one another. Lastly, are the conditions met for the sampling distribution of sample means to be approximately Normally distributed Recall that the sample distribution of mercury levels in fish in the 53 lakes appeared to be skewed to the right. However, the sample size is 53, which is ‘large enough’. Thus, we can reasonably assume that the sampling distribution of sample means is approximately Normal. References: Image: https://www.pinclipart.com/pindetail/owRJbh_file-mw-icon-checkmark-svg-creative-commons-check/Now that we have examined the data and checked the assumptions for the test, we can move on to formally defining the hypotheses: the null and the alternative. We write them in terms of a population parameter of interest the population mean, \\(\\mu\\), in this case. As mentioned in a previous lecture, the null hypothesis defines the skeptical perspective or the ‘no difference’ situation. It is written as: the population mean, \\(\\mu\\), is equal to some specified value, \\(\\mu_0\\) (mu-naught), the null value. Where does this null value come from It is typically defined by the research question. The alternative hypothesis, on the other hand, is the competing claim. It is typically the statement we are interested in demonstrating. There are three different forms the alternative hypothesis can take. It can be written as either: - \\(mu\\) is not equal to \\(\\mu_0\\), or - \\(mu\\) is greater than \\(\\mu_0\\), or - \\(mu\\) is less than \\(\\mu_0\\), The last two statements, “greater than” and “less than”, result in one-tailed tests, because we are only interested in differences in one particular direction. In contrast, the “not equal to” hypothesis results in a two-tailed test, because we are interested in differences in either direction. The majority of the tests in research articles are two-tailed tests. How do we know which alternative hypothesis to choose for our hypothesis test Again, it is defined by the research question. In the Mercury Content in Fish example, recall that the research question is Is the mean mercury level in largemouth bass in Florida lakes less than 1.0 ppm Let’s use information from this research question to define the null value and the direction of the alternative hypothesis. We can infer from the wording of the research question that we are interested in comparing a mean to a ‘standard’ value, and that the standard or null value of interest is 1.0 ppm. Therefore, the null hypothesis for this example is that the true mean mercury level in largemouth bass in all Florida lakes is equal to 1.0 ppm. Or, using notation, mu = 1.0 ppm. Because the researchers were interested in determining whether there is evidence for ‘less than 1.0 ppm’, the alternative hypothesis is that the true mean mercury level in largemouth bass in all Florida lakes is less than 1.0 ppm. Or, using notation, mu &lt; 1.0 ppm. To evaluate the claims in our hypotheses, we need to gather evidence (data). We first summarize the data using exploratory data analysis (summary statistics, tables, graphs) and then calculate a test statistic to measure the compatibility between the result from the data and the null hypothesis. In general, this test statistic standardizes the result from the study to a known statistical distribution. When carrying out a test for a single mean (given that the assumptions are met), the appropriate test statistic is the t-statistic, which has a t-distribution with n - 1 degrees of freedom. It is found by calculating the difference between the sample mean and the null value and dividing by the standard error (which is the sample standard deviation, s, divided by the square root of the sample size, n). We can think of this value as measuring how far the result we found from our sample data is from what we would expect (given sampling variability) IF the null hypothesis was really true. It is expressed in units of standard error: for example, a t-statistic of +2.0 means that the observed sample mean is 2 standard errors above the null hypothesized population mean. Large test statistic values represent large (relative) differences between the sample result and the null value and small test statistic values represent small (relative) differences between the sample result and the null value. Because we are using the t-distribution to carry out this hypothesis test, it is often referred to to as a t-test (specifically, it is a one-sample t-test when a single mean is being tested). Does this t formula look similar to something we have seen before Remember the formula Z = (value - mean)/SD The two distributions, the t- and z- (or Standard Normal) distributions are very similar. When we take a value, subtract a mean from that value, and divide the whole thing by some standard deviation measure, we are standardizing the original value. That is, we are measuring how far a value is from the mean, in SD units (for z-scores) or SE units (for t-statistics). So why the t-distribution In a previous lecture about confidence intervals for a mean, the t-distribution was used because the sample standard deviation, s, was used in the equation instead of the population standard deviation, \\(\\sigma\\) (sigma). And using s as an estimate of sigma adds extra uncertainty because it is a statistic and will vary from sample to sample. Recall that the distribution that accounts for this extra uncertainty is the t-distribution. A similar argument is made when carrying out hypothesis testing for a mean. When we are trying to carry out inference for a single mean (using either confidence intervals or hypothesis testing), the t-distribution with n - 1 degrees of freedom is the appropriate statistical distribution. Returning to the Mercury Content in Fish example, let’s walk through all of the aspects within the evidence piece. Data were collected and we explored the data via summary statistics and various plots. We observed that the mean mercury level in largemouth bass in our sample of n=53 lakes was 0.53 ppm. Then we wanted to test whether this value is unusual if we assume that the true mean mercury level in Florida lakes is 1.0 ppm (the claim we are testing against). To answer this question, we compute a t-test statistic using the data and the null value. The t-test statistic for this example is 0.53 minus 1, all divided by 0.34 divided by the square root of 53, which gives -10.06. This test statistic has a t-distribution with n-1 = 52 degrees of freedom. This test statistic tells us that the mean mercury level in our sample of lakes (0.53 ppm) is about 10 standard errors lower than the hypothesized null value of 1.0 ppm. When we plot the t-distribution with 52 degrees of freedom to see where our test statistic value falls, we see that a t-value of -10.06 is very far out in the left tail of the distribution. It appears that this sample result is very unlikely, or very unusual, if we assume that 1.0 ppm is the true population mean mercury level in fish in all Florida lakes. But, how unlikely is it Can we quantify this value To quantify how unusual the evidence is compared to what is assumed to be true, we calculate a p-value. The p-value is the probability that you would obtain a sample result this “unusual” if the null hypothesis were really true and any observed difference was simply due to sampling variability. To put it another way, it s the probability of getting our sample result (or one even more extreme) if the null hypothesis were true. As with any probability, the value can take on numbers between 0.0 and 1.0. A sample result would be deemed “unusual” or unlikely if the probability of it occurring (under the assumption that the null hypothesis was really true) is small (that is, the p-value is small) and a sample result would be deemed likely if the probability is large (that is, the p-value is large). The smaller the p-value, the less consistent or compatible the data are with the null hypothesis. But, how do we figure out what would be considered small By using something called the significance level, which is denoted by the Greek letter, alpha. The significance level, or level of unusualness , is typically set at a probability of alpha = 0.05 or 5%. So if something has a chance of occurring 5% of the time or less, then we would consider it to be unusual . The significance level for a study is chosen by the researcher at the beginning of the study, before collecting any data. Note that there is nothing magical about the typical significance level of 0.05. We could just as well set it at alpha = 0.01, or even alpha = 0.0001. So how do we decide what alpha should be Some research areas have standard accepted values of alpha in their field, but most studies will have a significance level of 0.05. Putting together the evaluation pieces, the last piece of the framework is to make a conclusion about the strength of evidence we have against the claim by comparing the p-value to the significance level, alpha.Let s visualize how to find the p-value for a test statistic using its distribution. Recall that for continuous population distributions (such as the Normal or t-distribution), the area under the curve tells us the probability that a value will occur within that specific range of values. We determine the appropriate area under the curve by the test statistic (and its distribution) and the alternative hypothesis. For a t-test, if the alternative hypothesis is that (mu) is less than the null value (a one-sided test), then we focus our interest in the lower (left) tail beyond our test statistic. So we locate the test statistic on the t-distribution and calculate the area under the curve to the left of that test statistic. This provides the probability of seeing the result we saw in our study (or less) if the null were really true. (Keep in mind that the test statistic might occasionally happen to be larger than the null value but we still want the area to the left of our test statistic.) If the alternative hypothesis is that (mu) is not equal to the null value (a two-sided test), then we are interested in both tails beyond our test statistic. A common way to calculate this is to take the absolute value of our test statistic, calculate the area under the curve to the right of that test statistic, and then multiple this value by two. This provides the probability of seeing the result we observed (or more extreme, in either direction) if the null were really true. Lastly, if the alternative hypothesis is that (mu) is greater than the null value (again, a one-sided test), then we are interested in the upper tail beyond our test statistic. So a similar procedure is carried out. We calculate the area under the curve to the right of the test statistic, and this is the probability of seeing the result we saw (or more) if the null were really true. (Again, keep in mind that the test statistic might occasionally happen to be smaller than the null value but we still want the area to the right of our test statistic.) Note that we will always use software to calculate p-values from our test statistic and the appropriate t-distribution. Once the p-value is found, then we can make a formal decision about the test. If the sample result is unlikely to occur (taking sampling variability into account) if the null hypothesis is true, then the p-value will be less than alpha and we say we reject the null hypothesis. That is, we have evidence against the null hypothesis and in favor of the alternative hypothesis. The sample data provide statistically significant evidence in support of the alternative hypothesis. If the sample result is likely to occur (taking sampling variability into account) if the null hypothesis is true, then the p-value will be greater than alpha and we say we do not reject the null hypothesis. That is, we lack evidence against the null. We do not have sufficient evidence to discard the null hypothesis. In either case, remember to always state the conclusion in the context of the problem and not just conclude reject the null or result is statistically significant. By the way, note that it is a quirk of statistical practice to never accept the null hypothesis, but only fail to reject it. This is because lacking strong evidence against the null hypothesis is not the same as having strong evidence for the null hypothesis. In the trial by jury example, the jury does not find the defendant innocent but only not guilty . As the colloquial saying goes, Absence of evidence is not evidence of absence. Let s evaluate the evidence for the Mercury Content in Fish example. Recall that the test statistic for this example was -10.06. We noticed that it was in the extreme left tail of the t-distribution. So what is the probability of seeing a t-value of -10.06 (or less, because of the alternative hypothesis) if the null hypothesis were really true To find this, we calculate the area under the t-distribution curve to the left of -10.06. This value turns out to be 4 x 10^-14 (in scientific notation), or really, really, really unlikely. Whenever you find a p-value that is really, really small, it s best to report the p-value as p &lt; 0.001 . Assuming we set the significance level, alpha, to 0.05, our p-value of &lt;0.001 means that we will reject the null hypothesis and conclude that there is evidence that the mean mercury level in largemouth bass in all Florida lakes is less than the guideline mercury level of 1.0 ppm. This is good news: the evidence suggests that the fish in Florida lakes are safe to eat. Testing for a single mean against a standard or skeptical claim is not as common in practice as comparing two groups or comparing two measurements on one group (paired data). Comparing two groups occurs when we have two distinct groups and the observations in each group are independent of one another (to be discussed in more detail in a future lecture). In contrast, paired data arises when we have two measurements of something on all members of a single group and those measurements are dependent on one another. Examples of paired data include: Before and after (or pre and post) measurements are taken on each participant in a study, A new treatment is tested on one side of the body and the placebo is tested on the opposite side of the body of the same participant, Study participants are matched based on demographic variables (such as age and gender) and one of each pair is assigned to the treatment group and the other is assigned to the control group, and Study participants are twins or siblings recruited as pairs. In all of these cases, the two measurements are likely to be related to one another, or dependent on one another, and are not independent. (For example, twins are likely to be more similar to each other than to an unrelated person.) When the variable of interest (the outcome variable) is a continuous variable, we can carry out a paired t-test. However, a paired t-test is essentially a one-sample test on a variable that is a paired difference. Let s look at an example to understand how this is the case.A study on patients with cystic fibrosis looked to see if patients pulmonary function (measured as FEV1 (% of predicted)) improved while on a new treatment during the course of their hospital stay. The researchers measured the FEV1 for 18 patients at admission (Pre FEV1), provided all of them the new treatment, and measured the FEV1 again on those same 18 patients at discharge (Post FEV1). They were really interested in the mean difference or mean change in FEV1 values from admission to discharge. So they calculated the difference (Post FEV1 Pre FEV1) for each patient (so that positive differences indicate improved lung function) and then calculated summary statistics (sample mean and sample standard deviation) for that difference or change variable. They found that the mean change in FEV1 was 12.2 (% of predicted) and the standard deviation of the change in FEV1 was 9.1 (% of predicted). References: Data: Pezzulo, A. A., Stoltz, D. A., Hornick, D. B., &amp; Durairaj, L. (2012). Inhaled hypertonic saline in adults hospitalised for exacerbation of cystic fibrosis lung disease: a retrospective study. BMJ open, 2(2), e000407.Because the researchers were interested in whether there was a mean change in FEV1 from admission to discharge, the null hypothesis for this example is (mu) is equal to 0, or the true mean change in FEV1 is equal to 0 (because remember the null hypothesis is always stated as no effect or nothing going on ). The alternative hypothesis in this case would be that the true mean change in FEV1 is not equal to 0, because the researchers are interested in any change (positive or negative) and didn t specify a direction. Carrying out the evidence and evaluation pieces of the hypothesis testing framework, it turns out that the mean change of 12.2 is 5.7 standard errors above the hypothesized mean of 0. Using a t-distribution with 17 degrees of freedom, the probability of seeing a result like this (or more extreme) if there really was no change is p = 1 x 10^-5 or p&lt;0.001. Because the p-value is less than the significance level of 0.05, we reject the null hypothesis and we can say there is evidence that the mean change is different from 0. It appears, based on the data, that the mean change is greater than 0, meaning the new treatment appears to improve lung function in cystic fibrosis patients. To close out the formal lecture on hypothesis testing, here are few words of advice: Use two-tailed tests in most situations, rather than one-tailed tests. Why As previously mentioned, to define a one-tailed test, a researcher must predict which direction the data will go prior to collecting the data, when planning a study. A one-tailed test can sometimes be useful because it gives a more focused hypothesis and reduces the necessary sample size. However, if the data end up going the opposite direction than expected, then one would end up with a very large p-value and not be able to reject the null hypothesis, even if the difference was very large. For this reason, we encourage using two-tailed tests in nearly all cases, except in the rare case when the researchers truly have no interest at all in one of the directions. Report the actual p-value. Avoid reporting p-values as an inequality (e.g., p &lt; 0.05), unless the value is really, really small. In that case, report the p-value as &lt; 0.001. Also, don t report p-values to more than 3 decimal places. Being that precise in our estimate does not add any relevant information to the reader. There are no sharp distinctions between p-value increments. For example, a p-value of 0.06 provides about the same degree of evidence against the null hypothesis as a p-value of 0.05. One should use multiple sources of evidence to make decisions and not just solely rely on the p-value to make a statistical (and practical) conclusion. The p-value does not indicate the magnitude, direction or clinical importance of an observed result. It merely estimates the role of just by chance as an explanation for the observed result in comparison to the hypothesized value. Remember that we don t conclude we accept the null when we obtain a large p-value. A large p-value just means that our sample result is consistent (given sampling variability) with the null hypothesized value. To provide a better picture of other plausible values that the true population value can take, we recommend you also supply a confidence interval. The null value would then be one of many plausible values that the truth can take (based on our sample result as the best guess ). "]]

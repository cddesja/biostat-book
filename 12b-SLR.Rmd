Similar to correlation analysis, linear regression is a technique that is used to explore the relationship between two numerical variables that have a linear relationship. Unlike correlation, however, regression allows us to do more exploring of the relationship. For one, it allows us to determine the line that best describes the linear relationship between the two numerical variables. It also allows us to estimate the change in one variable (Y) that corresponds to a given change in the other variable (X). Lastly, it allow us to predict the value of one variable (Y) for a specified value of the other (X).  

If only one numerical variable (X) is used to predict the value of the other numerical variable (Y), the analysis is called simple linear regression (often denoted as SLR). When two or more variables (X1, X2, etc.) are used to predict the value of a numerical variable (Y), the analysis is called multiple linear regression. We will cover multiple linear regression in a future lecture.

Let’s work through SLR, using the same Mercury Content in Fish and Water Acidity example we used in the previous lecture.We have used several terms throughout the course when we examine the relationship between two variables. For example, in a previous lecture, we talked about “exposure and disease”, where exposure occurs first and we are interested in its relationship (if any) to the disease. We also talked about “factor and outcome”: for example, how does the factor affect the outcome. You might have even encountered “X and Y” or “independent and dependent” language sometime in your previous coursework. In statistics, you may see the terms “explanatory (or predictor) variable” and “response variable”. The table on this slide lists synonyms for these terms that you might encounter in practice or in reading the medical or public health literature. 

Note: Unlike when computing the correlation coefficient, in regression, it’s important to correctly identify the response (Y) and explanatory (X) variables. The response variable is the variable to be predicted from or explained by the explanatory/predictor variable. 

In our example, mercury in fish is the response variable since we are interested in predicting this from water acidity, the explanatory variable. Let’s explore the process of fitting a line to a set of data. 

When two numerical variables have a statistical relationship (as opposed to a deterministic one) the data points will not all fall perfectly on a straight line. Instead, the data will appear to be a cloud of points. So with the variability that is associated with the points around an imaginary line, how do we decide which line is the “best fitting”? 

A few of the many possible lines through the data points in this study are illustrated in the plot above. Which one do you think is the “best” line for the data? As you are contemplating the answer, think about the criteria that you are using to decide which line is the “best fit” to the data. (For example, are you looking for the line to go nearest to the middle of all of the data? Or the line that seems to have the same slope as the general trend of the data? Or something else?)If you chose Line A as the “best fitting” line, then you would be correct. The reason that Line A is the “best fit” is because it gets closest to all of the points (in an intuitive sense). This line goes by various names, such as the line of best fit or the linear regression line or the fitted regression line or the least squares regression line. The least squares regression line is the line that minimizes the distances between the line itself and the observed values of the response variable (y). To obtain this line, we only consider the vertical distances from each point to the line, as shown on the plot above. These vertical distances are called residuals. We square these distances (so that points below the line don’t cancel out points above it), add them up for all points, and find the line that minimizes this “sum of squared distances”. From the equation for the best fit line, we can calculate the predicted y value for each x value: the predicted values form the line (shown in blue above).

A formal definition of a residual, e, is the difference between the observed and predicted values of the response variable y; that is, observed y minus predicted y, or y minus y-hat, for a given data point. The predicted values of y are the y-values along the best-fitting line. As depicted in the scatterplot above, a residual is the vertical deviation (or distance) from a data point to the line. Points above the line will have positive residuals and points below the line will have negative residuals. If the predicted values closely match the observed values, the residuals will be small. 

For example, for the data point where x = 7.8 and y = 0.77, the predicted y-value (y-hat) is 0.343 (as indicated by the yellow star on the line at x = 7.8). So the residual for that data point is 0.770 – 0.343 = 0.427. Since the residual is positive, we know that the observed y-value is above the line (which we can also see from visual inspection of the scatterplot). 

Note that the predicted value of y for a given x-value is not the same as the observed value of y in most cases. The predicted y-value will be the same as the observed value of y only in cases where the data point lies exactly on the line. Thus far, we’ve been vague about the components that go into the regression line. You may remember from a previous math class that the equation of a line can be written as y = mx + b, where m is the slope and b is the intercept. In statistics, we have the same form but use different letters: b0 for the intercept and b1 for the slope. We write the least squares line as y-hat = b0 + b1x. Let’s break down the parts the go into the regression line. 

The y-hat denotes the y-values predicted by the regression model. 

The y-intercept, b0, identifies where the regression line crosses the y-axis, or the value of y when x equals 0. 

The slope, b1, reflects the change in Y for a one-unit change in X, or ”rise over run”, where you rise Y amount for one-run over in X. 

The least squares regression line is obtained by choosing slope and intercept values that minimize the sum of the squared residuals.

In practice, finding the values for the intercept, b0, and slope, b1, is done using a computer. However, these values can also be calculated by hand from summary statistics using the equations given on the slide above, using the  sample mean of x [x-bar], sample mean of y [y-bar], sample standard deviation of x [sdx], sample standard deviation of y [sdy], and the correlation between the variables x and y [r].*Note: There was a typo in the transcript. While the transcript has been updated for Spring 2021 (bold and underlined text is the updated information), the audio recording still has the error. This will get corrected once we return back to working in the office.

The regression line for the Mercury Content in Fish and Water Acidity example is shown in the equation and plot above. The value of the slope, b1, is -0.152 and the value of the intercept, b0, is 1.53. The equation states that for a given value of water acidity, pH (the x  variable), the predicted average mercury content in fish (in ppm) (y-hat) is given by y-hat = 1.53 – 0.152x. (*minus sign updated in equation) 

The sign of the slope is negative, which means there is a negative linear relationship between water acidity and average mercury content in fish; lakes with higher water acidity will tend to have lower levels of mercury in fish, and vice versa. The magnitude of the slope tells us that for every 1-unit increase in water acidity, we would expect the average mercury content in fish to decrease by 0.152 ppm. 

The intercept describes the expected average mercury content in fish when there is no water acidity in a lake. Intercepts do not always have realistic interpretations, as in this example. If a lake has a pH of 0, it is considered very acidic (think “battery acid” acidic). This is not a realistic situation.We can use the regression line to predict a value of y for a particular value of x. 

Let’s see how this works for our example. What is the expected average mercury content in fish (in ppm) for a lake that has water acidity (pH) of 6.3? Insert 6.3 into the equation in place of x and solve for y-hat: 

y-hat = 1.53 - 0.152*6.3 = 0.572 

The predicted average mercury content in fish for a lake that has water acidity of 6.3 is 0.572 ppm. A question we might have once we obtain the regression line is, “How well does the line fit the data?” This can be assessed using the coefficient of determination, or R2 value (which is the square of the correlation coefficient, r).  R2  describes the proportion of the variation in y that is explained by the regression line. The larger the value of R2, the better the fit of the regression line to the data. 

For our example, the R2 value is 0.33, which means that 33% of the variation in average mercury content in fish (Y) is explained by the linear relationship with water acidity (X). Since only 33% of the variation in average mercury content in fish is explained by water acidity, there are likely other variables that explain some of the remaining 67% of the variation. Multiple regression analysis uses more than one explanatory variable to predict the response variable, as we will discuss in a future lecture. If there are other explanatory variables significantly related to average mercury content in fish, including them in a multiple regression model will increase the R2 value. Here are a few additional notes about simple linear regression.

The first thing you should do when you have to carry out any type of analysis is plot the data! Similar to correlation, this is also true when it comes to simple linear regression. The regression line is a model describing the linear association between two numerical variables. A relationship that is curved or wavy would need a nonlinear model to appropriately describe the association. 

Predictions using regression line equations are only valid within the range of x-values in the collected data. For the Mercury Content in Fish and Water Acidity example, the range of acidity (pH) is between 3.6 and 9.1. It would not be appropriate to use this regression line equation to predict average mercury in fish for a lake that has a pH of 10 or a lake that has a pH of 2. There may be a different relationship between these two variables beyond the range of the collected data so the relationship identified by the regression line equation should not be extrapolated beyond the range of the x-values. 

Similar to correlation, regression is influenced by outliers. Therefore, regression techniques are not robust to outliers, especially outliers that lie to the far right or far left relative to the other points. Outliers in the Y direction will have large residuals. Outliers in the X direction can have a large influence on changing the regression results. 

As you could see from the formula for finding the slope, there is a close connection between correlation and regression. Because r is used to find the slope, both values will have the same sign. In other words, both values indicate the direction of the relationship. Where they differ is that the magnitude of the correlation coefficient indicates the strength of the linear relationship, whereas the magnitude of the slope indicates how much the response variable changes as a result of a change in the predictor. 
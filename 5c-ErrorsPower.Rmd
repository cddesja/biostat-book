Hypothesis testing is a very useful tool for making a decision about the population based on the data from a sample. However, hypothesis tests are not flawless: they do not always lead us to the correct conclusion 100% of the time. For example, using the US criminal justice system analogy, it sometimes happens that an innocent person is found guilty or that a guilty person walks free. (Perhaps youÕve seen news articles or movies about cases like these.) In statistics, we can quantify how often we are likely to make these types of errors. We can even quantify how often we are likely to make the correct conclusion about a real effect (this is called statistical power). This lecture will present the concepts of errors and power in hypothesis testing. When we do hypothesis testing, there are two things that could really be true: 
The null hypothesis could really be true: there is no difference or effect. (This is shown in the left column in the table.) 
The null hypothesis could really be false: there is a difference or effect. (Right column.)
Keep in mind, though, that we never know what the truth is. (If we did, we wouldnÕt need to do a hypothesis test!)

There are also two decisions that we might make based on our data:
1) We could decide that we lack evidence against the null (i.e., we fail to reject the null hypothesis (H0) because the p-value > ??). In other words, we decide that there is no difference or effect. (This is shown in the top row in the table.)
2)  We could decide that we have evidence against the null and in support of the alternative hypothesis (i.e., we reject H0 because the p-value < ??).  In other words, we decide that there is a difference or an effect. (Bottom row in table.)

Since we donÕt know the truth, our decisions might be wrong. There are two ways for our decisions to be right and two ways (called errors) for our decisions to be wrong. 

The two ways for our decisions to be right are highlighted in yellow in the table:
1) We failed to reject the null hypothesis when it was in fact true. (This is the upper left yellow box.) In other words, we concluded correctly that there was no difference or effect.
2) We rejected the null hypothesis when it was in fact false. (This is the lower right yellow box.) In other words, we concluded correctly that there was a difference or effect.

The two ways for our decisions to be wrong are:
1) We rejected the null hypothesis when it was in fact true. (Lower left box.) In other words, we concluded INCORRECTLY that there was a difference or effect. This is called a Type I (Type One) error or a false positive result.
2) We failed to reject the null hypothesis when it was in fact false. (Upper right box.) In other words, we concluded FALSELY that there was no difference or effect. This is called a Type II (Type Two) error or a false negative result. This type of error can be thought of as Ômissing a real differenceÕ or Ôfailing to see a real effectÕ.Since we do not know the truth, we can never know if we made an error or not in a given study. However, hypothesis testing is designed so that we can set how likely it is that we will make an error over the long term, over many studies: that is, we can set what the long-term probability of making an error is.
 
The probability of making a Type I error is defined as the probability that we reject the null hypothesis when it is in fact true. (In the statistical notation given on the slide, Pr means Òthe probability ofÓ, and the vertical line between Òreject H0Ó and ÒH0 trueÓ is read as Ògiven thatÓ.) The probability of making a Type I error is called alpha (?). This is the same alpha that you saw in a previous lecture: the significance level for a hypothesis test. The significance level, alpha, is set by the researcher prior to the study. Typically alpha is set to 0.05 or 5%. Setting alpha at 5% means that, over the long term and many studies, we will have a 5% chance of making a Type I error. In other words, we will have a 5% chance over the long term of saying there is an effect when there really isnÕt.

The probability of making a Type II error is defined as the probability that we fail to reject the null hypothesis when it is in fact false. The probability of making a Type II error is called beta (?). As we will see shortly, the value of beta is related to the statistical power of the study, and is set by the researcher prior to the study. Typically beta is set to 10% or 20% (corresponding to power of 90% or 80%, respectively).  Setting beta to 10% means that, over the long term and many studies, we will have a 10% chance of making a Type II error. In other words, we will have a 10% chance over the long term of missing a real effect.

Notice that the typical value for alpha (5%) is smaller than the typical values for beta (10% or 20%). This is because in a typical health-related study, a Type I error is seen as less tolerable than a Type II error. In a typical study, it is seen as worse to say there is an effect (of a new treatment, for example) when there isnÕt, than to miss an effect when it is there, so alpha is set small and beta is allowed to be higher. There is a direct trade-off between Type I and Type II errors, as we will see shortly: we can only increase the one by decreasing the other. The optimal trade-off between alpha and beta will depend on the field and the goals of the study. In some situations, it might actually be preferable to make a lot of false positive decisions (high alpha) in order to avoid making any false negative decisions (very low beta).
One of the values that we typically want to be as large as possible is called statistical power. 

Power is the probability of correctly rejecting the null hypothesis, or the probability of rejecting the null hypothesis given that it is in fact false. ItÕs the ability to see a difference or an effect when there really is one. (This is the yellow box in the table above.) Power is defined as one minus the Type II error rate, beta. By setting the Type II error rate, beta, for our study at a small value, we can make the power (1 minus beta) of our study large. 

Research studies in medicine and public health have in the past typically been designed to have a power of 80%, but it is increasingly common these days to design for 90% power. A study with 90% power will have a 10% Type II error rate. This means that if the same study were repeated over and over, and a hypothesis test was carried out each time, about 90% of the tests would correctly reject the null hypothesis and conclude there is a significant difference when there really is one, and 10% of the tests would incorrectly fail to reject the null hypothesis and conclude there is not a significant difference when there really is one. 

Keep in mind that we never know which of our hypothesis test results are correct and which are errors, since we donÕt know the truth!

Power can also be thought of as sensitivity. A more powerful test is more sensitive to small differences or effects.As we mentioned earlier, there is a trade-off between Type I and Type II error rates, and between the Type II error rate and power. In addition, all three are affected by the size of the effect and by the sample size. LetÕs investigate the relationship between power, errors, sample size, and size of the effect visually. 

This plot shows a typical case. The distribution on the left-hand side of the plot (solid black line curve) represents the sampling distribution of the sample mean assuming that the null hypothesis was really true. This is the same distribution that we saw in a previous lecture. The center of the distribution is at the null hypothesized population mean, µ0. However, suppose that the truth is actually that the population mean µ equals some alternative value (denoted µA in the plot). The sampling distribution of the sample mean in this case is on the right-hand side of the plot (blue dashed line curve). The difference between the null value, µ0 , and the alternative value, µA, is called the effect size.  

The Type I error rate for our hypothesis test is set at alpha=0.05 (the vertical black line on the plot). When we carry out a hypothesis test, we either reject the null hypothesis if our sample mean result is to the right of the vertical black line, or we fail to reject it if our sample mean result is to the left of that line.

If the null hypothesis is really true, then we need to focus on the left-hand sampling distribution. The region shaded in red represents alpha, the Type I error rate, which is typically 5% of the area under the null hypothesis sampling distribution. [We are assuming a one-sided test here, for simplicity.] As we conduct a hypothesis test, we look to see if the sample mean lies in the region that is shaded in red (sometimes called the Ôrejection regionÕ), which would indicate that the sample result is unusual under the null hypothesis and the p-value will be less than alpha. If so, we reject the null hypothesis and conclude that there is evidence in support of the alternative hypothesis. However, IF the null hypothesis was really true, then this conclusion would be a Type I error. This would occur in only about 5% of the sample results over the long term. 

If the alternative hypothesis is really true, then we need to focus on the right-hand sampling distribution. The region shaded in yellow represents beta, the Type II error rate, which is typically 10% of the area under the alternative hypothesis sampling distribution (assuming the study power was set at 90%). As we conduct a hypothesis test, if the sample mean lies to the left of the vertical black alpha=0.05 line, this would indicate that the sample result is NOT unusual under the null hypothesis and the p-value will be greater than alpha. If so, we fail to reject the null hypothesis and conclude that there is no evidence in support of the alternative hypothesis. However, IF the alternative hypothesis was really true, then this conclusion would be a Type II error. This would occur in only about 10% of the sample results over the long term. 

Power is the chance that we obtain a sample result that is to the right of the alpha=0.05 line, when in fact, the true population mean is µA. Power would thus be the area that is under the blue dashed-line curve AND to the right of the alpha=0.05 line. If beta was set at 10%, then the power will be 90%. In this case, nearly all of the area under the blue dashed-line curve is to the right of the alpha=0.05 line.

We would like this area for power to be as large as possible and the areas for the errors to be as small as possible. However, as already mentioned, there are unavoidable trade-offs.As previously mentioned, the researcher sets the significance level, the Type I error rate, in advance. You might think that the best approach in a hypothesis test would be to set the significance level, alpha, at a very low level, so that the risk of a Type I error would be very low. That is, there will be a small chance of mistakenly concluding that a result is statistically significant. Unfortunately, there is a direct trade-off between the Type I error rate and the Type II error rate. Lowering the Type I error rate (making alpha smaller) necessarily increases the Type II error rate (beta) and therefore decreases the power of the test. Lowering alpha means that you are requiring stronger and stronger evidence of ÒunusualnessÓ from your data before you will conclude that there is a differenceÐor in other words, you are making your test less and less sensitive so it will only see really BIG effects. This makes it harder and harder to mistakenly conclude that there is a difference when there isnÕt, but it correspondingly makes it easier and easier to miss a real difference that happens to be not big enough.

This can be seen visually in the plot above. When alpha is decreased to 0.01 (holding everything else the sameÐsame hypotheses, same variability, same sample size), the vertical line shifts to the right, making the chance of a sample result falling into the red region tiny (alpha has decreased) and thereby making the chance of falling into the yellow region bigger (beta has increased). As a result, power also decreases when alpha decreases, other things being equal.What would happen if we increased alpha? The converse of the situation described on the previous slide happens. When alpha is increased from 0.05 to 0.10 (holding everything else the sameÐsame hypotheses, same variability, same sample size), the vertical line shifts to the left, making the chance of a sample result falling into the red region larger (alpha has increased) and thereby making the chance of falling into the yellow region smaller (beta has decreased). 

Stated differently, the risk of making a Type I error would be higher, but the risk of making a Type II error would be lower (and power would be higher). If the null hypothesis was true, there is a larger chance that we will mistakenly conclude there is evidence of an effect when in fact there isnÕt one (i.e. make a Type I error). But if the alternative hypothesis was true, there is a smaller chance that we will miss a real difference (i.e. make a Type II error) and a larger chance that we will catch a real difference. So increasing alpha also increases power, other things being equal. We just looked at the trade-off between Type I error and Type II error (and power), but letÕs look at two other features that affect the distributions: effect size and sample size. 

What would happen if the true effect size, or difference between the null value and the alternative value, was larger? If everything else stayed the same (same alpha level of 0.05, same variability, and same sample size) and the only thing that is different is that the centers of the two distributions are farther apart, then the chance of making a Type I error (red region) stays the same (since alpha was kept at 0.05), but the chance of making a Type II error (yellow region) is smaller, and the chance of correctly concluding there is an effect (power) gets larger. It is easier to see larger effects than smaller ones. 

Of course, we wonÕt ever know what the true population mean is, but if the true population mean is farther away from the hypothesized null value, then there is a larger chance we will correctly conclude that there is a difference when there really is one.What about increasing the sample size? How will this affect errors and power? Recall from a previous lecture that as the sample size increases, the variability in the sampling distributions decreases. That is, the sampling distributions get narrower. 

So what is the effect of sample size on errors and power (holding all else constant)? If the sample size is increased, both sampling distributions (the one under the null hypothesis and the one under the alternative hypothesis) become narrower. Since we have fixed the Type I error at 5%, as the distributions get narrower, the vertical alpha=0.05 line has to move to the left so that the rejection region (red area) will still contain 5% of the area under the null hypothesis sampling distribution. Thus the chance of making a Type I error (red region) stays the same, but the chance of making a Type II error (yellow region) gets smaller, and the chance of correctly concluding there is an effect (power) gets larger. 

Increasing the sample size has the same effect as increasing the effect size: both (assuming all else is held constant) reduce the Type II error rate and increase the power of the study. The difference is that the true effect size is usually not under the control of the study investigator while the sample size usually is. So in general, increasing the sample size is usually the only practical way to increase the power of a study.
What are the implications if we reject the null hypothesis and conclude that there is a statistically significant difference or effect? Does that imply that the effect is large enough to be interesting or clinically significant? The answer is no. A small p-value just means the sample results would be surprising if the null hypothesis was true. So what are some possible explanations for why one might get a p-value less than 0.05?
 
Explanation #1: It is possible that there really is a meaningful effect or difference. This is what everyone thinks about and hopes for when they see a small p-value. The effect is statistically significant AND clinically significant.

Explanation #2: The effect is statistically significant but not clinically significant. In other words, there is a small (but not meaningful) effect or difference. So while the p-value may be small, it could be that we had a large sample size and low variability so it was easy to detect a difference, even if that difference isnÕt large. But this small difference might not be practically or clinically significant. It is good to look at multiple sources of evidence to make a conclusion and not just rely on only the p-value.

Explanation #3: There is really no effect or difference, but we just happened to get an extreme sample result, just by chance: the result is a false positive. If alpha is set at 0.05, then we would expect to get a Type I error or false positive result about 5% of the time in the long run. 

Explanation #4: The result arose due to Òp-hackingÓ. P-hacking refers to ÓfishingÓ in the data to try to find a statistically significant result, when in fact, there is none. Some examples of how researchers p-hack include:
Slicing and dicing data by analyzing various subsets and only reporting the results that produced low p-values, or 
Tweaking analyses in various ways and only reporting the results that give p-values less than 0.05, or 
ÒPeekingÓ at the data before analyzing it to help define their alternative hypothesis. 
P-hacking or ÔfishingÕ is one of the most damaging things researchers can do to the integrity of a study. It fills the scientific literature with irreproducible results and makes people skeptical about research findings. It is essential for researchers to be transparent in reporting how a study was conducted and how the data were analyzed.

In sum: be cautious when interpreting small p-values or statistically significant results. DonÕt automatically default to Explanation #1 without also considering the possibility that the true explanation is one of the others listed. Consider multiple sources of evidence (e.g., point estimates, interval estimates, sampling method, study design) before making a conclusion about the population.What are the implications if we fail to reject the null hypothesis and conclude that there is not a statistically significant difference or effect? Does that imply that the effect is not interesting or clinically significant? The answer is again no. A large p-value just means the results would not be surprising if the null hypothesis was true. So what are some possible explanations for the why one might get a p-value greater than 0.05?

Explanation #1: It is possible that there really is not a meaningful effect or difference. This is what everyone thinks about when they see a large p-value.

Explanation #2: It is possible that there really is a meaningful effect, but it was too small to be detected in the study. The study did not have enough power to detect the effect. (Perhaps the researchers thought the effect would be larger and powered their study to detect that larger effect. Their study would then have been underpowered for the smaller effect. Or perhaps the variability in their study was much larger than expected and kept them from being able to see a small difference.)

Explanation #3: There is really an effect or a difference, but we just happened to get a sample result that is not extreme given the null hypothesis, just by chance: the result is a false negative. If the study was powered at 90%, then we would expect to get a Type II error or false negative result about 10% of the time in the long run. 

Remember, a high p-value does not prove that the null hypothesis is true: it just means that we donÕt have sufficient evidence to reject the null hypothesis. Just as in the previous slide, consider multiple sources of evidence (e.g., point estimates, interval estimates, sampling method, study design) before making a conclusion about the population.You may be wondering at this point, how does one go about choosing the significance, or alpha, level? The default is alpha=0.05. To use a different alpha value, you will need to thoroughly justify your choice when you report the study. Here are some guidelines: 

If the goal of the study is to answer exploratory questions of interest, particularly in pilot studies that are generating data for planning future studies, then people sometimes use a larger value of alpha such as perhaps 0.10 or 0.20. 

If the goal of the study is to overturn a long-standing theory or assumption, then the evidence needs to be stronger to convince people that the long-standing theory is questionable, so a smaller value of alpha may be helpful, such as perhaps 0.01. (Using a smaller value of alpha is analogous to Ôsetting a higher barÕ for considering a result to be significant.)

Or, if the consequences of rejecting the null hypothesis are going to be substantialÐmaybe it results in an expensive change in policy or practiceÐit might be helpful to use a smaller value of alpha. 

However, it is also important to consider the trade-offs between the Type I and Type II error rates when designing a study. As previously discussed, setting alpha at a smaller value necessarily increases beta and decreases power.
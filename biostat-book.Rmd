--- 
title: "Biostatistics (DRAFT)"
author: "Christopher Desjardins, Laura Le, and Ann Brearley"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook."
---

# Introduction

What is **biostatistics**? How does it fit within the larger field of research? What will you learn from this course? These are the questions that will be addressed in this lecture. 

Let’s begin with the question “What is statistics?”.

There are many definitions, but they all have similar themes. 

For many people (maybe even you), statistics is seen as a branch of mathematics. However, John Tukey, a famous statistician in the mid-1900’s, stated that “statistics is a science, not a branch of mathematics, but uses mathematical models as essential tools.” Just as engineering, chemistry, and economics use math as a tool but are seen as separate fields from math, so should statistics be distinguished from mathematics.

Alan Agresti and Christine Franklin have described statistics as “the art and science of learning from data” (this is also the title of their introductory statistics textbook). The goal of statistics is to translate data into understanding of the real world. At the heart of it all is the word data. 

Examples of areas in which statistics is used include business, climate change research, manufacturing quality control, government policy, education, finance, and even sports (have you seen the movie Moneyball?).

## Cycle of Research
Medical or public health research, like all scientific research, begins with a *research question* about a population of interest. Biostatistical methods are used to help answer this research question. Let’s describe this process using the Cycle of Research diagram shown here.

```{r fig.cor, fig.height = 8, fig.width = 8, echo = FALSE}
library(grid)
grid.newpage()
vp <- viewport(width=0.8,height=0.8)
pushViewport(vp)

## x-axis
grid.rect(x = 1,
          y = 1,
          width=unit(1, "inches"), height=unit(1, "inches"))
grid.rect(x = 0,
          y = 1,
          width=unit(1, "inches"), height=unit(1, "inches"))
grid.rect(x = 1,
          y = 0,
          width=unit(1, "inches"), height=unit(1, "inches"))
grid.rect(x = 0,
          y = 0,
          width=unit(1, "inches"), height=unit(1, "inches"))

grid.text("Population", x = 0, y = 1)
grid.text("Sample", x = 1, y = 1)
grid.text("Statistic", x = 1, y = 0)
grid.text("Parameter", x = 0, y = 0)

grid.lines(x = c(0.1, .9),
           y = c(1, 1),
           gp = gpar(fill = "gray80", col = "gray80"),
           arrow = arrow(angle = 20, length = unit(0.1, "inches"), type = "closed"))
grid.text("Study Design", x = 0.5, y = 1.0, gp = gpar(cex = 1.2))

grid.lines(x = c(1, 1),
           y = c(0.9, 0.1),
           gp = gpar(fill = "gray80", col = "gray80"),
           arrow = arrow(angle = 20, length = unit(0.1, "inches"), type = "closed"))
grid.text("Sampling", x = 1, y = 0.5, gp = gpar(cex = 1.2))

grid.lines(x = c(0.9, .1),
           y = c(0, 0),
           gp = gpar(fill = "gray80", col = "gray80"),
           arrow = arrow(angle = 20, length = unit(0.1, "inches"), type = "closed"))
grid.text("Estimation", x = .5, y = 0, gp = gpar(cex = 1.2))

grid.lines(x = c(0, 0),
           y = c(0.1, 0.9),
           gp = gpar(fill = "gray80", col = "gray80"),
           arrow = arrow(angle = 20, length = unit(0.1, "inches"), type = "closed"))
grid.text("Inference", x = 0, y = 0.5, gp = gpar(cex = 1.2))
````

### Step 1: Population to Sample

The scientific *Cycle of Research* begins with a *population*. A population is the complete set of individuals who share some common characteristics. The population is the group of people we are interested in learning about. The population of interest in a given study may be U.S. adult males, or it may be Finnish preschoolers, or it may be HIV-positive adults in South Africa. Perhaps we are interested, for example, in knowing how tall ten-year-old American boys are. The population in this case would be all ten-year-old boys in the U.S.

When we want to answer a specific question about a particular population, typically we obtain a sample. A *sample* is a subset of the population of interest. The sample is the group of people we actually study. This is the group of people from whom data are collected. In our example, we might randomly sample 1,000 ten-year-old boys from the U.S. population to serve as our study’s sample.

You may wonder, if the goal is to make some conclusions about the population, why not just collect data on the entire population? Samples are often studied instead of entire populations for several reasons. 

One reason is cost. Often it would be too expensive and/or too time-consuming to collect data from an entire population of interest. For example, perhaps we are interested in the blood pressures of residents of Massachusetts. It would be much too expensive to obtain blood pressure measurements from every resident of the state, so we obtain a sample of residents instead. Take a careful look at this figure. Note that the samples have different numbers of people in them, and different mean (or average) diastolic blood pressure values. Also note that none of the mean values for the samples are the same as the mean blood pressure for the population.  We will explore these concepts later.

<INSERT FIGURE>

Another reason for using a sample instead of the population is feasibility. Studying the entire population may simply not be possible. For example, it is not possible to determine the effect of a new treatment on an entire population of patients if the treatment is not yet available to the entire population. 

A third reason is accuracy. Measurements from a study’s sample may be more accurate than measurements from a population. More time and effort can be devoted to carrying out the measurements for the smaller number of people in the sample than is feasible for an entire population. We also can’t measure everyone in the population at the same time in the same way. Measurements may change over time (as we have all seen with our weight). If we have to measure a lot of participants, we may need multiple people or instruments to do it, which can also decrease accuracy.

Finally, it may be unethical to study the entire population. If you're administering a treatment that might be harmful or withholding a treatment that might be beneficial.


*Step 1 in the Cycle of Research* is to design a study in order to obtain a suitable sample from the population or populations of interest. There are a number of ways doing this. These will be discussed in more detail later in this book.

### Step 2: Sample to Statistic
Once we have a sample, *Step 2 in the Cycle of Research* is to use the sample data to calculate statistics. A *statistic* is any number calculated from the sample data. The sample mean is a statistic. The sample variance is another statistic. In our example, we might measure the height of each boy in our study’s sample of 1,000 ten-year-old American boys and then calculate the average (or mean) height. That average height is a statistic.

### Step 3: Statistic to Parameter
Step 3 in the Cycle of Research is to use sample statistics to estimate population parameters. A *parameter* is a variable which describes some aspect of the population, but whose true value is unknown. The population mean is a parameter. The population variance is another parameter. In our example, the population parameter of interest is the average height of ALL ten-year-old boys in the U.S. The true value of this parameter is unknown because it is not possible to measure the heights of all ten-year-old American boys at once.

Sample statistics can be used as estimates of the unknown population parameters. The sample mean is a good estimate of the population mean. The sample variance is a good estimate of the population variance. In our example, the mean height of the 1,000 ten-year-old boys in our study’s sample is an estimate of the true mean height of ALL ten-year-old boys in the United States. 

###  Step 4: Parameter to Population
Step 4 in the Cycle of Research is to use the population parameter estimates we have obtained to *infer* something about the population we are interested in. *Statistical inference* is the process of drawing conclusions about a population on the basis of observations from a sample. In our example, we might want to know if ten-year-old American boys are taller now than they were fifty years ago. We could use the estimated true mean height of all ten-year-old American boys we obtained from our sample average in Step 3 and compare it to the value from fifty years ago. If the values were different, we could ask whether that difference is large enough to be *statistically significant*. 

Each time a researcher poses a scientific question and the investigation is deemed worthwhile and feasible, the scientific Cycle of Research begins again.
  
The goal of this course is to develop your ability to do four things:

1. To understand the principles behind basic statistical analyses;
2. To choose appropriate statistical analyses for a given scientific context;
3. To carry out statistical analyses using R or SAS and understand the resulting output; and
4. To read and interpret statistical results in the literature of your field of interest.

The topics for the course will run from descriptive analyses to inferential analyses, such as confidence intervals, hypothesis tests, and linear regression.

<!--chapter:end:index.Rmd-->

# Variable Types

## Scales of Measurement

There are two broad types of measurement scales: *categorical* and *numerical*. 

Categorical scales are *qualitative*. Examples of variables measured on a categorical scale include sex, blood type, race, or disease status. The data at an individual level consist of what category that individual falls into (for example: male or female). The data at the study level are the number of study participants in each category (for example: how many men and how many women were included in the study). 

Numerical scales are *quantitative*. Examples of variables measured on a numerical scale include weight, height, survival time, or blood pressure. Variables measured on a numerical scale will often have measurement units associated with them. For example, a person’s age might be measured in years, or in months, or, for newborns, in weeks or even days. On a numerical scale, the differences between numbers have meaning. The difference between 32 years and 33 years of age has the same meaning as the difference between 85 years and 86 years of age. All variables measured on numerical scales are therefore “interval variables”.

### Categorical Scales
Categorical measurement scales can be further divided into *nominal* scales and *ordinal* scales.

Nominal scales have two or more categories with no natural ordering. For example, religious affiliation could be described on nominal scale using the categories of Catholic, Protestant, Jewish, Muslim, Hindu, Buddhist, Other. The order of listing the categories is irrelevant. Examples of variables measured on a nominal scale include sex, race, blood type, or marital status. 

A *binary scale* is a special case of a nominal scale, in which there are only two categories, such as “Male”/”Female” or “Child”/”Adult”. The responses to any question with a “Yes”/”No” answer are measured on a binary scale. Common binary scale measurements in medicine and public health include disease status (“Has the disease”, “Does not have the disease”) and diagnostic test result (“positive”, “negative”). (CHRIS - I don't know if I buy that binary scales are a special case of a nominal scale. Your examples are ordinal).

*Ordinal scales* have two or more categories that do have a natural ordering. Examples of variables measured on an ordinal scale include Apgar score, tumor stage, Likert-type scale survey questions, or social class. On an ordinal scale, numerical assignments are relative and do not represent any interval relationship between categories. For example, Apgar scores for newborn infants range from 0 to 10, and higher scores indicate better functioning, but the difference between scores of 8 and 9 may not have the same implications as the difference between scores of 2 and 3. 

A given variable can be measured using a number of different scales. The measurement scale that is used determines what type of variable it is. For example, smoking status could be measured on a binary scale, as “Smoker”/”Non-smoker”. It could be measured on a nominal scale, as “Ex-smoker”, “Current smoker”, “Never smoked”. It could even be measured on an ordinal scale, such as “Never smoked”, “Quit smoking >10 years ago”, “Quit smoking 1-10 years ago”, “Quit smoking within the last year”, “Current smoker”. Smoking status could even be measured on a numeric scale by measuring the number of years a person smoked, or the number of years since they quit smoking.

The type of variable, in turn, determines which *summary*, *plotting*, and *analysis* methods are appropriate. 

Categorical data is typically summarized in tables using the numbers (or counts or frequencies) and proportions (or percentages) of study participants in each category.

Appropriate plots for categorical data include *bar graphs* and *pie charts*, which help the reader to visualize the number or proportion of study participants in each category.

Summary statistics and plots are described in more detail later.

We will explore a number of statistical analysis methods that are appropriate for categorical data in this book. These include the use of confidence intervals for estimating a single proportion, methods for comparing proportions in two groups, including confidence intervals for relative risks or odds ratios, and methods for comparing proportions in two or more groups, including Chi-square test, binomial exact test, or Fisher’s exact test. 

### Numerical Scales
Numerical measurement scales can also be further divided, into *continuous scales* and *discrete scales*.

Continuous scales describe characteristics that can take on any real number value, such as 98.7 or 52.63 or 0.014. Examples of variables that are measured using continuous scales include blood pressure, temperature, age, weight, or height.

Discrete scales describe characteristics that can have only integer values. Examples of variables that are measured using discrete scales include the number of children in a family, the number of births in a year, or the number of accidents in a month.

Continuous measurement scales can be further divided into *interval scales* and *ratio scales*.

On an interval scale, the intervals or differences between values have the same meaning throughout the scale (as is true for any numeric scale), but the zero of the scale doesn’t mean “none” of the quantity of interest. For example, temperature is commonly measured on an interval scale (degrees C or degrees F). The difference between 32F and 33F has the same meaning as the difference between 97F and 98F; however, 0˚F does not mean “no temperature”. As a result, it would make no sense to compute or interpret *ratios* of temperatures. If the high temperature today in Phoenix, Arizona, is 80F and the high in Nome, Alaska, is 40F, it does not mean that Phoenix is twice as hot as Nome. 

On a ratio scale, both the differences between values and the ratios of values make sense. For example, weight is measured on a ratio scale. The difference between 10 kg and 11 kg has the same meaning as the difference between 35 kg and 36 kg. Furthermore, 0 kg means “no weight” so it makes sense to compute weight ratios. Something that weighs 50 kg is twice as heavy as something that weighs 25 kg.

Continuous data is typically summarized in tables using means and standard deviations, or medians and ranges or inter-quartile ranges. If the variable is a ratio variable, reporting the coefficient of variation (CV) is also appropriate.

Appropriate plots for continuous data include dot plots, box plots, histograms, and scatterplots (for comparing two continuous variables).

Summary statistics and plots are described in more detail later.

In this book, we will explore a number of statistical analysis methods that are appropriate for continuous data. We will cover the use of confidence intervals to estimate a mean, confidence intervals or t-tests to compare two means, and analysis of variance (ANOVA) to compare three or more means. We will cover using correlation and simple linear regression to explore relationships between two continuous variables. We will also learn how multiple linear regression is used to model the relationship between a continuous outcome and multiple possible predictors (including treatment or exposure), and to adjust for potential confounding factors. 

One important note: there are relationships between numerical and categorical scales that can sometimes make it tricky to determine what kind of measurement scale is being used.

A variable measured on a numerical scale can be converted to a variable measured on a categorical scale. This may be called “categorizing” the variable. For example, age can be measured on a continuous numerical scale, in years. It can also be categorized by dividing the continuum of years into age group, such as “0-5 years”, “6-10 years”, “11-19 years”, “20-39 years”, “40-59 years”, “60-79 years”, and “80+ years”. Age group, in this case, is an ordinal categorical variable. Age could also be categorized by dividing the continuum into two groups, such as “child” and “adult”. In this case, age group is a binary nominal categorical variable.

A variable measured on a categorical scale can be coded numerically. For example, the ordinal variable social class could be coded numerically as 0=Lower class, 1=Working class, 2=Lower middle class, 3=Upper middle class, 4=Upper class. It is important to remember that coding categorical data using numbers does not make the data numeric. The codes simply represent the names of the categories. 

The nominal variable religious affiliation could be coded numerically as 1=Catholic, 2=Protestant, 3=Jewish, 4=Muslim, 5=Hindu, 6=Buddhist, 7=Other. Remember that coding religious affiliation numerically does not make it into an ordinal variable; there is no inherent ordering to the religious affiliation categories.

Let’s look at a real example. The *Blood1* dataset in the Stat2Data package contains measurements of three variables related to blood pressure, taken on 500 adults.  

The variable *SystolicBP* is the systolic blood pressure measured for each person, in millimeters of mercury (mm Hg). 
The variable *Smoke* is the smoking status recorded for each person, labeled a “1” if the individual was a smoker or a “0” if the individual was not a smoker.

The variable *Overwt* is the weight group recorded for each person, labeled a “0” if the individual was in the normal weight range, “1” if the individual was overweight, and 2 if the individual was obese. 


```{r, echo = FALSE, results = "asis"}
library(Stat2Data)
library(xtable)
data(Blood1)
print(xtable(head(Blood1, n = 10), caption = "(\\#tab:blood1) First 10 participants in the Blood1 dataset"), type = "html")
```

The data for the first ten participants in the *Blood1* dataset are shown in Table \@ref(tab:blood1). Let’s examine these variables one by one.

1. The first variable listed is systolic blood pressure, *SystolicBP.* This variable is numeric because the values are numbers and have units of measurement. Blood pressure is typically reported to the nearest one millimeter of mercury, as here, but it is nevertheless a continuous variable, since you could have a blood pressure of 138.2 mmHg or 74.5 mmHg. Furthermore, blood pressure is measured on a ratio scale, since a blood pressure of zero mmHg means no pressure, and a blood pressure of 140 mmHg means there is twice as much pressure as a blood pressure of 70 mmHg. The *SystolicBP* variable in this dataset is therefore a numeric, continuous, ratio variable.

2. The second variable listed is smoking status, *Smoke.* This variable is categorical, since it tells you which category the person falls into: smoker or non-smoker. The data happen to be coded here as “1” for smoker and “0” for non-smoker, but it is not numeric data. The data could just as well have been coded “Y” for smoker and “N” for nonsmoker, or even as the words “smoker” and “nonsmoker”. This variable is not ordinal. While you may feel that not smoking is better than smoking, there’s no inherent ordering to the two categories. Finally, this variable is binary, since it has only two categories, smoker and non-smoker. The *Smoke* variable in this dataset is therefore a categorical, binary variable. 

3. The third variable listed is weight status, *Overwt.* This variable is categorical, since it tells you which category the person falls into: normal weight, overweight, or obese. The data happen to be coded here using the numbers “0” for normal, “1” for overweight, and “2” for obese, but they could just as well have been coded “1”, “2” and “3”, or “10”, “50”, and “300”, or “A”, “B”, and “C”, or even as the words, “normal”, “overweight”, and “obese”. This variable is ordinal, since there is an inherent ordering to the categories that matches the numbers: overweight feels like it’s “in between” normal and obese, just as “1” is between “0” and “2”. Note that there is an order to the categories, but not a magnitude: there is no sense that obese (coded “2”) is twice as much weight as overweight (coded “1”); and there is no sense that normal (coded “0”) means “no weight”. This variable is not binary, though, since there are more than two categories. The *Overwt* variable in this dataset is therefore a categorical, ordinal variable.

One further note about the *Overwt* variable. It is likely that this variable was derived from a body mass index (BMI) measurement by choosing cut-points: for example, perhaps normal was set as BMI values of 24.9 or below, overweight as BMI values of 25.0 to 29.9 and obese as BMI values of 30.0 or more. This is an example of “categorizing” an underlying numeric variable.

## Summarizing Data

### Categorical Data
Recall that categorical variables are variables that are qualitative, and can be either nominal or ordinal.

In the Blood Pressure example, there were two variables in the dataset that were categorical: *Overwt* and *Smoke*. We summarize categorical data by: 

The **number** of observations in each category, which we find by simply tallying the number of observations in that category. 
The **proportion** of observations in each category, which we find my taking the number in each category and dividing by the total number of observations in the study. These proportions will be a value between 0 and 1. 

The **percentage** of observations in each category, which is the proportion multiplied by 100, followed by a percentage sign.

While not required, it often helps to summarize the data in a table.

Let’s go through an example to see how these summary measures work, using the smoking status variable from the Blood Pressure example. 

Of the 500 participants, 266 were smokers and 234 were nonsmokers. In this dataset, we see that there are more smokers than nonsmokers. 

It is appropriate to compare two numbers, or counts, when the denominator is the same or very similar. But, for example, if we were comparing the number of smokers between males and females and the total number of males and females was different (for example, 100 males vs. 50 females), it would not be appropriate to compare the counts of smokers between males and females. 

Rather, it is more appropriate to compare groups using proportions and percentages. In the Blood Pressure dataset, the proportion of smokers is 0.532, or 53.2% of the individuals are smokers, and the proportion of nonsmokers is 0.468, or 46.8% of the individuals are not smokers. (Note: Percentages are more common in reports and articles, and may be reported without the percentage sign next to the number. If this happens, authors will often denote somewhere in the table that the number represents a percent.)

Pie charts are one way that we can graphically represent categorical data. Pie charts allow us to visualize the differences between proportions in the various categories. To create the pie chart, we need to know the proportion or percentage in each category. 

In the Blood Pressure example, we saw that there were about as many smokers as nonsmokers, with slightly more smokers. We can visually see that in the pie chart as well; it looks like the “smoker” piece of the pie has more than the “nonsmoker” piece. However, it’s not exactly clear how much more.

```{r}
## Insert pie chart
```

Because of this issue with pie charts (the difficulty in comparing slices and estimating percentages), statisticians tend to prefer other graphical methods for categorical data.(CHRIS - There's perception studies about this)

A better graphical summary for categorical data (and the one preferred by many statisticians) is the bar plot. The bar plot can be used with any summary measure: number, proportion, or percentage. Bar plots are often arranged so the categories are on the x- (horizontal) axis and the summary measure is on the y- (vertical) axis; however, the opposite arrangement can be used as well (i.e., categories on y-axis and summary measure on x-axis). 

As with the pie chart, we can see which categories contain more participants and which contain fewer: the taller bars occur for the categories with more participants (or a higher proportion) and the shorter bars indicate the categories with fewer participants. Unlike the pie chart, the y-axis tells us the number or proportion for each category. As a result, we get both the visual comparison and the actual value.

As we did with the pie chart, we can visually see that there are more smokers than nonsmokers. However, we can also see the approximate numbers in each category: there are about 30 more smokers than nonsmokers.

```{r}
library(ggplot2)
Blood1$smoke.f <- ifelse(Blood1$Smoke == 0, "Nonsmoker", "Smoker")
Blood1 |>
  ggplot(aes(smoke.f)) +
  geom_bar(fill = "blue", alpha = .5, col = "blue") +
  xlab("") +
  ylab("Count") +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank()) +
  scale_y_continuous(breaks = seq(0, 260, 50))
```

### Numerical Data
There are many different ways to summarize numerical data. The most common summaries are measures of center (such as means or medians) and measures of spread (such as standard deviations or interquartile ranges). In addition, we typically want to note any extreme or unusual observations in the data.

If we wanted to pick one number to represent a set of data, we’d probably want to pick the value that indicates the center or central tendency of the data values, or the one that is a “typical” or average value. 

However, we know that there will (most likely) be variability in our data. That is, not all of the values will equal each other. So, the measure of center should not be our only summary measure for numerical data. We are also interested in how spread out, or how varied, our data values are from one another. We do this by computing measures of spread. 

There are other summary measures that we may compute as well to understand our data, such as percentiles or the minimum or maximum.

Finally, we want to note if there are any observations that are very different or unusual from all of the other observations in the sample. 

#### Measures of Center

##### Mean

Some measures of center use the values of the variable. The individual values in the sample are denoted as $x$ with a subscript $i$, where $i$ runs from 1 to the number of observations in our data. For example, the first observation is $x_1$, the second observation is $x_2$, and so on. The mean (or average) value, noted as $\bar{x}$ (pronounced x-bar), is the result of adding up all the individual values, $x_i$, and then dividing by the number of observations in our sample, denoted by $n$.

$$
\bar{x} = \frac{\Sigma x_i}{n}
$$
In our Blood Pressure example, when we add up the blood pressure measurements for all 500 adults and divide by 500, we get a mean value of 145 mm Hg. 

Note that the mean is calculated using all values in the dataset. If there is one observation or several observations that are much higher or lower than the rest of the data, it will raise or lower the mean more than it would if those observations were similar to the rest of the data.

##### Median
Sometimes, it makes more sense to summarize the data by the order as opposed to by the value. Why? Because sometimes there are data points that are so different from the rest that they overly influence the mean. For example, let’s say our data points were 1, 2, 3, 4, and 100. If we took the average of these 5 data points, we would get the value 22. But does 22 seem to be typical? Or the center value? It is a lot higher than four of the values and much much lower than the fifth. It doesn’t seem to represent any portion of the data very well.

One of the features of numerical data is that you can put it in order from smallest to largest. Let’s take advantage of this additional feature of numerical data to create different summaries for the typical value and the spread of the data.

When we order the observations from smallest to largest, there will be a middle value. The **median** is the value of the middle observation in the dataset. (If there are an even number of observations in the dataset, then the median is the average of the two middle values.) The median is also referred to as the **50th percentile** because 50% of the observations are below the median and 50% are above the median. 

For the Blood Pressure dataset, if we order the observations from smallest to largest, the middle observation would be between the 250$^{th}$ and 251$^{st}$ observations (because there are an even number, 500, of observations in this dataset). When we average these two values, we get a median of 140.5 mmHg, which is similar, but not the same, as what we calculated for the mean. 

Note that the median is calculated using only the middle value or values. If one observation in the data is much higher or lower than the rest of the data, it will be part of the order of the data, but the value itself is not used to find the median. Therefore, observations that are very different (either higher or lower) than most of the data do not affect the value of the median.

##### Other Measures of Center
Other measures of center that are encountered in the medince and public health include the **trimmed mean** and the **geometric mean**. 

To calculate a trimmed mean, a specified percentage of the highest and lowest values for that variable are excluded before calculation of the mean. For example, the 10% trimmed mean would exclude the highest 10% and lowest 10% of the observations and then a mean would be calculated based on the included observations (the middle 80% of the data). This makes the trimmed mean less sensitive to those observations that are much higher or lower than the rest of the data.

To calculate the geometric mean, the data are first log-transformed, then the mean is calculated on the log scale, and lastly, the mean is transformed back to the original scale. The geometric mean can be useful for summarizing highly skewed data.

#### Measures of Spread

##### Variance and Standard Deviation
One common measure of the spread of numerical data is the variance (or the related measure, the standard deviation). 

$$\text{Variance}: s^2 = \frac{\Sigma(x_i - \bar{x})^2}{n - 1}$$

The variance, denoted $s^2$, is calculated in several steps. The first step is taking the actual value for each observation, denoted $x_i$, as before, and subtracting the average value, $\bar{x}$. This difference is called the **deviation**, $x_i – \bar{x}$. Values that are far from the mean will give us large deviations, while values close to the mean will give us small deviations. Values that are below the mean will give us negative deviations, while values above the mean will give us positive deviations.

However, we need to pull all these deviations into one number. Note that, because we are subtracting the average, some deviations will be negative and some will be positive. If we simply add up these values, the negative and positive deviations will cancel each other out, and the sum will be zero, which is not a useful measure of the spread of the data.  Our goal is to quantify the difference from the mean, but we don’t really care if the value is below or above the mean. One way we could emphasize the difference is to *square the difference*. Then, the focus is on the size of the difference, as opposed to whether the value is above or below the average. (Note that we could also take the absolute value, but it is easier mathematically to work with the squares of the differences.)

The second step, therefore, is to add up the squares of the deviations for all observations in the sample. This is sometimes called the *sum of squares*.

The third and last step is to divide the sum of squares by the sample size n - 1, giving a single number called the variance. The variance is a modified average of the squared deviations. We use n - 1 instead of n because it gives a less *biased* estimate of the variance for all potential participants. 

The units for variance are the units of the variable squared. Because of this, it is hard to directly interpret the variance value. We typically take the square root of the variance so that the units of measurement match the original data. When we do this, we obtain the **standard deviation**. The standard deviation can be interpreted as the average deviation of the observations away from the mean. 

$$
\text{Standard Deviation (SD)}: s = \sqrt{s^2}
$$

Note that, like the mean, we use all observations in the data to find the standard deviation. So if one observation is much higher or much lower than the rest of the data, it will influence the value of the variance and standard deviation.

Let’s calculate the variance and standard deviation for the Blood Pressure example.

$$s^2 = \frac{(133 - 145)^2 + (115 - 145)^2 + \dots + (180 - 145)^2 + (174 - 145)^2}{500 - 1} = 783.72 \text{ mm Hg}^2$$

We find the deviations between the individual values and the sample mean of 145, square those deviations, add up all the squared deviations, and divide by n - 1, which is 499, and we get a variance of almost 784 mm Hg squared.

$$
s = \sqrt{783.72} = 27.99 \text{ mm Hg}
$$

Taking the square root, we find that the standard deviation is almost 28 mm Hg. That is, the average deviation away from the mean for the systolic BP variable is roughly 28 mm Hg.

##### Interquartile Range

Another common measure of the spread of numerical data is the **interquartile range**, or **IQR**. This measure, like the median, is based on the order of the observations, not on their values. 

The interquartile range is calculated from the first and third quartiles of the data and is the range of the middle 50% of the data. The first quartile (or 25th percentile) is the median of the lower half of the data. The third quartile (or 75th percentile) is the median of the upper half of the data. To obtain the interquartile range, we subtract the first quartile from the third quartile.

Note that like the median, we order the data from smallest to largest and focus only on the values for the middle half, from Q1 through Q3. So if one observation in the data is much higher or much lower than the rest of the data, it won’t affect the value of the interquartile range.

Also note that the exact definition of the median, and thus of the quartiles, varies slightly from one software package to another. For small datasets, the median and IQR given to you by software may differ slightly from the values you obtain by hand, but they will be close. For large datasets, the differences will be negligible.

What is the interquartile range for the Blood Pressure data?

The first quartile (the 25th percentile) is the average of the 125th and 126th values, which is 130 mm Hg.
The third quartile (75th percentile) is the average of the 375th and 376th values, which is 162.5 mm Hg. 

The interquartile range is the difference between 162.5 and 130, which is 32.5 mmHg. This represents the range of values for the middle 50% of the data.

(Note that the IQR for this dataset happens to be a bit bigger than the standard deviation.)

Another useful measure of spread is the **range**. The range is simply the distance between the **minimum** (the smallest value) and the **maximum** (the largest value) in the dataset. 

The minimum, 1st quartile, median, 3rd quartile, and maximum comprise the **five-number summary**, which is often used as a group to summarize data. It gives a sense of the typical value of the data as well as how the values vary (both the middle 50% of the data as well as the whole data range). 
 
Let’s find the range of systolic blood pressure readings from the Blood Pressure example. 

* Smallest 5: `r head(sort(Blood1$SystolicBP), n = 5)`
* Largest 5: `r tail(sort(Blood1$SystolicBP), n = 5)`
* Minimum: 67 mm Hg
* Maximum: 224 mm Hg


We see that 67 mm Hg is the smallest value in the dataset, so it is the minimum. 
The largest value is 224 mm Hg, the maximum.

$$
\text{Range: } 224 - 67 = 157 \text{ mm Hg}
$$
The range is 157 mm Hg.

Putting this all together, for the Blood Pressure dataset, the five-number summary for the systolic blood pressure would be:

	minimum: 67 mm Hg
	Q1: 130 mm Hg
	median: 140.5 mm Hg
	Q3: 162.5 mm Hg
	maximum 224 mm Hg

##### Other Measures of Spread
Other measures of spread include the **mean absolute error** and the **coefficient of variation**.

The mean absolute error (or MAE) is when you use the absolute difference between each observed value and the mean, instead of the squared difference, as used for the variance. You would then add up all the absolute differences and divide by the number of observations in the sample.  

The coefficient of variation (or CV) is the sample standard deviation divided by the sample mean, and is usually expressed as a percentage. For example, if the standard deviation is 5 and the mean is 50, then the coefficient of variation is 0.10 or 10%. The sample standard deviation is about 10% as large as the sample mean. The coefficient of variation helps us compare variability for different measurements that have different ranges of values.

#### Other Summary Measures
Other summaries which use the order of the data include finding the observation that has a certain percentage of data below it. This is called a **percentile**. For example, the 10th percentile is the value in the dataset for which 10% of the sample values are below it (and thus the other 90% of the sample values are above it).

Note that the first quartile, median, and third quartile are all percentiles.

#### Outliers and Robustness
One last important step in describing a set of data is to note observations that are unusually different from the rest of the data. A data value may be much higher than most of the data, or much lower. When a data point doesn’t “fit” with the rest of the data, we call it a potential outlier.

As we have mentioned already, extreme or unusual values can overly influence those summary measures based on value, like the mean and the standard deviation, but have less of an effect on those summary measures based on order, like median and IQR. Summary measures that are not as affected by outliers are called **robust statistics**.

#### Histograms
Let’s move on to graphical summaries of numerical data. One way to visualize numerical data is with a **histogram**. The histogram divides the data into groups (or bins) based on their values and graphs the number or frequency in each bin. The observations are often divided into equal size groups (such as 60-79, 80-99, etc.), but this isn’t required. (CHRIS - A histogram, for visualizing purposes only, turns a numerical variable into a categorical one). 

For histograms, the data values are plotted on the x-axis and the number of participants for that data value or range are plotted on the y axis. 

Histograms (for numerical data) may appear similar to bar plots (for categorical data). There are, however, two big distinctions between histograms and bar plots: 
There is an inherent order to the values on the x axis of a histogram – we create partitions based on the values in their numeric order, and
There is no space between the bars on a histogram. In the bar plot, there is a bit of a space between each bar to indicate that they are just different categories and there is not necessarily an order to those categories. In histograms, the numeric order of the values is maintained.

The advantage to this type of graph is that you can quickly get a sense of any patterns in the data. If the data look evenly distributed around the middle, we refer to the data as **symmetric**. If we see that the peak or bulk of the participants are to the left, and the number of participants per group slowly decreases to the right, we say the data are **right skewed**. If we see that the bulk of the participants are to the right and there’s a slow decrease in the number per group to the left, we say the data are **left skewed**.

The disadvantage is that this type of graph doesn’t give you any summary measures of the data, and doesn’t identify any potential outliers.

```{r, bloodhist}
Blood1 |>
  ggplot() +
  geom_histogram(aes(SystolicBP), fill = "blue", col = "black", alpha = .5) +
  xlab("Systolic Blood Pressure (mm Hg)") +
  ylab("Frequency") +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank())
```

Let’s examine the histogram for the Blood Pressure example, shown in Figure \@ref(bloodhist). The systolic blood pressure observations are centered in the vicinity of 130 mm Hg, and range from about 60 to 240 mm Hg. Over 1/3 of the people appear to have values between 120 and 140 mm Hg (the tallest peak). There are only a few people with systolic blood pressure below 100 mm Hg or above 200 mm Hg. We would say this data has a slight right skew, because the tail of the distribution is to the right of the bulk of the data.



#### Boxplot 

Another way to visualize numerical data is with a **boxplot**. Boxplots have also been called box-and-whisker plots (see the box and whiskers in the image shown). 

To create a boxplot, you use the five-number summary: 

* The smallest observation that is not considered an outlier (if no potential outliers, the minimum) marks the end of the bottom “whisker” as shown above.
* The first quartile is the bottom edge of the “box”.
* The median is the line near the middle of the “box”.
* The third quartile is the top edge of the “box”.
* The largest observation that is not considered an outlier (if no potential outliers, the maximum) marks the end of the top “whisker”. 
* If there are potential outliers, they are usually plotted as a dot beyond the “whiskers”. 

For boxplots, we find potential outliers using the 1.5*IQR rule.

The 1.5*IQR rule states that potential outliers are any observations which are either:
* greater than 3rd quartile + 1.5*IQR, or 
* less than 1st quartile – 1.5*IQR.

```{r, spboxplot}
Blood1 |>
  ggplot() +
  geom_boxplot(aes(SystolicBP), fill = "blue", col = "black", alpha = .5) +
  scale_y_continuous(limits = c(-1, 1), breaks = NULL) +
  xlab("Systolic Blood Pressure (mm Hg)") +
  ylab("") +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank()) +
  geom_text(aes(x = 140.5, y = 0.4, label = "Median"), hjust = "left") +
  geom_text(aes(x = 130.0, y = 0.4, label = "1st Quartile", hjust = "left")) +
  geom_text(aes(x = 162.2, y = 0.4, label = "3rd Quartile", hjust = "left")) +
  geom_text(aes(x = 81.7, y = 0.4, label = "Smallest non-potential outlier", hjust = "left")) +
  geom_text(aes(x = 210.5, y = 0.4, label = "Largest non-potential outlier", hjust = "left")) +
  coord_flip()
```
  
For the Blood Pressure example, we can see from the boxplot (Figure \@ref(spboxplot)) that the median is about 140 and the IQR ranges from 130 to about 160. We can also see that there’s a possible right skew, since the median is not in the middle of the box and is closer to the Q1 line. In addition, we can find the potential outliers using the 1.5*IQR rule. Potential outliers would be those values that are:

  less than 130 – 1.5*32.2 = 81.7 , or 
  greater than 162.5 + 1.5*32.2 = 210.5.

The smallest value in the dataset that is NOT a potential outlier is 81 mmHg and the largest value that is NOT a potential outlier is 211, so we draw a whisker to those values, and draw a horizontal line at those values. The remaining observations (below 81.25 or above 211.25) are plotted as dots below and above the ends of the whiskers.

The advantages of a boxplot are that we can see any potential outliers and get a good sense of center and spread based on order. Side-by-side boxplots are also very useful for quickly comparing two or more datasets.

The disadvantage of a boxplot is that we lose some information about the shape of the data. (CHRIS - We could introduce violin plots, sina plots, or overlay points). 


<!--chapter:end:01-variables.Rmd-->

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
```

# Sampling
As we described in the Cycle of Research, **statistical inference** is the process of drawing conclusions about a population on the basis of observations from a sample. Put another way, statistical inference is the process of **generalizing** study results from the study **sample** to the **population of interest**. Drawing correct inferences about the population depends on knowing how representative the sample is of that population. A sample is representative of the population if every person or unit in the population of interest has the same chance of being included in the sample. If the chance of being included in the sample differs from person to person, then we are vulnerable to **selection bias**, which will affect our ability to make accurate inferences about the population.

## What is selection bias? 

In general, “bias” refers to systematic error or inaccuracy in a result. The result deviates systematically from the truth. There are a number of kinds of bias that can be introduced into a research study, usually unintentionally, at any stage from design through to publication.

Selection bias is formally defined as “A systematic tendency to favor the inclusion in a sample of selected subjects with particular characteristics while excluding those with other characteristics” (Pocket Dictionary of Statistics). Selection bias means that some members of a population were more likely to be included in the study than others. For example, if potential study participants were contacted by randomly selecting names from SnapChat, then people who do not have a SnapChat account will be systematically excluded. The resulting study sample is not representative of the population the researchers intended to study. Since the study sample is not representative of the population, the study results cannot be generalized to the population. 


## Random Sampling
**Random sampling** methods are used in research studies to minimize this type of bias. Random sampling is a sampling technique for which the probability of each individual being selected into the sample is known. Another term used for this kind of sampling is probability sampling. Three random sampling methods are commonly used in medical or public health studies: **simple random sampling**, **stratified random sampling**, and **cluster random sampling**.

### Simple Random Sampling
A simple random sample (or SRS) is a random sample where each member has the __same__ chance of being selected, and members are selected __independently__ from each other. In other words, knowing whether other individuals were selected into the sample tells you nothing about whether or not you will be selected into the sample. When simple random sampling is carried out, each potential sample of n individuals from the population is equally likely to be selected.

For example, in a simple random sample from the population of Minnesota residents, all residents are equally likely to be chosen, regardless of who they are or where they live. In addition, the probability of a given resident being chosen does not increase (or decrease) if their neighbor is chosen. 

There are two important things to note about simple random samples:

First, taking a simple random sample from a given population doesn't guarantee that your particular sample will "look like" your population, for the same reason that tossing a fair coin 10 times doesn't always produce exactly 5 heads and 5 tails.

Second, for practical reasons, it is rarely possible to obtain a "true" simple random sample from a population. Instead, it is an ideal that studies try to achieve to the closest degree possible, in order to obtain a sample that is representative of the entire population.

Obtaining a simple random sample (SRS) requires first that all possible members of the population of interest are identified and listed. Once that is accomplished, a random sample of size n can be chosen using a number of possible methods. 

An identification (ID) for each member of the population can written on slips of paper and placed in a hat, and someone can randomly select n slips of paper. This ultra-low-tech method works for small populations, such as children in a school classroom.

A table of random numbers can be used to select n items from the population list. This method is useful when you do not have access to a computer.

A computer program (for example, random.org) can be used to randomly select n items from the population list. For example, we could have a numbered list of all 51,000 students at the University of Minnesota (our population of interest). We could use a random number generator to give us 1,000 random numbers, between 1 and 51,000. Then we would select the 1,000 students whose numbers on the list correspond to the randomly chosen numbers. These 1,000 students would form our sample.

In the health sciences, situations where you can obtain a sample via simple random sampling are quite RARE due to feasibility issues, or sometimes even to desirability issues. Study designs often dictate the nature of sampling. 

### Stratified Random Sampling
In stratified random sampling, the population is divided into subgroups, or “strata”, with similar characteristics, and a simple random sample is selected from each stratum. Common stratification variables include gender, age group, or clinic. 

Typically, the motivation for doing stratified sampling is to ensure that you have "enough" of a particular subgroup to carry out your analyses of interest. For example, if you were interested in looking at differences in breast cancer tumor characteristics between men and women, you'd do stratified sampling to ensure that you had enough men in the sample to make the sex comparison with reasonable precision.

Stratified random sampling requires that information about the stratification variable or variables is available for all potential participants.

### Cluster Random Samping

In cluster sampling, “interventions” are delivered to groups rather than individuals. The population is divided into clusters and a sample of clusters (random or not) are selected. Cluster sampling is often done in a hierarchical or multistage fashion, with the selected clusters further divided into sub-clusters from which another sample (random or not) is selected. Cluster sampling is often used in epidemiologic studies.

For example, if a study is interested in the effect of spraying insecticide to prevent malaria, the sampling is done on houses rather than individuals. Everyone in the house (the cluster) experiences the intervention at the same time, but the outcome of interest is measured at the individual level. Because of this, cluster sampling is an example of non-independent sampling. 

Clusters are often defined geographically. For example, you might divide a city into neighborhoods, and the neighborhoods into blocks, and the blocks into houses. You would randomly choose a specified number of neighborhoods, and then randomly choose a specified number of blocks within those neighborhoods, and then randomly choose a specified number of houses on each block. The random selections could be simple random samples, or they could be systematic random samples (for example: select every 6th house in the block). 

Cluster sampling does not require that all members of a population be identified and listed. 

### Non-Random Sampling
In non-random sampling, or non-probability sampling, the probability that a given participant is selected is unknown and may be influenced by selection bias.

In __convenience sampling__, participants who are readily available are enrolled until the desired sample size is reached. That is, researchers just “conveniently” grab participants who are available. Volunteer sampling is a sampling method that relies on participants who choose to respond (for example, online surveys) and are a type of convenience sample.

__Quota sampling__ is a non-random version of stratified random sampling. Quota sampling occurs when the sample that is obtained has the same proportions of individuals as the entire population with respect to pre-specified known characteristics or traits. That is, the population is divided into categories with a required quota for each category. Participants are enrolled into the study from each category until the quota for that category is reached.  

In __systematic random sampling__, every kth item is chosen. For example, if you were sampling customers in a store, you might approach every 10th person who walked by, or every 20th person. Systematic random sampling should not be used if there are cyclical patterns in the data. For example, selecting hospital admissions data from every seventh day or every 12th month may not be representative of the overall population of admissions and could introduce bias.

The problem with non-random samples is that they may be __biased.__ Not all members of the population of interest have the same probability of being included in the study, so the study sample is not representative of the population. The participants who are “readily available” may differ in some ways from the population as a whole. For example, if a study of sleep apnea were done by sampling people who had recently been to a clinic for any reason, then people who had not been sick recently, or people who had no access to medical care, or people who were in hospitals, prisons, or nursing homes, or military personnel on active duty, would not be included in the study.

However, in many studies, we actually use one of these approaches to obtain our sample but then view it (and do inference) as if it were an SRS. Often, this non-random sampling is "close enough" to random sampling that we are OK with it. For example, when recruiting for a clinical trial of a new HIV medication, we don't make a numbered list of all individuals with HIV and then contact them randomly; instead, we approach people attending HIV clinics and ask them to participate. If we want to infer something about the population, we then have to make the argument that this approach approximates random sampling because, for example, the patients attending the clinic during the recruitment period (and who agree to participate) are representative of and have approximately the same characteristics (such as gender, age, socioeconomic status, etc.) as the overall population of HIV-infected individuals.

Note that if data were collected on every member of the population of interest, then statistical inference would not be necessary. You do not need to infer from a sample to a population if you already have complete data on the population. Descriptive statistics completely represent the population when you have population data; inferential statistics are unnecessary.

## Discrete Distributions
### Random Variables
A **random variable** is an abstract concept that represents the values that could arise from a random process, before they are observed. A random process could be something like tossing a coin or rolling a die, or it could be randomly sampling participants from a population of interest for a survey or a clinical study. We typically use capital letters to represent random variables.

For example, the variable smoker, which describes a participant’s smoking status in the Blood Pressure dataset, can be seen as a random variable symbolized by a capital letter $Y$. The smoker random variable can take on only two possible values: 0 if the participant is a nonsmoker, and 1 if the participant is a smoker.

Lowercase letters are used to represent the actual observed values. Every person in the Blood Pressure dataset has a measured value for the random variable smoker. The first person in the dataset is a nonsmoker, so their value is 0. We use a lower case letter, $y$ in this case, with a subscript of 1 to indicate that this is the first person in the dataset, followed by the value for that person of 0 since they are a nonsmoker (that is, $y_1 = 0$). Similarly, person 15 in the Blood Pressure dataset is a smoker, so their value is 1. We note this with a lower case y, then the subscript 15, and set that equal to 1 (that is, $y_15 = 1$).

#### Discrete Random Variables
A discrete random variable is a variable that can take on only a finite (or limited) number of possible values. There are two types of discrete random variables: discrete numerical variables, which have a finite set of whole number values, and categorical variables, which have a finite number of possible categories.  

The smoker variable in the Blood Pressure dataset, a categorical variable, is a discrete random variable because it can take on only two possible values: smoker (1) or nonsmoker (0).
 
The outcome of a die roll, a discrete numerical variable, is a discrete random variable because we can roll only one of six possible numbers: 1, 2, 3, 4, 5, or 6.  

### Distributions
A distribution describes how often different values for a random variable arise from a random process. There are three types of distributions that are discussed in this book.

A __sample distribution__ describes how often each possible value of a variable has occurred in a specific sample. When we graph the sample data for a variable using a barplot or a histogram, we are visualizing the sample distribution.

A __population distribution__ describes how often each possible value of a variable occurs in the population of interest. 

A __sampling distribution__ describes how often each possible value of a sample statistic (such as the sample mean) could occur in all possible samples. Sampling distributions will be discused later.

#### Categorical Distributions
Let’s start with distributions for categorical variables. 

Categorical variables can be summarized graphically using barplots, or in tables using the number and/or proportion in each category. Either kind of summary is a sample distribution, but for clarity we will focus on graphical summaries.

```{r}
library(gridExtra)
cdc <- data.frame(prop = c(.425, .736 - .425, 1 - .736),
                  levels = c("Obese", "Overweight", "Normal"))
cdc$levels <- factor(cdc$levels, levels = c("Normal", "Overweight", "Obese"))

Blood1$overlab <- ifelse(Blood1$Overwt == 0, "Normal",
                         ifelse(Blood1$Overwt == 1, "Overweight", "Obese"))
Blood1$overlab <- factor(Blood1$overlab, levels = c("Normal", "Overweight", "Obese"))
g0 <- Blood1 |>
  ggplot(aes(overlab)) +
  geom_bar(aes(y=..prop.., group = 1), fill = "blue", col = "blue", alpha = .5) +
  ylab("Proportion") +
  ggtitle("Sample Distribution",subtitle = "Weight Distribution N = 500") + 
  xlab("") +
  theme_bw() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor = element_blank())

g1 <- cdc |>
  ggplot() +
  geom_bar(aes(levels, prop), stat = "identity", fill = "blue", col = "blue", alpha = .5)+ 
  ggtitle("Population Distribution",subtitle = "US Adult Weight Distribution") + 
  theme_bw() +
  xlab("") +
  ylab("Proportion") +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor = element_blank())
grid.arrange(g0, g1, nrow = 1)
```


The barplot on the left above is a sample distribution for the *Overwt* variable in the Blood Pressure dataset. Recall that the Blood Pressure dataset contains data on a sample of N = 500 adults from the U.S. population. The plot is a sample distribution because it describes how often different values of the *Overwt* variable occur in this sample. For each possible category of the variable (normal, overweight, or obese), the plot shows the proportion of participants in the sample who fall into that category. This plot could also have been produced as the number of participants in the sample who fell into each category. 

The barplot on the right above is a population distribution for weight in the U.S. adult population as determined by the Centers for Disease Control and Prevention (CDC). It is plotted using the same weight categories as in the Blood Pressure dataset. This plot is a population distribution because it describes how often different values of the weight variable occur in the population. For each possible category of the variable (normal, overweight, or obese), the plot shows the proportion of the U.S. adult population that falls into that category. Population distributions always use proportions or probabilities on the y-axis.

Note that the overall structure is the same. We have the categories on the x-axis and a measure of quantity on the y-axis.  

We can see that the proportions in each weight category in the sample data (on the left) are similar to but not exactly the same as the true proportions in the population (on the right). We will learn later how to use data from a sample (as shown on the left) to make inferences about the population (as shown on the right). For now, let’s focus on what information the population distribution gives us. 


A population distribution gives us several different kinds of information.

First, it gives information about overall trends. For example, looking at the population distribution for weight in U.S. adults (above), we see that the normal weight category has the lowest proportion, and the obese weight category has the highest proportion.

Second, it gives information about the proportion of the population in a particular category. In our example, we see that the proportion of U.S. adults who are classified as obese is .425, the proportion classified as overweight is 0.311, and the proportion classified as normal weight is 0.264.

Finally, it gives information about the proportion of the population in several categories. In our example, if we would like to know the proportion of the U.S. adult population that are not obese, we could add up the proportion that are normal weight and the proportion that are overweight (0.264 + 0.311 = 0.575) and find that 57.5% of U.S. adults are not obese.  

Proportions obtained from population distributions can also be interpreted as probabilities. Since the proportion of U.S. adults who are obese is 0.425, then if we select a person at random from the U.S. adult population, there is a 42.5% chance (or a probability) that the selected person will be obese.  

##### Bernoulli Distribution

```{r}
ggplot(data.frame(levels = c("Obese", "Not Obese"),
                  prop = c(0.425, 0.575))) +
  geom_bar(aes(levels, prop), stat = "identity", fill = "blue", col = "blue", alpha = .5)+ 
  ggtitle("US Adult Obesity Distribution") + 
  theme_bw() +
  xlab("") +
  ylab("Proportion") +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor = element_blank())

```

For example, suppose we were interested in a random binary variable $X$, which describes whether a random U.S. adult is obese or not. $X$ takes on the value 1 if the person is obese, and 0 if they are not. The population distribution of the random variable $X$ could be described using a Bernoulli distribution with p = 0.425. A plot of this distribution (above) shows us that the probability of the event, being obese, is 0.425, and the probability of not having the event, not being obese, is 0.575.  

Theoretical distributions are described using mathematical notation by giving the name of the distribution and the parameters that describe it. The notation is to list the random variable and then a tilde ($\sim$) symbol, which indicates that the variable has a given distribution. (The tilde can be read as “is distributed as”.) The tilde is followed by the name of the theoretical distribution followed by parentheses that include the parameter or parameters that define the distribution. 

The general notation for a Bernoulli distribution, then, would be $X \sim \text{ Bernoulli(p)}$, read as “X is distributed Bernoulli with probability p”, where p is the probability of the event of interest. 

In our example, we would write $X \sim \text{ Bernoulli(0.425)}$. The binary obesity variable $X$ is distributed Bernoulli with probability 0.425.

##### Discrete Numerical Variables
Now, let’s turn to distributions for discrete numerical variables. 

Discrete numerical variables can be summarized graphically using histograms, or in tables using the number and/or proportion in each category. Either kind of summary is a sample distribution, but for clarity we will focus on graphical summaries.

```{r}
set.seed(1235)
ud <- sample(1:6, size = 330, replace = TRUE)
ud <- as.data.frame(ud)

g0 <- ud |>
  ggplot(aes(ud)) +
  geom_bar(aes(y=..prop.., group = 1), fill = "blue", col = "blue", alpha = .5) +
  ylab("Proportion") +
  ggtitle("Sample Distribution",subtitle = "Distribution of 330 Random Die Rolls") + 
  xlab("") +
  scale_x_continuous(breaks = 1:6, labels = 1:6) +
  theme_bw() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor = element_blank()); g0
up <- data.frame(x = 1:6)
g1 <- up |>
  ggplot(aes(x)) +
  geom_bar(aes(y=..prop.., group = 1), fill = "blue", col = "blue", alpha = .5) +
  ggtitle("Population Distribution",subtitle = "Distribution of Infinite Die rolls") + 
  scale_x_continuous(breaks = 1:6, labels = 1:6) +
  theme_bw() +
  xlab("") +
  ylab("Proportion") +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor = element_blank())
grid.arrange(g0, g1, nrow = 1)
```

The histogram on the left is a sample distribution for the results of rolling a standard six-sided die 330 times. The plot is a sample distribution because it describes how often different values (1 through 6) of the die roll occurred in this sample. In this particular sample of 330 rolls, the die came up as “1” 57 times, as “2” 488 times, and so on.

The histogram on the right is a population distribution for rolling a fair standard six-sided die. This plot is a population distribution because it describes how often different values of a die roll occur in the population. For each possible value of the die roll (1 through 6), the plot shows the probability of obtaining that value. If the die is fair, the probability of any given die roll value is the same: 1 in 6, which gives 0.167 or 16.7%.

Note that the overall structure is the same: We have the possible values on the x-axis and a measure of quantity on the y-axis.  

We can see that the proportions for each die roll value in the sample data (on the left) are similar to but not exactly the same as the true probabilities in the population (on the right). We will learn later how to use data from a sample (as shown on the left) to make inferences about the population (as shown on the right). For now, let’s focus on what information the population distribution gives us. 

A population distribution gives us several different kinds of information.

First, it gives information about __overall trends__. For example, looking at the population distribution for all possible rolls for a fair die (above), each die roll has an equal probability of happening. This is because we are assuming that the die is a fair die, and each number has an equal chance of happening. 

Second, it gives information about the __probability in a particular category__. In our example, there are six different values on the die (1 through 6), and each has an equal chance of being rolled. Therefore, the chance of rolling any one number is 1 out of 6, or 0.167. For example, the probability of rolling a 1 would be 1/6 or 0.167.

Finally, it gives information about the __probability in several categories__. In our example, if we would like to know the probability that a die roll would be less than 4, we would add up the probabilities for all possible die rolls less than 4 (that is, rolls of 1, 2, or 3): 0.167 + 0.167 + 0.167 = 0.5. We would find that the probability of rolling a number less than 4 with a fair six-sided die is 1/2.

##### Binomial Distribution

One common discrete numerical distribution model is the **Binomial distribution**, which is used to model the number of times a specific event occurs in multiple attempts. For example, how many times would we get heads if we tossed a coin four times? The Bernoulli distribution, which we discussed earlier, tells us that the probability of obtaining heads on a single coin toss (assuming the coin is fair) is 0.50 or 50%. But what if we tossed the coin more than once? How many times should we expect heads? The Binomial distribution gives probabilities for each possible number of heads we could get if we tossed the coin multiple times. The Binomial distribution is characterized by two parameters: p, which is the probability of the event of interest on any given attempt, and n, which is the total number of attempts.

The random variable X gives the number of events (heads, in our example) that occur when a binary outcome (such as obtaining heads on a coin toss) is measured on a number, n, of different independent attempts (such as coin tosses). The possible values of X range from 0 to n: in other words, in n tries, we could obtain anywhere between 0 and n events. The probability of the event (heads, in this case), p, is assumed constant for all attempts.  Under these assumptions, the number of events that occur, X, is distributed as Binomial with parameters n and p, where n is the number of attempts and p is the probability of the event. In symbols, X ~ Binomial(n,p). 

In our example, what would happen if we tossed a fair coin four times? How many heads should we expect? Here, the number of attempts, n, is 4, and the probability of the event (heads), p, is 0.5. We could conceivably obtain anywhere between 0 heads and 4 heads. The probabilities for each of the possible number of heads that could be obtained, 0 through 4, are given in the theoretical distribution plot above. The most likely result, with a probability of 0.375 or 37.5%, is that we will obtain two heads (and two tails) in our four tosses. There is a somewhat lower probability (25%) of obtaining one head, or three heads. There is a very low probability (6.25%) of obtaining no heads, or all four heads.

###### Example
Now let’s look at an example related to public health. Suppose that the (known) prevalence of obesity in the U.S. adult population is p = 0.379 or 37.9%. If we randomly sample four adults from the U.S. population, what is the probability that exactly two of the adults will be obese? What is the probability that two or fewer (i.e. half or less) of the adults will be obese? How likely is it that all four of the adults will be obese?

These probabilities can be obtained by using a Binomial distribution for the random variable X, which is the number of the selected U.S. adults who are obese. The Binomial distribution with the number of independent attempts (people) set at n=4 and the probability of the event (obesity) set at p = 0.379 is shown in the plot above. The probability of exactly two of the four people being obese (X = 2) is 0.332 or 33.2%, while the probability of two or fewer being obese is 0.844 or 84.4%. The probability of all four being obese is quite low, at 0.021 or 2%.

```{r}
bin.prob <- data.frame(probs = c(pbinom(0:4, 4, prob = .379)[1], diff(pbinom(0:4, 4, prob = .379))),
  numbers = c(0:4))
ggplot(bin.prob, aes(x = numbers, y = probs)) +
  geom_bar(stat = "identity", width = 1, col = "black", fill = "maroon", alpha = .9) +
  xlab("Number of Obese Adults") +
  ylab("Probability") +
  ggtitle("Binomial(4, 0.379)") +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank())
```

So… how do we obtain these Binomial probabilities? There are two ways. The first is to use the Binomial formula, and the second is to use statistical software.

###### Binomial Probabilities by Formula

$$
Pr(X = k) = \frac{n!}{(n - k)!k!}p^k(1 - p)^{n - k}
$$

$$
\text{where } n! = n * (n - 1) * ... * 2 * 1
$$

If the random variable X has a Binomial distribution, we can calculate the probability of any number of events, k, given n attempts with probability p, using the Binomial formula given above. For our coin toss example, we would use n = 4, p = 0.5, and then calculate the probabilities one at a time for k = 0 heads, k = 1 head, k = 2 heads, k = 3 heads, and k = 4 heads.

This calculation can get tedious and error prone as the number of attempts n increases. A better way is to use statistical software. 

###### Binomial Probabilities by Software
Statistical software such as SAS or R allows us to easily calculate Binomial probabilities. The exact syntax will depend on the software used, and will be discussed in the activities, but the information that must be provided to the software is the same for any software. We will need to tell the software the name of the distribution (here, the binomial distribution), the number of attempts or observations (n), the probability of the event of interest (p), and the number of events or “successes” (k) we are interested in. For our coin toss example, we would use the Binomial distribution with n = 4, p = 0.5, and k=the number of heads (“successes”) we wanted the probability for.

Let’s work through a few cases using our public health example. Recall that we assumed that the (known) prevalence of obesity in the U.S. adult population is p=0.379 or 37.9%, and that we have obtained a random sample of four adults from this population.

What is the probability that exactly two of the four adults are obese? 

In this case, the number of attempts or observations is the number of adults in our sample, which is four, so n = 4. 

  The probability of the event of interest (being obese) is p = 0.379.

```{r}
ggplot(bin.prob, aes(x = numbers, y = probs)) +
  geom_bar(stat = "identity", width = 1, col = "black", fill = ifelse(bin.prob$numbers == 2, "yellow", "maroon"), alpha = .9) +
  xlab("Number of Obese Adults") +
  ylab("Probability") +
  ggtitle("Binomial(4, 0.379)") +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank())
```

The number of events we are interested in is two, so k = 2. This is indicated by the yellow bar in the plot above.

We can either insert these values n, p and k into the Binomial formula given previously and calculate the probability, or we can insert them into our software of choice (R or SAS) and it will give us the probability. In either case, the probability that exactly two of the four adults in our sample are obese is 0.332 or 33.2%.

What if, instead, we wanted to know the probability that two or fewer of the four adults are obese? 

The number of attempts or observations is still the number of adults in our sample, which is four, so n = 4. 

  The probability of the event of interest (being obese) is still p = 0.379.

```{r}
ggplot(bin.prob, aes(x = numbers, y = probs)) +
  geom_bar(stat = "identity", width = 1, col = "black", fill = ifelse(bin.prob$numbers <= 2, "yellow", "maroon"), alpha = .9) +
  xlab("Number of Obese Adults") +
  ylab("Probability") +
  ggtitle("Binomial(4, 0.379)") +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank())
```

Now, however, the number of events we are interested in is two or fewer, so k could be two, or one, or zero. This is indicated by the yellow bars in the plot above.

Again, we could use the Binomial formula given previously, and calculate the probability for k = 0, and then the probability for k = 1, and then the probability for k = 2, and then add the three numbers together.

Alternatively, we could tell our statistical software that we wanted the probability of k or fewer events (using a slightly different command than the one for the probability of exactly k events). 

In either case, the probability that two or fewer of the four adults in our sample would be obese is found to be 0.844 or 84.4%.

Suppose instead we wanted to know the probability that more than two of the four adults are obese (i.e. that k = 3 or k = 4)? This is indicated by the red bars in the plot above. In this case, we would simply subtract the probability of two or fewer being obese (which is 0.844) from the total probability (which is 1.0), to give 0.156 or 15.6%. 

Finally, what if we wanted to know the probability that between 1 and 4 of the four adults are obese? In other words, what is the probability that we will have more than 1 but less than 4 obese adults?

The number of attempts or observations is still the number of adults in our sample, which is four, so n = 4. 

  The probability of the event of interest (being obese) is still p = 0.379.

```{r}
ggplot(bin.prob, aes(x = numbers, y = probs)) +
  geom_bar(stat = "identity", width = 1, col = "black", fill = ifelse(bin.prob$numbers %in% 2:3, "yellow", "maroon"), alpha = .9) +
  xlab("Number of Obese Adults") +
  ylab("Probability") +
  ggtitle("Binomial(4, 0.379)") +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank())
```

What is k? Between one and four events means that the number of events (being obese), k, could be two or three. This is indicated by the yellow bars in the plot above. 

We could use the Binomial formula given previously, and calculate the probability for k=2, and then the probability for k=3, and then add the two numbers together.

With software, it gets a bit more complicated here, since most statistical software is set up to calculate the probability of being less or equal to a specified value of k. So in this case, we need to ask the software for the probability of less than or equal to 3 events (i.e. the probability of 3 or 2 or 1 or 0 events), and then ask it for the probability of less than or equal to 1 event (i.e. the probability of 1 or 0 events) and then subtract the two numbers. In this case, the probability of 3 or fewer events (3 or fewer of the four people being obese) is 0.979 or 97.9%. The probability of 1 or fewer events (1 or fewer of the four people being obese) is 0.512 or 51.2%. Therefore, the probability that between 1 and 4 of the four adults being obese is 0.979 – 0.512 which is 0.467 or 46.7%.

In either case, we will get the same result: the probability that between 1 and 4 of the four adults in our sample would be obese is 0.467 or 46.7%.

We must be very careful with words here. The probability that between 1 and 4 of the adults are obese depends on whether you include the 1 and/or the 4.  We could have four possible cases, depending on whether we include 1, include 4, include both, or include neither. The resulting probabilities for the four cases will be different.

```{r}
ggplot(bin.prob, aes(x = numbers, y = probs)) +
  geom_bar(stat = "identity", width = 1, col = "black", fill = ifelse(bin.prob$numbers %in% 1:4, "yellow", "maroon"), alpha = .9) +
  xlab("Number of Obese Adults") +
  ylab("Probability") +
  ggtitle("Binomial(4, 0.379)") +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank())
```

For example, the plot above shows the case where we include both 1 and 4 as well as the numbers between 1 and 4, as indicated by the yellow bars. 

As before, we can calculate this probability in two ways.

We could use the Binomial formula given previously, and calculate the probabilities for k=1, and k=2, and k=3 and k=4, and then add the four numbers together.

With software, we need to ask the software for the probability of less than or equal to 4 events (i.e. the probability of 4 or 3 or 2 or 1 or 0 events), and then ask it for the probability of less than or equal to 0 events (i.e. the probability of 0 events) and then subtract the two numbers. In this case, the probability of 4 or fewer events (4 or fewer of the four people being obese) is 1.000 or 100%. The probability of fewer than 1 events (0 of the four people being obese) is 0.149.  Therefore, the probability that at least 1 and at most 4 of the four adults are obese is 1 – 0.149 which is 0.851 or 85.1%.  

Compare this result to our previous slide, where the probability that between 1 and 4 of the four adults are obese was 0.467, which is slightly more than half the probability that between 1 and 4 adults inclusive are obese.

It’s critical when finding probabilities for discrete distributions to pay attention to the words used to describe the values of interest, and determine which values are part of the range of interest.

##### Other Discrete Distribution

There are many other theoretical distributions relevant to specific kinds of discrete data, including the Hypergeometric distribution, the Poisson distribution, and the Negative Binomial distribution. For example, the Poisson distribution is used to model the distribution of what is called “count data” (for example, the number of births each day at a hospital) and is characterized by a single parameter, lambda ($\lambda$), the average rate of the event being counted (for example, the average number of births per day at that hospital). 

### Continuous Distributions
The key difference between a discrete random variable and a continuous random variable is that a continuous random variable can, at least in theory, have infinite possible values. This is because any given range of numbers (say the range from 0 to 1) can be divided into an arbitrarily large number of pieces: we could divide it into tenths, or hundredths, or thousandths, and so on. 

For example, if we measure systolic blood pressure, we could in principle measure it to the nearest one millimeter of mercury (mm Hg), or to the nearest tenth of a millimeter, or to the nearest hundredth of a millimeter, without limit. 

```{r}
head(Blood1$SystolicBP)
```

Of course, in practice, there are no blood pressure cuffs that can measure a person’s blood pressure to a precision of dozens of decimal places. In addition, even if our measurement device can measure blood pressure to a precision of (say) two decimal places, for practical reasons we may choose to report the measured blood pressure only to the nearest whole number, as shown above for the first six observations for the *SystolicBP* variable in the Blood Pressure dataset. Nevertheless, we consider blood pressure to be a continuous random variable. 

Let’s look at continuous distributions.

Continuous variables can be summarized graphically using histograms or boxplots, or in tables using the mean and standard deviation (or median and IQR). Either kind of summary is a sample distribution, but for clarity we will focus on graphical summaries.

```{r}
ggplot(Blood1, aes(SystolicBP)) +
  geom_histogram(bins = 10, col = "black", fill = "blue", alpha = .5) +
  xlab("Systolic Blood Pressure") +
  ylab("Frequency") +
  theme_bw() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor = element_blank())  
```

The histogram on the left above is the sample distribution for the *SystolicBP* variable in the Blood Pressure dataset. Recall that this sample consists of 500 people drawn from the U.S. population. The plot is a **sample distribution** because it describes how often different values of the *SystolicBP* variable occur in this sample. The y-axis denotes the number of individuals in our sample that take on certain values. Alternatively, you can construct histograms with “frequency” on the y-axis to denote the proportion of individuals.

The plot on the right above shows the population distribution for systolic blood pressure among all U.S. adults. This plot is a population distribution because it describes how often different values of systolic blood pressure occur in the population. The y-axis denotes the proportion of individuals in the population that take on certain values. 

Unlike the population distribution for a discrete variable, the population distribution for a continuous variable is usually represented using a density curve instead of a histogram. This is because a continuous variable has an infinite number of possible values, so the probability at any one particular value (such as at 147.539872846384 mm Hg) is effectively zero. Therefore, probabilities for continuous variables can only be determined for ranges of values, not for single values. 

The area under the density curve represents probability. The total area under the density curve is equal to 1.00 or 100% probability. The area under the curve within a specific range of values represents the probability that the variable has a value in that range. In our example, estimating by eye the area under the curve above 200 mm Hg, it appears that the probability that a randomly chosen U.S. adult has a systolic blood pressure above 200 mm Hg is quite small, roughly (very roughly) 0.05 or 5%.

Note that the overall structure of the sample distribution and the population distribution is the same: we have the possible values on the x-axis and a measure of quantity on the y-axis. The sample histogram for systolic blood pressure has the proportion in each bin on the y-axis. As previously mentioned, it could also be plotted with the count (or frequency) in each bin on the y-axis. The population density curve, on the other hand, always has probability on the y-axis. 

We can see that the proportions for each range of systolic blood pressure values in the sample data (on the left) are similar to but not exactly the same as the true probabilities in the population (on the right). We will learn later in this course how to use data from a sample (as shown on the left) to make inferences about the population (as shown on the right). For now, let’s focus on what information the population distribution gives us. 

A population distribution gives us several different kinds of information.

First, it gives information about overall shape and trends. In our example, the population distribution (above) has a single peak (that is, unimodal) and appears slightly skewed to the right. 

Second, it gives information about the probability for a range of values. In our example, we see that the most common systolic blood pressure value among U.S. adults is where the peak of the curve lies, at approximately 130 mm Hg. We can also see that most U.S. adults have systolic blood pressures between about 90 and 210 mm Hg, and that only a few have values above 210 mm Hg.

Unlike for discrete variables, we cannot find the probability for a single value of a continuous variable, since, as discussed earlier, the probability for any single value of a continuous variable is essentially zero.

Here is an example:
One measure of hypertension (high blood pressure) is systolic blood pressure over 140 mm Hg. We can use the population distribution to find the proportion of U.S. adults who have high blood pressure by finding the area under the density curve above 140 mm Hg. We find that the proportion of U.S. adults with blood pressure above 140 mm Hg is 0.388 or about 39%. Note that, since the probability of having a systolic blood pressure of exactly 140 is essentially zero, the proportion of U.S. adults with blood pressure at or above 140 mm Hg is also 0.388.

#### Normal Distribution
One very common theoretical distribution for continuous variables is the Normal distribution. It is also called the Gaussian distribution in honor of Carl Friedrich Gauss who noted this general shape in his work and characterized it, or the “bell curve” because the shape looks like a bell.

A Normal distribution is always unimodal (has only one peak) and symmetric. Due to its symmetry, its mean and median are identical. The majority of values are very close to the peak with fewer values further away from the peak. In principle, a Normal distribution has nonzero probability at any value all the way from negative infinity to positive infinity. In practice, Normal distributions are often used to model variables that can only take on positive values, such as systolic blood pressure or serum cholesterol.

Each Normal distribution is characterized by two parameters: the population mean, μ (pronounced mu), and the population standard deviation, σ (pronounced sigma). The mean defines the center or peak of the distribution and the standard deviation characterizes its width or spread. The random variable X represents the values that a continuous variable could take on (such as systolic blood pressure values). If this continuous variable is normally distributed, then we would say that the variable, X, is distributed as Normal with parameters μ and σ, where μ is the population mean and σ is the population standard deviation. In symbols, we write X $\sim$ Norm($\mu, \sigma$). 

For all Normal distributions, slightly more than two thirds of their values (68.27% to be exact) lie within plus or minus one standard deviation from the mean, 95.45% of the values lie within plus or minus two standard deviations from the mean, and 99.73% of the values lie within plus or minus three standard deviations of the mean. This is sometimes referred to as the “68 – 95 – 99.7 rule” for Normal distributions.

```{r}
x <- seq(20, 140,1)
h50 <- dnorm(x, mean = 50, sd = 10)
h80 <- dnorm(x, mean = 80, sd = 20)

labels <- c("N(50, 10)", "N(80, 20)")

plot(x, h50, type="l", lty=1, xlab="", lwd = 2,
  ylab="Density", main="Normal Distributions", col = "gold")
lines(x, h80, col = "maroon", lwd = 2)

legend("topright",
  labels, lwd=2, lty=c(1, 1),
  col=c("gold", "maroon"),
  bty = "n")
```

The plot above shows two different Normal distributions. The gold curve is a Normal distribution with a mean of 50 and a standard deviation of 10. The red curve is a Normal distribution with a higher mean of 80 and a larger standard deviation of 20, so it is shifted to the right relative to the gold curve and is wider.

Now let’s look at an another example in more detail. Serum cholesterol is an indicator of heart health, and for U.S. women ages 20-34, the population distribution for serum cholesterol looks very similar to a normal distribution. We can model this population distribution by using a Normal distribution with mean cholesterol of 185 mg/dL and standard deviation of 39 mg/dL, as shown in the plot above. 

We can then use this distribution to answer questions about probabilities. For example, what is the probability that a randomly selected U.S. woman in this age range would have a serum cholesterol level above 240 mg/dL? This probability turns out to be 0.079 or slightly under 8%.

Or… what is the probability that a randomly selected U.S. woman in this age range would have a serum cholesterol level below 240 mg/dL? This turns out to be 0.921 or about 92%.

Or… what is the probability that a randomly selected U.S. woman in this age range would have a serum cholesterol level between 200 and 240 mg/dL? This turns out to be 0.271 or about 27%.

So… how do we obtain these Normal probabilities? There are two ways. The first is to use the formula for the Normal distribution, and the second is to use statistical software.

$$
Pr(X \leq x) = Pr(X < x) = \int_{-\infty}^x \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}
$$

If the random variable X has a Normal distribution with known mean and standard deviation, we can calculate the probability of X being less than any specific value, x, using the Normal distribution equation given above.

This calculation requires knowledge of calculus to do the integration and, like any hand calculation, is prone to error. A better way is to use statistical software. 

We can use any statistical software package to calculate normal probabilities. It will require that we provide the name of the distribution (here, a Normal distribution), the parameters of the distribution (here, the population mean and the population standard deviation), and the specific value of interest.  

Software will typically by default give the probability of being less than the specific value of interest. If we want to calculate the probability of being greater than the specific value of interest, or the probability of being between two specific values, then we have to do a bit more work. 

What if we want to calculate the probability of being above a certain value?

Doctors use serum cholesterol levels to monitor heart health, and when the cholesterol level exceeds 240 mg/dL, doctors often start some sort of treatment or intervention, such as medication. What proportion of young women would need some sort of intervention?  In other words, what is the probability that a randomly selected U.S. woman between the ages of 20 and 34 would have a serum cholesterol level above 240 mg/dL?

Software will give us the probability that a randomly selected woman from this population would have a cholesterol level below 240 mg/dL, which turns out to be 0.921.  Since the probability of all events is 1, the probability of having a cholesterol level above 240 mg/dL is simply 1 – 0.921 or 0.079.

Alternatively, we could be interested in the probability of being between two values.

For example, serum cholesterol levels below 200 mg/dL are considered “healthy”, while serum cholesterol levels between 200 and 240 mg/dL are considered “at risk” – they are not high enough to warrant some sort of medical intervention, but the person’s heart health still needs to be closely monitored. We would like to know the proportion of U.S. women aged 20-34 who have serum cholesterol levels in this “at risk” range of 200-240 mg/dL. How do we calculate this?

Let’s look at what what we need and what we can get from software.

We want to know the proportion of women with cholesterol levels between 200 and 240 mg/dL.

Software can tell us the proportion of women with cholesterol levels at or below a specified value. So software can tell us the proportion at or below 200 mg/dL and the proportion at or below 240 mg/dL. 


We find the proportion of women with cholesterol levels between 200 and 240 mg/dL by subtracting the proportion with levels below 200 mg/dL (which is 0.650) from the proportion with levels below 240 mg/dL (which is 0.921), giving 0.921 – 0.650 or 0.271. We can say that 27.1% of U.S. women age 20-34 have serum cholesterol levels between 200 and 240 mg/dL. Equivalently, we could say that there is a 27.1% chance that a randomly selected U.S. woman age 20-34 would have a serum cholesterol level in the “at risk” range of 200 – 240 mg/dL.

We can also use software to turn the calculation around and find the values associated with a given percentage (or proportion) of women in the population.

For example, what if we wanted to know what range of values of serum cholesterol would include 95% of the women in our population of interest?

We can find that range using either software or our knowledge of normal distributions.

If we use software, we start with the fact that we want the middle 95% of values. This means that the remaining 5% of values would be divided evenly above and below this range. So, the lower end of the range would be the cholesterol level which is above the lowest 2.5% of values. This value is called the 0.025 quantile or the 2.5% percentile of this normal distribution. Similarly, the upper end of the range would be the cholesterol level which is below the highest 2.5% of values, or, equivalently, above the lowest 97.5% of values. This value is called the 0.975 quantile or the 97.5% percentile of this normal distribution.

Statistical software can give us any specified quantile of a normal distribution. We must specify the name of the distribution (Normal), the parameters of the distribution (here, the mean of 185 mg/dL and the standard deviation of 39 mg/dL), and the quantiles we are interested in (here the 0.025 and 0.975 quantiles).

In our example, software will tell us that the 0.025 quantile is at 107 mg/dL, and the 0.975 quantile is at 263 mg/dL.

In this particular case, we can also use our knowledge of the normal distribution to find the values directly, by using the “68 – 95 – 99.7 rule” for normal distributions. This rule tells us that 95% of the values will be within two standard deviations from the mean. In our example, the mean cholesterol is 185 mg/dL, and the standard deviation is 39 mg/dL. So the lower end of the middle 95% of the cholesterol values will be two standard deviations below the mean, at 185 – 2*39 = 107 mg/dL. Similarly the upper end of the middle 95% of the values will be two standard deviations above the mean, at 185 + 2*39 = 263 mg/dL.


Since all normal distributions are the same except for their mean and standard deviation, it can be convenient to standardize the data and model it using the Standard Normal distribution. 

We can standardize any continuous random variable X by taking each value and subtracting the mean (this tells us how far away from the mean that value is), and then dividing by the standard deviation (this tells us how many standard deviations away from the mean that value is). The resulting standardized random variable is typically called Z and its values are denoted as z or a “z-score”. Positive z-scores indicate that the value is above the mean, while negative z-scores indicate that the value is below the mean. Larger z-scores indicate that the value is further away from the mean than smaller z-scores.

In our serum cholesterol example, if a particular woman has a cholesterol value of 263 mg/dL, then her standardized cholesterol level would be (263 – 185)/39 or a z-score of 2.0. Her cholesterol level is 2 standard deviations above the mean cholesterol level. 

The Standard Normal distribution can be used to model a standardized continuous variable. The Standard Normal distribution has the same shape as any Normal distribution, but it always has a mean of zero and a standard deviation of 1.0. We write that Z is distributed as Normal with mean 0 and standard deviation 1. In symbols, Z ~ Norm(0, 1). 

Standardizing variables can come in handy when we don’t have access to software but need to find a probability. Most statistics textbooks contain tables which give the probability of being below Standard Normal values between -3.49 and 3.49 or between -4 and 4.  There are also websites and even apps that can find the probabilities for a standard normal value. 

Let’s use the Standard Normal distribution to address the question: what is the probability that a U.S. woman aged 20-34 has a serum cholesterol level warranting some sort of medical attention or intervention, that is, a cholesterol level above 240 mg/dL?

We start by finding the z-score for this value, which is 1.41. In other words, 240 mg/dL is 1.41 standard deviations above the mean value.   

We can then use software, a table, or a website to find the probability associated with a z-score of 1.41. Since we need the probability of being above 1.41, we can certainly find the probability of being below 1.41 and subtract that value from 1. But we can also take advantage of the symmetry of the Standard Normal distribution around its mean of 0 and find the probability of being below -1.41, since that is the same as the probability of being above 1.41. Using the standard normal table, that probability is 0.0793.

The Normal distribution is important in statistics for a number of reasons. One reason is that the sample or population distributions for some continuous variables that we encounter in real life look approximately Normal. Another reason is that the sampling distribution for sample means is approximately Normal if the sample size is large enough. (We will explore sampling distributions in the next unit.) A third (although minor) reason is that the Binomial distribution (which we explored in an earlier lecture) becomes approximately Normal if the sample size is large enough, even though it is a distribution for discrete variables. 

The Binomial distribution has a mean and standard deviation that depends on the sample size (or the number of attempts), n, and the probability of the event, p, as shown on the slide above. The mean, np, is the expected number of “successes” or events in n attempts. The standard deviation increases as the number of attempts, n, increases. Therefore, as the sample size increases, the mean of the Binomial distribution increases (so the peak of the distribution shifts to the right) and the standard deviation increases (so the distribution broadens). 

If the sample size, n, is large enough, the Binomial distribution looks nearly Normal, as shown for n=30 and p=0.379 in the plot above. It can therefore be modeled using a Normal distribution using the same mean and standard deviation for that particular Binomial distribution.

How large a sample size is “large enough”? This depends on the probability of the event of interest, p. The sample size needs to be large enough that we’d expect to see at least 10 participants in the sample who have the event (np), and at least 10 participants in the sample who don’t have the event of interest (n(1-p)). 

The fact that the Binomial distribution is approximately Normal for large n used to be helpful in doing statistical calculations, since Normal distribution probabilities were readily available in tables. This is called using the “Normal approximation to the Binomial”. It is becoming less helpful now, since exact Binomial probabilities are easily calculated using statistical software. 

There are many other theoretical distributions relevant to specific kinds of continuous random variables, including the Exponential distribution, the Lognormal distribution, the Beta distribution, and the Gamma distribution. These distributions will not be discussed further in this course.






<!--chapter:end:02-sampling.Rmd-->

# Sampling Distributions
In this lecture, we will introduce the abstract concept of sampling distributions and their importance to statistics. In an earlier lecture, we learned about common theoretical distributions, such as the Binomial distribution and the Normal distribution. We also learned about population parameters that define these distributions, such as p, µ, and ?.

Theoretical distributions and their parameters are useful when we are trying to understand the probability of observing various outcomes in random samples given a known characteristic of the population. 

For example, suppose for the population of people who suffer from occasional migraine headaches, 60% of them get some relief from taking ibuprofen. 60% is assumed to be a known characteristic (or parameter) of the population. One question we could answer based on this information is, if we have a random sample of 50 people who suffer from occasional migraines, what is the chance that 10 of them will get relief from ibuprofen? We can use the Binomial distribution to answer this question. In this example, we are working from the known population parameter to tell us information about the unknown sample.However, when analyzing data, we are trying to make an inference about the population given the result or estimate from the sample. It essentially is working the opposite way from that described on the previous slide. 

Using the same example as the previous slide, suppose we are interested in investigating how effective ibuprofen is for people who suffer from occasional migraine headaches. To do this, we go out and obtain a sample from the population and get an estimate of the proportion who get relief after taking ibuprofen. We could then use the results from the sample to make an inference about the population parameter of interest, the proportion who get relief. So the unknown characteristic here is the population parameter and the known part is the result from our sample. Theoretical distributions are used during the process of making an inference but as a way to model the behavior, or distribution, of the sample statistic. 

What do we mean by this last sentence? This lecture addresses that question, which is the foundation for statistical inference.Suppose we want to know the proportion of people in the United States who have diabetes. One approach would be to survey everybody in the United States and calculate the proportion of them that have diabetes. Suppose this population proportion is 0.093 or 9.3%1..

References:
1 CDC (2014): https://www.cdc.gov/media/releases/2014/p0610-diabetes-report.html
Images: 
Outline of US: https://pixabay.com/en/us-map-outline-us-map-america-1674031/
People in crowd: https://pixabay.com/en/people-group-crowd-colorful-308531/But, suppose that we didn’t know the population proportion (it’s a black box) and that it isn’t possible to collect data from everybody. Instead, we have to obtain a representative sample of people from the population, make measurements on them, and then use that information to infer something about the proportion in the entire population. How can we use information from a single sample to say something about the population? We have to understand a key thing about samples: sampling variability.

References:
Images: 
Outline of US: https://pixabay.com/en/us-map-outline-us-map-america-1674031/
People in crowd: https://pixabay.com/en/people-group-crowd-colorful-308531/What is sampling variability? 

Sampling variability is the term we use to explain what happens when the random sampling process is repeated. It tells us what we intuitively know. If many random samples were collected, the measured quantities, or sample statistics, from those samples will vary from one another. Additionally, the sample statistic we obtain will (most likely) be different from the population quantity, or parameter, of interest. 

We need to be able to understand the behavior of how the sample statistics vary in order to be able to make an inference about the population parameter. This information about how much a statistic varies from sample to sample is key in helping us know how accurate an estimate is. How can we do this? We can simulate what would happen if random samples of the same size were repeatedly taken.Let’s go back to the Diabetes example to examine the variability of the proportion from sample to sample. 

Suppose researchers obtain a random sample of 100 people from the United States. They calculate the proportion in their sample (denoted p-hat) who have diabetes and find that 0.110 or 11% of those in their sample have diabetes. Two other researchers do the exact same thing. In their random samples of 100 people, one finds that 7% have diabetes and the other finds 12% have diabetes. Each of these values are an estimate of the true population proportion. 

Just by chance, these researchers collect samples that have a higher or lower value than the true population value of 0.093, and the sample statistics are each different from one another. This is an example of sampling variability. However, taking only three samples does not give us a very good understanding of the behavior of the sample proportions in repeated sampling. We need to see what happens if we repeat the sampling process many, many more times. 

References:
Images: 
People in crowd: https://pixabay.com/en/people-group-crowd-colorful-308531/

If the study with random samples of size n = 100 is repeated 1,000 times (that is, collect 1,000 samples), and the calculated sample proportion for each sample is plotted, a dot plot of the sample proportions would look like this. If we repeat our study over and over indefinitely, the collected sample proportions would form the sampling distribution of sample proportions – defined as the distribution of proportions from all possible samples of this size. In general, whenever a distribution is made up of sample statistics (e.g., means, medians, standard deviations), the distribution is called a sampling distribution of that statistic.

What do we notice about the behavior of the sample proportions over repeated random samples? Recall that when we evaluate distributions of numerical data, we look at the shape, the center, and the spread. 

The shape of the distribution of sample proportions is approximately bell-shaped, with the center being around approximately 0.093 (the population proportion), and the range of values spanning 0.02 to 0.18, with the average deviation away from the mean equaling 0.029. Recall that the average deviation away from the mean is another way of saying standard deviation. When we are describing the sampling distribution, we call the standard deviation of the sample statistics standard error (denoted std. error in the plot).But, what would have happened to the sampling distribution of sample proportions if the samples each had n = 500 instead of n = 100 observations? The dot plot above shows the sampling distribution of that scenario. 

From this dot plot, we see that the shape is still approximately bell-shaped and the center is still at the population proportion of 0.093. However, compared to the sampling distribution of sample proportions from samples of size 100, the sampling distribution of sample proportions for samples of size 500 has a much smaller spread. The sample proportions on the previous slide ranged from 0.02 to 0.18, and in this case the sample proportions range in values from around 0.06 to about 0.14 and have a standard error of 0.013.To drive the point home, let’s examine one more scenario. What do you think would happen if samples each had n = 1,000 observations, instead of n = 500 or n = 100? The dot plot above shows the sampling distribution of that scenario. 

Similar to the last two dot plots, we still see that the shape is approximately bell-shaped (but even more bell-shaped than the previous two) and the center is at the population proportion of 0.093. But now, the spread of the sample proportions is even smaller. The sample proportions range in values from around 0.064 to 0.125 and have a standard error of 0.0093.What general behaviors were noticed as we examined the three sampling distributions of sample proportions?

The first behavior pertains to shape. As the sample size (n) increased, the sampling distribution of sample proportions looked more bell-shaped. This bell-shaped pattern is observed in the sampling distributions for many statistics (but not all) and will play an important role in the majority of inferential methods we use in this course. 

The second pattern observed is that the center of the sampling distribution was always located near the population parameter that we are trying to estimate. It would be exactly on the population parameter had we sampled an infinite number of times (which is the precise definition of sampling distributions), but because there were only 1,000 samples in each of the dot plots (which was done for demonstration sake), the center was not always quite equal to the population parameter. This idea of the center of the sampling distribution being equal to the population parameter is important because we want to know we are “hitting” our target–the parameter of interest–on average across all possible samples. That is, our statistic is neither overestimating nor underestimating the value it is trying to estimate; but rather, it is hitting the parameter of interest, on average. 

Lastly, as the sample size increased, the width or spread of the sampling distribution decreased. That is, as n increased, the sample statistics tended to be closer to the true population parameter value, thus making the variability of the sample statistics smaller. Besides the shape of the distribution, knowing how the statistics vary from sample to sample (i.e., the spread of the sampling distribution) is what we really care about. We use this information in making an inference from the sample to the population because it tells us how precise an estimate is. Let’s review the different distribution types that have been presented thus far: population distributions, sample distributions, and sampling distributions.

Recall from a previous lecture that population distribution describes the distribution of a characteristic of that population. It displays the proportion or probability of the values that make up the characteristic. As we learned in this lecture, typically, this distribution is unknown, and information about that distribution (e.g., population parameters; population proportion p) are often what we want to try to estimate. In the Diabetes example, we assumed the population proportion of people in the United States who have diabetes was 0.093. 

However, what we most likely have access to is a sample of data from the population. If we plot the data from our sample, we create a sample distribution, which is a distribution of a characteristic of the sample. It is similar to the population distribution but it most likely will have different proportions of the values than the population. We use the information from the known and observed sample to help us estimate the unknown population value/parameter of interest. 

Unlike population and sample distributions, which are distributions of cases or observations, sampling distributions are distributions of statistics, where each “dot” (a.k.a., sample statistic) is aggregate information from a sample of observations and many, many, many ”dots” (a.k.a., sample statistics) are obtained so we can understand the behavior of the ”dots”. While we can simulate what the sampling distribution will look like (as we have in this lecture), we do not observe this distribution directly in real life. This distribution is an abstraction of what would happen if we could mimic the sampling behavior over and over again. It helps us understand the sampling variability of the statistics so that we can make an inference about the population from the sample. 

While sample distributions are an important part of the data analysis process, sampling distributions are the foundation for statistical inference (as mentioned earlier).The concept of sampling distributions for sample proportions was presented in this lecture. But, any statistic that we calculate from a sample has its own sampling distribution. For example, there is a sampling distribution for sample means (which will be discussed in a future lecture), or for sample relative risks. As we discussed earlier, many statistics have sampling distributions that are bell-shaped, but some statistics have sampling distributions that are not bell-shaped (e.g., sample median, sample standard deviation, sample relative risk, sample odds ratio). 

<!--chapter:end:03-sampling_distributions.Rmd-->

# Applications

Some _significant_ applications are demonstrated in this chapter.

## Example one

## Example two

<!--chapter:end:04-application.Rmd-->

A sample proportion is a ?point estimate? of a population proportion. In contrast, a confidence interval for a proportion is an ?interval estimate? of a population proportion that reflects the precision of the estimate. A confidence interval gives a plausible range for a population proportion based on observed data. Confidence intervals can also be used to make inferences about a population proportion. This lecture will introduce the concept of confidence intervals, focusing on confidence intervals for a proportion.

We learned earlier that theoretical distributions and their parameters are useful when we are trying to understand the probability of observing various outcomes in random samples given a known characteristic of the population. For example, if we know the true prevalence of diabetes in the population, we can use the Binomial distribution to say things about random samples from that population. This is an example of the upper arrow in the figure above.

However, when analyzing data, we have a single sample in hand that we want to use to say something about the population it came from. For example, if we calculate the proportion of people with diabetes in a sample, what does this tell us about the proportion with diabetes in the entire population? This would be an example of the lower arrow in the figure above: using information about the sample to find out about the population. 

How do we do this? The key, as we will see, is to use sampling distributions!Suppose we are interested in understanding the prevalence of diagnosed diabetes in younger adults aged 20 - 44 years old in the United States. How would we obtain an estimate of this number? How confident are we that we have indeed estimated accurately the percent of people in this age group with diabetes in the United States?  

Let?s assume that we have on hand a representative sample of 500 Americans between 20 and 44. Now suppose we ask each one ?Have you been diagnosed with diabetes??, and 53 of them say ?Yes?. If we divide 53 by 500 we obtain the sample proportion, the proportion of younger adults in our sample who have diabetes, in this case 10.6%. 

The sample proportion, which we refer to as p-hat, is an estimate of the true proportion, p, of diabetic younger adults in the entire US population. It may be somewhat lower than the true population proportion, or it may be somewhat higher.  But how much higher or lower is plausible given our sample? Getting an estimate of 10.6% from a sample of 500 seems pretty unlikely if the true population proportion is, say, 50%. But is this estimate consistent with a true population proportion of, say, 14%? We can use confidence intervals to tell us.
A confidence interval is an interval estimate that tells us about the precision of the estimate. Loosely, we can think of it as a range of true population parameter values that are plausible given the observed data. 

Without worrying for now about how we calculated it, a 95% confidence interval for the true population proportion from our diabetes study is 7.9% to 13.3%. This interval is telling us that true population proportions between 7.9% and 13.3% are relatively plausible given our observed sample proportion of 10.6%. 

There are a few ways of writing out the confidence interval, including ?lower CI value? to ?upper CI value? (e.g., 7.9% to 13.3%), or placing parentheses around the two CI values and separating them with a comma (e.g., (7.9%, 13.3%)). Confidence intervals for a proportion can denoted either as proportions or as percents. 

So what does the 95% mean? Informally, it quantifies how confident we are that the true population proportion falls in the given range. We?ll give a more formal definition shortly.

So how did we calculate this confidence interval from the data?Determining a confidence interval requires us to invert or ?flip? our thinking, to go from ?known? sample to ?unknown? population (instead of ?known? population to ?unknown? sample as we did when we explored sampling distributions). 

Let?s think about the sampling distribution of sample proportions again. The dot plot above shows the values of the sample proportion, p-hat, calculated for each of 1,000 samples, each with size n=500, from a population that has a known population proportion, p, of 0.093 or 9.3%. Previously, we saw that the sampling distribution for p-hat (the sample proportion) is approximately Normal (a.k.a. bell-shaped) with a mean equal to the true population proportion of 0.093 and a standard error (i.e., standard deviation of the sampling distribution) equal to 0.013, which mathematically works out to be the square root of p*(1-p)/n. 

{It is important to recall that the standard error (denoted as SE) is just the ?fancy? name denoting a special case of the standard deviation (denoted as SD). The standard error is the standard deviation when applied to a sampling distribution for a statistic. But it?s still a standard deviation (period). Using the term ?standard error? just alerts us that it describes a sampling distribution and not the population or the sample observations (our data), which would both be described with the term ?standard deviation?.} 

Since the sample size, n, is big here, the observed sample proportions are pretty close to being a Normal distribution. We can see that this is the case by observing the Normal curve that is laid over the dots in the plot and noting how similar they are. This Normal curve has the same mean and standard deviation (a.k.a. standard error) as the sampling distribution; that is, it has a mean of 0.093 and a standard deviation of 0.013. So, using this Normal approximation, what can we say about the likely values of p-hat? Using the 68-95-99.7 rule of Normal distributions, roughly 68% of the sample proportions will fall within one standard deviation (a.k.a. one standard error) of the mean of 0.093, and about 95% of them will fall within two standard deviations (a.k.a. two standard errors) of the mean of 0.093. Building on the ?within 1 or 2 SE from the mean? idea, let?s see what the cutoff values for these sample proportions would be. 

The dot plot on the previous slide has been removed from the plot and all that remains is the Normal curve with same mean of 0.093, the true population proportion, and the same standard error of 0.013. This is depicted in the top curve on the slide. 

If we wanted the middle 68% of the sample proportions, we would take p, the mean of the sampling distribution for sample proportions, plus or minus 1*SE. So the middle 68% of the sample proportions are between 0.080 and 0.106. Similarly, the middle approximately 95% of the sample proportions are between 0.067 and 0.119, which is p plus or minus 2*SE. 

In fact, we can use the properties of Normal distributions to tell us how many standard errors above and below 0.093 we have to go to contain any arbitrary percentage of the data. For example, it turns out that a range of plus or minus 1.64 standard errors from the mean contains approximately 90% of the data, so in our case, about 90% of the sample proportions should fall between 0.072 and 0.114.  

Recall that we can standardize data by calculating ?z-scores? that tell us how many standard deviations the observation is from the mean. If the data are statistics, such as sample proportions, then the z-score tells us how many standard errors away from the mean the sample proportion is. The same sampling distribution for the sample proportion is shown in the lower curve on the slide, in standardized form with the z-score on the horizontal axis.

Note that the product of the value of the z-score and the standard error is known as the margin of error.  Using what we know about where values of p-hat are likely to fall given the true value of p, we can ?flip? our thinking and say something about where p might lie based on the observed value of p-hat. That is, we have just seen that 95% of the sample proportions will lie within ~2SE of the true proportion. Flipping this around tells us that for any given sample proportion, there is a 95% chance that the true proportion will lie within ~2SE of it (and a 5% chance that the true proportion will be further than ~2SE from it). 

This leads us to the formula for the confidence interval of a proportion: p-hat +/- its margin of error. The margin of error consists of two parts: the z-value, which is determined by the degree of confidence we want, and the standard error of p-hat.

Notice that we?ve dressed up the z-value term a little by adding the subscript 1 ? alpha/2. This subscript indicates the appropriate z-value corresponding to a specified level of confidence. For instance, for a 95% confidence interval, alpha equals 1 ? 0.95 or 0.05. Therefore, 1 ? alpha/2 equals 0.975 which tells us that our z-value should be the number that exceeds 97.5% of the observations in a Standard Normal distribution. It turns out that this number is about 1.96. 

Next we move on to the second component, the standard error of the sample proportion, p-hat. It turns out that the formula for this standard error involves the true population parameter, p. However, this is a problem since p is unknown! What do we do? We simply replace p with its estimate, p-hat. This results in an estimated standard error rather than the true standard error, which uses p.
For our example, the sample proportion of young adults with diagnosed diabetes was 53/500 = 0.106, so p-hat = 0.106 and the estimated SE(p-hat) = 0.014 (Notice that the estimated SE of p-hat is slightly different than the true SE of p-hat on slide 5.) For a 95% confidence interval, alpha = 0.05 and the z-value is 1.96. This gives a margin of error of 1.96*(0.014) = 0.027. Our 95% confidence interval for the true population proportion of young adults who have diabetes is therefore 0.106 +/- 0.027, which gives (0.079, 0.133). 

Now, how do we interpret this range? We can say things like:
?I am 95% confident that the interval between 0.079 and 0.133 contains the true proportion of US young adults with diabetes.? 
Or
?A plausible range of values for the true proportion of US young adults with diabetes is 0.079 to 0.133.?

But these interpretations are somewhat ambiguous. What does ?95% confident? mean? How do we define ?plausible??

Next, we will define precisely what we mean by a 95% (or any %) confidence interval.This plot shows our sample proportion, 0.106 (black dot) and the true population proportion, 0.093 (solid red vertical line). We see that our sample proportion, while not the same as the population proportion, is close.

In this case, the 95% confidence interval (or CI) constructed from our one sample contains the true population proportion. How often will this be the case?Let?s say we were able to obtain 50 different samples of size n=500 from the same population. From each sample, we calculate a confidence interval for the true population proportion. The plot above shows 95% confidence intervals for 50 hypothetical random samples taken from our US young adult population where the true proportion of diabetes is 0.093 or 9.3%. We see that the sample proportions, p-hat, vary a bit from sample to sample as indicated by the dots in the center of each interval. For 47 of the samples, the calculated 95% confidence interval contains the true population proportion of 0.093. But for three of the samples, the calculated confidence interval misses the true population proportion (as indicated by the red horizontal lines in the plot above). And, lo and behold, 47 out of 50 is 94%, which is very close to the 95% confidence level we used to calculate the confidence intervals. 

This isn?t a coincidence. If we repeated the study and obtained 1,000 samples of size n=500, instead of 50 samples, we would get 1,000 slightly different point estimates and 1,000 slightly different confidence intervals, due to sampling variability. Those calculated confidence intervals would contain the true population proportion of 0.093 in about 950 (or 95%) of the samples, but would miss the true value in about 50 (or 5%) of the samples. 

This plot tells us how to precisely define a 95% confidence interval: A 95% confidence interval is an interval such that, when estimated on repeated samples, approximately 95% of those estimated intervals will contain the true population parameter and approximately 5% will miss it. 90% confidence intervals will ?hit? 90% of the time and ?miss? 10% of the time; 99% confidence intervals will hit 99% of the time and miss 1% of the time, and so on.What if we wanted to be more than 95% confident? If we wished to be 99% confident, then our confidence interval would need to be wider, so that 99% of the time it would ?catch? the true population value and only 1% of the time it would ?miss? it. A confidence interval is kind of like a butterfly net: a wider net has a better chance of catching the butterfly. In our imaginary study, a 99% confidence interval for the true proportion of US young adults with diabetes is 0.071 to 0.141.

What if we wished to be less than 95% confident? If we wished to be 90% confident, then our confidence interval would need to be narrower, so that it only ?catches? the true population value 90% of the time. An interval with a lower level of confidence will produce a confidence interval that is narrower than one with a higher level of confidence. 

Recall that the z-value in the confidence interval calculation is determined by the desired confidence level. The z-values for three common confidence levels are:
1.645 for a 90% confidence level, 
1.96 for a 95% confidence level, and
2.575 for a 99% confidence level. 

Increasing the confidence level is a double-edged sword. On one hand, we can feel more confident that our interval will contain the truth. But on the other hand, our interval will be wider, so it gives us less information about where the true value lies. Conversely, decreasing the confidence level makes it more likely that we will ?miss? the truth, but in exchange our intervals will be narrower.

So, where does all this leave us when we are staring at one confidence interval calculated from the one sample we have on hand? Since we don?t know the true population parameter, we don?t know whether this particular interval has ?hit? or ?missed? the true value. But, since we know that confidence intervals will ?hit? the true value a relatively high percentage of the time (a percentage of the time that we determine by fixing the confidence level), we take a sort of ?leap of faith? and view the interval as a plausible range for the true population parameter. We have mentioned several times the idea of ?flipping? our thinking when it comes to confidence intervals. To recap, the ?flipping? part comes when instead of thinking about the variation among the proportions from many samples when we ?know? the population proportion, we are thinking about estimating an ?unknown? population proportion from a single sample. Let?s connect the concept of the sampling distribution for a proportion (when it is approximately Normal) to the concept of confidence intervals.

The same image of multiple confidence intervals estimated from multiple samples is presented as from a previous slide. But this time, the sampling distribution for a proportion with mean 0.093 and standard error of 0.013 is overlaid on top of the confidence intervals. The dark blue vertical lines at 0.068 and 0.118 (that is, at ~2SE below and above the mean) denote the middle 95% of the sample proportions in the sampling distribution. That is, 95% of the sample proportions fall within ~2 standard errors of the mean, the population proportion, but 5% of the sample proportions are outside of this region. So while most of the sample proportions are ?near? the population proportion, just by chance we might get a sample proportion that is ?far away? from the population proportion. 

Notice that the 47 sample proportions (blue dots) that are within the middle 95% of the sampling distribution have a confidence interval that contains the population proportion. In contrast, the 3 sample proportions (red dots) that are not within this region have a confidence interval that doesn?t contain the population proportion. 

To put it all together now, based on the sampling distribution of sample proportions, we know that for 95% of the sample proportions (the ones in the middle of the sampling distribution) the true population proportion is going to be within ~2SE of that sample proportion, and for the other 5% of the sample proportions (the ones in the tails), the true population proportion is going to be further than ~2SE from that sample proportion. So if we put an interval of +/- ~2SE around?any?observed sample proportion, then 95% of the time, this interval will contain the true population proportion.A confidence interval method is a ?recipe? for calculating a confidence interval based on observed data. And just as in cooking, there are different recipes for making the same dish, and each recipe has its own strengths and weaknesses.

The Wald method is the one we have just described. It generally works fairly well, but becomes less accurate when the sample size is small. 

The modified Wald method involves a small tweak to the standard confidence interval formula that improves its behavior in small samples. For more information on this method, see the following website: https://www.graphpad.com/support/faq/the-modified-wald-method-for-computing-the-confidence-interval-of-a-proportion/. 

Because they are calculated by adding and subtracting the margin of error from the sample proportion, both the Wald and modified Wald methods generate symmetric confidence intervals. This can be a problem if the sample size is very small or the estimated proportion is close to 0 or 1, because then the confidence interval may include values below zero or above 1, which are impossible values for a population proportion. An alternative confidence interval recipe that avoids this problem is the Clopper-Pearson Exact Binomial method. The word ?exact? refers to the fact that this method does not rely on the Normal distribution approximation; instead, it uses the fact that the sampling distribution of the sample proportion is a re-scaled version of the Binomial distribution. Confidence intervals estimated using the Clopper-Pearson method may not be symmetric, but will never contain values outside the range of 0 to 1. Like most exact methods, Clopper-Pearson confidence intervals usually require a computer to calculate; they cannot be easily hand-calculated like Wald confidence intervals. See the following website if you are interested in the formula: https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/PASS/Confidence_Intervals_for_One_Proportion.pdf. 

No matter which recipe is used, as the size of the sample increases, the confidence interval gets narrower, which means that the estimate is more precise. Recall that the process of using a sample proportion to estimate a population proportion is an example of statistical inference. Statistical inference for a proportion relies on several assumptions. 

We assume that the sample is a random (or representative) sample from the population of interest. In this way, we are able to generalize the results from this sample to the population it came from. This assumption would be violated, for example, if a sample was obtained in a way that under-represented people of color. The prevalence of diabetes is higher in people of color than in whites, so the results from a non-representative sample would not be generalizable to the U.S. young adult population as a whole.

We assume that the observations are independent. This assumption would be violated, for example, if siblings were included in the sample. Siblings may share genetic and environmental risk factors for diabetes and so might be more similar to each other than two people chosen at random would be.

If we are using the Wald method, we assume that the sample is ?large enough? for the sampling distribution of sample proportions to be approximately Normal. For proportions, ?large enough? requires at least 5 events (or ?successes?) and at least 5 non-events (or ?failures?). In our example, this assumption would be met if our sample contained at least 5 people with diabetes and at least 5 people without diabetes. If the sample size is too small, consider using other methods, such as the exact binomial method, to compute confidence intervals.

If these assumptions are violated, the confidence intervals we calculate may give us faulty information about the true population proportion. For instance, intervals might be too narrow, suggesting a more precise estimate than we actually have, or they might be centered at the wrong place, and hence more likely to ?miss? the true population proportion.

<!--chapter:end:04-confidence_intervals.Rmd-->

In an earlier lecture, we described the behavior of sampling distributions for a sample proportion, in order to understand how we can use a sample proportion to make inferences about a population proportion. In this lecture, we will describe the behavior of sampling distributions for a sample mean, in order to understand how we can use a sample mean to make inferences about a population mean. Sampling distributions for sample means behave similarly to those for sample proportions, but there are some important differences.Suppose we want to know the average body mass index (BMI) of US adults whose age is between 30 and 60 years old. Suppose that BMI measurements could somehow be collected from all US adults between those age ranges. This plot shows results that might be obtained from such a census*. A plot that shows the measurement results for an entire population is known as a population distribution. The BMI measurements in this census fall between 14 and 57 kg/m2. Half of the adults have BMI values less than the median BMI of 25.13 kg/m2, and the mean BMI was 25.58 kg/m2. The shape of the population distribution of BMI values in US adults appears to be more or less Normal but skewed to the right (or positively skewed), with a long right tail of very high BMI values.

References: 
*These BMI measurements were obtained from 5209 people as part of the Framingham Heart Study (http://www.framinghamheartstudy.org). This population may not be exactly like the entire US population in all respects.It is not usually possible or feasible to obtain measurements on the entire population of interest. Rather, we usually obtain a sample of observations. 

Suppose we conduct a study to investigate the average BMI of US adults, aged 30-60: we obtain a single sample, a simple random (and therefore representative) sample of n=100 adults from the population shown on the previous slide. A dot plot for the 100 BMI values in our sample is shown here, with one dot for each person in the sample. A plot that shows all of the measurement results from a sample is called a sample distribution. The mean BMI in our sample is 25.47 kg/m2, which is slightly lower than the true population mean of 25.58 kg/m2. 

What would the sample means look like if we repeated the sampling process a few more times?

Suppose we obtain three more random samples of 100 adults from the same population. The sample distributions of BMI values for these three samples are shown here. Each of the sample distributions has a somewhat different shape, a somewhat different mean, and a somewhat different standard deviation, as we would expect due to sampling variability. 

However, collecting only three samples does not give us a very complete understanding of the behavior of the sample means in repeated sampling. Let?s see what happens if we repeat the sampling process many, many more times. 
If we repeat the sampling process 1,000 times, collecting samples of size n=100 each time, and we calculate the mean BMI from each sample, a dot plot of all 1,000 sample means would look like this. Recall that true sampling distributions consist of the sample statistics from all possible samples. This distribution of only 1,000 sample means is an approximation of the true sampling distribution for samples of size n=100. We are using this approximate sampling distribution to help us better understand the concept of sampling distributions. 

What shape does this sampling distribution have? The shape appears very close to Normal (bell-shaped). Surprisingly, even though the underlying population of BMI values in US adults is right-skewed, the sampling distribution of sample means from repeated samples of size n=100 appears to be very close to symmetric. 

Where is the center of this sampling distribution? The mean of this sampling distribution (25.59 kg/m2) is very close to the true population mean of 25.58 kg/m2. This discrepancy is because we only took 1,000 samples and not an infinite number of samples. If we were able to take an infinite number of samples, the mean of the sampling distribution would exactly equal the true population mean.

What about this sampling distribution?s spread? The sample means range from about 24 to 27 kg/m2. The standard deviation (or standard error) of this sampling distribution is 0.42 kg/m2.  What would happen if we repeated the sampling process, but this time we used a much larger sample size of n=500 instead? A dot plot of all 1,000 sample means would look like this. How has increasing the sample size from n=100 on the previous slide to n=500 on this slide affected the sampling distribution?

The shape of this sampling distribution has not changed a lot. It is still very close to Normal: bell-shaped, unimodal and symmetric.

The center of this sampling distribution, given by the mean of 25.58 kg/m2 , which equals the true population mean of 25.58 kg/m2.  

The biggest difference is in the sampling distribution?s spread. Before, with a sample size of n=100, the sample means ranged from about 24 to 27 and the standard error of the sampling distribution was 0.42 kg/m2. 
Now, with a sample size of n=500, the sample means range from about 25 to 26 and the SE is 0.19 kg/m2. With a larger sample size, the sampling distribution is narrower. With a larger sample, the sample means from the various samples are all much more closely clustered around the true population mean.

Note that the number of samples is identical in both cases; it is only the sample size that differs. Increasing the number of samples that we take does not narrow the sampling distribution, although it does give us a clearer picture of its shape. It is only increasing the size of each sample taken which will narrow the sampling distribution.What would happen if the sample size were very small? Suppose we repeated the sampling process, but this time we used a very small sample size of n=20 instead? A dot plot of all 1,000 sample means would look like this. How has decreasing the sample size from n=100 on a previous slide to n=20 on this slide affected the sampling distribution?

The shape of this sampling distribution has changed a bit. Unlike in previous slides, it has a slight right skew, but it?s fairly close to Normal: bell-shaped, unimodal and symmetric. 

The center of this sampling distribution, given by the mean of 25.61 kg/m2, is again very close to the true population mean of 25.58 kg/m2.  

The biggest difference is in the sampling distribution?s spread (notice the limits on the x-axis changed from 24 to 27.5 in the two previous slides to 22.5 to 29.5 on this slide). Before, with a sample size of n=100, the sample means ranged from about 24 to 27 and the standard error of the sampling distribution was 0.42 kg/m2. 
Now, with a sample size of n=20, the sample means range from about 22.5 to 29.5 and the SE is 0.95 kg/m2. With a SMALLER sample size, the sampling distribution is WIDER. With a SMALLER sample, the sample means from the various samples are MORE WIDELY SPREAD AROUND the true population mean.
What general behaviors have we noticed as we examined the sampling distributions both for sample proportions (in an earlier lecture) and for sample means (this lecture)?

The first behavior we observe pertains to shape. As the sample size (n) increases, the sampling distribution both for sample proportions and for sample means looks more bell-shaped, unimodal and symmetric? that is, more Normal in the statistical sense. 

The second pattern we observe is that the center of the sampling distribution is always located very close to the true population parameter, even for relatively small samples.

Lastly, we notice that as the sample size increases, the width or spread of the sampling distribution decreases. That is, as the sample size, n, increases, the sample statistics tend to be closer to the true population parameter value, thus making the variability of the sample statistics smaller. What we have observed in these sampling distributions is actually a fundamental theorem in statistics, the Central Limit Theorem (frequently abbreviated as CLT). 

The Central Limit Theorem tells us two things. First, the Central Limit Theorem states that if we choose a large enough sample size, n, then the sampling distribution of the sample means will be approximately Normal (unimodal, symmetric and bell-shaped) EVEN IF the underlying population is not Normal at all (for example, multimodal or severely skewed). 

If the population distribution itself is Normal, then the sampling distribution of the sample means will be Normal for any sample size, even a sample size of n=1. The next logical question is ?how large of a sample is ?large enough? for the CLT to hold for the distribution of sample means?? 

That depends on the shape of the population distribution. 

If the shape of the population distribution is completely Normal (that is, if the measurement of interest is truly Normally distributed in the population), then a minimum of a sample size of 1 is needed in order for the distribution of sample means to be approximately Normal. In other words, if the population is truly Normal, then the sampling distribution will be Normal, for ANY size of sample.

If the shape of the population distribution is not Normal but is approximately symmetric, then one rule of thumb is that a minimum sample size of about 15 is needed in order for the sampling distribution of sample means to be roughly Normal.

If the shape of the population distribution is not Normal and not symmetric, but instead is skewed, then one rule of thumb is that a minimum sample size of about 30 is needed in order for the sampling distribution of sample means to be roughly Normal. Keep in mind, though, that the less Normal or the less symmetric the population distribution is, the larger the sample size that will be needed to ensure that the sampling distribution of sample means will be roughly Normal.The second thing the Central Limit Theorem tells us is that the sampling distribution of sample means will be centered at the true population mean, ? (mu), and will have a standard error, SE (sometimes called the standard error of the mean, SEM), equal to the population standard deviation, ? (sigma), divided by the square root of the sample size, n. 

This applies to the true sampling distribution, which is for all possible samples of size n. As previously mentioned, our example sampling distribution contained only 1,000 samples of size n=100, not all possible samples, so it is a good but not quite perfect approximation of the true sampling distribution. Its sample mean is nearly but not quite equal to the true population mean and its sample standard error is nearly but not quite equal to the population standard deviation divided by the square root of n. In our BMI example, the mean of our approximate sampling distribution after obtaining 1,000 samples of size n=100 (25.59 kg/m2) is very close to the true population mean of 25.58 kg/m2. The true population standard deviation is 4.24 kg/m2 and the sample size is 100, so the standard error, SE, of the sampling distribution should be 0.424 kg/m2, which is quite close to the value we saw in our approximate sampling distribution plot (0.42 kg/m2). 

An important thing to notice: the SE calculation shows us that as the sample size increases, the standard error decreases, and the sample means will get closer and closer to the population mean. This is consistent with what we observed earlier when we compared the sampling distributions for samples of size n=20, n=100 and n=500.

Note that we do not use the Central Limit Theorem to get a better approximation of the population parameter of interest by using lots of samples. In most cases, it isn?t practical or even possible to re-sample from the same population thousands of times. In most cases, we only ever get one sample and we need to make our inferences about the population based on that one single sample. What we DO use the Central Limit Theorem for is to tell us how close our sample statistic is likely to be to the true population parameter, i.e. how well the sample statistic estimates the true population value. 
The Central Limit Theorem also applies to sample proportions. In this case, the CLT states that if we choose a large enough sample size, the sampling distribution of sample proportions from random samples of size n will be approximately Normal (unimodal, symmetric, and bell-shaped) and will be centered around the true population proportion, p, with a standard error equal to the square root of p*(1-p)/n.

Totally optional, but if you are curious: The CLT only applies to means, but it happens to also work for proportions because a proportion is in fact a mean of a Bernoulli distribution of 1?s (e.g. diabetic) and 0?s (e.g. not diabetic). A Bernoulli distribution isn?t even remotely close to Normal in shape (see the earlier lecture on sampling distributions for a sample proportion), but the sampling distribution of sample proportions is nevertheless, per the CLT, approximately Normally distributed.

Again, how large is ?large enough?? 

Recall that we find the proportion for an event or category of interest. In our example from an earlier lecture, the ?event? we were interested in was the proportion of young adults with diagnosed diabetes. One common rule of thumb is that the sampling distribution of sample proportions will be approximately Normal if the number of events or ?successes? (n*p) and the number of non-events or ?failures? (n*(1-p)) are both at least 10. In our example, this assumption would be met if our sample contained at least 10 people with diabetes and at least 10 people without diabetes. 

If p, the population proportion, is unknown, then we use the sample proportion, p-hat, to estimate it. As we did earlier for proportions, let?s review the different distribution types that have been presented thus far for means: population distributions, sample distributions, and sampling distributions.

The population distribution for our BMI example is shown in the upper left here. This describes the distribution of BMI values in the entire population of US adults. Typically, this distribution is unknown, and information about that distribution (e.g., population parameters such as the mean, mu (?)) are often what we want to try to estimate. 

However, what we most likely have access to is a single sample of data from the population. If we plot the data from our one sample, we create a sample distribution, which is a distribution of a characteristic in the sample. The sample distribution of BMI values for our one sample of size n=100 from US adults is shown in the upper right here. We can use the information from the known and observed sample to help us estimate the unknown population value/parameter of interest. 

Unlike population and sample distributions, which are distributions of cases or observations, sampling distributions are distributions of statistics, where each ?dot? (a.k.a., sample statistic) is aggregate information from a sample of observations and many, many, many ?dots? (a.k.a., sample statistics) are obtained so we can understand the behavior of the ?dots?. The approximate sampling distribution for mean BMI values for samples of size n=100 is shown in the lower center here. While we can simulate what the sampling distribution will look like (as we have in this lecture), we do not observe this distribution directly in real life. This distribution is an abstraction of what would happen if we could repeat the sampling behavior over and over again. It helps us understand the sampling variability of the statistic so that we can make an inference about the population from the sample. 

While sample distributions are an important part of the data analysis process, sampling distributions are the foundation for statistical inference.We have now explored the behavior of the sampling distributions for sample proportions and for sample means. Both are approximately Normal under certain conditions, as given by the Central Limit Theorem. One key difference is that when we are considering proportions (e.g. the proportion of young adults with diabetes), there are only two parameters needed to describe the behavior of the sampling distribution: the population proportion, p, and the sample size, n.  In contrast, when we are considering means (e.g., the mean BMI in US adults), there are three parameters needed to describe the behavior of the sampling distribution: the population mean, mu, the population standard deviation, sigma, and the sample size, n.

<!--chapter:end:05-sampling_distributions_mean.Rmd-->

# Final Words

We have finished a nice book.

<!--chapter:end:05-summary.Rmd-->

This lecture will continue exploring the concept of confidence intervals and will focus on confidence intervals for a mean.
Every winter, warnings are issued about the increased risk of cardiac events while shoveling snow. Examples include a Popular Science article titled ?Don?t let snow shoveling give you a heart attack? (from Jan. 4, 2018) or a Weather Network article titled ?Winter safety: How shoveling snow can lead to a heart attack? (from Dec. 28, 2017). 

A study was designed to compare cardiac demands among people who shoveled snow manually compared to those who used a electric snow blower1. Ten apparently healthy untrained men cleared tracks of snow using both methods and their maximum heart rates (in beats per minute, or BPM) were recorded during both activities. Let?s just focus on the situation when the men shoveled snow manually. 

For this study of 10 men, the mean maximum heart rate when they shoveled the snow manually was 175 beats per minute (BPM), with a standard deviation of 15 BPM. 

We know that this sample mean value of 175 BPM is an estimate of the population mean. More specifically, the sample mean maximum heart rate value, x-bar, is a point estimate of the population mean maximum heart rate, ? (mu). It is our single best guess of what the true unknown population value may be, based on the data we have. But we know that sample mean values vary from one sample to another, due to sampling variability. How confident are we that we have indeed estimated accurately the heart rates for healthy men when they are dealing with snow?  

References: 
Image: https://pixabay.com/en/shoveling-man-snow-work-male-tool-17328/
1Study: Franklin et al. (1995). Cardiac demands of heavy snow shoveling. JAMA, 273(11).As we did in a previous lecture, we can use our sample data to calculate a confidence interval for the true population mean. This confidence interval is an interval estimate of the true mean because it consists of a range or interval of values instead of a single number. The center of the interval marks the point estimate and the width of the interval reflects the precision (or ?exactness?) of our estimate. 

For our example, the sample mean is 175 BPM, and a 95% confidence interval for the true population mean is 164.3 to 185.7 BPM. 

A confidence interval for a mean is an estimate of the population mean, ? (mu), which reflects the precision of the estimate. We can also say that a confidence interval for a mean is an interval around the sample mean, x-bar, which will include the true population mean a specified percent of time.

So how do we calculate this confidence interval from the data?
The general formula for a confidence interval is the point estimate plus or minus the margin of error, where the point estimate is the sample statistic and the margin of error is a value for degree of confidence times the standard error of the sample statistic. 

Using what we know about sampling distributions of means and the general formula for confidence intervals, we can construct the confidence interval for a mean. 

Recall that if the sample size, n, is large enough, then the Central Limit Theorem applies. This means that the sampling distribution of sample means will be approximately Normal with a mean equal to the true population mean and a standard error (SE) equal to the population standard deviation (sigma) divided by the square root of n. This tells us that for 95% of the sample means (the ones in the middle of the sampling distribution) the true population mean is going to be within ~1.96 standard errors of that sample mean, and for the other 5% of the sample means (the ones in the tails), the true population mean is going to be further than ~1.96 standard errors from that sample mean. (This is another example of the ?flip? in thinking we first discussed in a previous lecture.) So if we put an interval of +/- ~1.96 standard errors around?any?observed sample mean, then 95% of the time, this interval will contain the true population mean.

The equation for a 95% confidence interval for a population mean is thus the sample mean plus or minus 1.96 standard errors. 

We can construct a confidence interval for a mean for any arbitrary confidence level, y%, (such as 90%, or 99%), just as we did for proportions, using the appropriate z-value. The appropriate z-value to use is the value on the Standard Normal curve such that the area of 1-(alpha/2) lies below that value. For 95% confidence, alpha would be 0.05, and we would look for z-value such that 0.975 of the total area (which is 1 minus 0.05 divided by 2) lies below that value. 

Putting it all together, we end up with a confidence interval for a population mean, mu, equal to the sample mean, x-bar, plus or minus the margin of error, where the margin of error equals the z-value times the standard error of the mean. The standard error of the mean equals the population standard deviation, sigma, divided by the square root of the sample size, n. Let?s use the confidence interval equation to calculate an interval estimate for our Heart Rate and Snow Shoveling example. 

A 95% confidence interval for the population mean maximum heart rate for men who shovel snow is 175 plus or minus 1.96 times the population standard deviation, sigma (?), divided by the square root of our sample size, which was n=10. 

Do you see an issue with this calculation? What value is unknown to us? If you said the population standard deviation, sigma, you would be correct. The Central Limit Theorem only applies exactly if we know the standard deviation of the population. In real life, we almost never do. 

But we can estimate the value of the population standard deviation using our data. What value from our data would be our ?best guess? of what the population standard deviation would be? Answer: The sample standard deviation, s, is our ?best guess? or estimate of the population standard deviation, sigma (?). So we can replace the population standard deviation, sigma, in our equation above with the sample standard deviation, s. 

However, this estimation of the standard deviation introduces a bit more uncertainty in our confidence interval estimate, and the uncertainty will be higher when the sample size is smaller.  We need to account for this extra uncertainty when creating our confidence interval. We account for this extra uncertainty in using s, the sample standard deviation, instead of sigma, the population standard deviation, by using a distribution that has extra area in its tails: the t-distribution, instead of the Standard Normal or z-distribution. 

When we have s instead of sigma, the standardized sampling distribution of sample means is no longer a Standard Normal distribution. When we use s, an estimate of sigma, the standardized sampling distribution of sample means now has a t-distribution. Every theoretical distribution is completely defined by an equation with particular characteristics or ?parameters?. For example, the Binomial distribution is characterized by a sample size, n, and a proportion, p; a Normal distribution is characterized by a mean, ?, and a standard deviation, ?). The t-distribution is no different. The t-distribution is characterized by the size of the sample, which is reflected in a parameter called the degrees of freedom (df). When we are making inferences for a single mean, the t-distribution has n-1 degrees of freedom. In the Heart Rate and Snow Shoveling example, the degrees of freedom for the t-distribution is 10 ? 1, which equals 9.

The shape of the t-distribution is similar to that of the Standard Normal or z-distribution (symmetric, unimodal, centered at zero), but when the sample size is small, the t-distribution has more variability (thicker tails). As the sample size increases, the t-distribution narrows and approaches the Standard Normal distribution. This can be seen in the plot above. As the degrees of freedom increase from 3 to 15, we see that the t-distribution curve gets closer and closer to the Standard Normal distribution. This makes sense because we expect a larger sample to tend to give a better estimate of the population standard deviation, sigma, so our extra uncertainty by using s becomes less. Putting it all together, we end up with a confidence interval for a population mean, mu, equal to the sample mean, x-bar, plus or minus the margin of error, where the margin of error equals a constant value from the t-distribution (called a t-value) times the estimated standard error of the mean. The estimated standard error of the mean equals the sample standard deviation, s, divided by the square root of the sample size, n. 

Recall that the margin of error reflects the precision of our estimate of the population mean. Note that we are assuming that our sample is a random sample from the population, so that it is representative of the population and gives us an unbiased estimate of both the standard deviation and the mean. An accurate estimate is one that is both precise and unbiased. 

What factors affect the precision of our estimate? Looking at the equation, we can see that the margin of error will be wider, less precise, if the variability in the population (estimated by the sample standard deviation, s) is higher. The margin of error will be smaller, more precise, as the sample size, n, increases, both because the standard error (SE) decreases and because the t-value decreases. And the margin of error will be wider if a higher degree of confidence (such as 99% confidence vs. 95% confidence) is desired, since the t-values increase with increasing confidence level. (If you want to be more sure of ?catching? the true mean, you must use a wider ?net?.) 
Now let?s finish creating a confidence interval for our Heart Rate and Snow Shoveling example. 

The sample mean maximum heart rate of men who shovel snow is 175 BPM and the sample standard deviation is 15 BPM, so an estimate for the SE of x-bar equals 15/sqrt(10) = 4.743. If we want to be 95% confident, then y = 95 and alpha = 0.05. The t-value is the value in the t-distribution with n-1 = 9 degrees of freedom which gives 0.975 area lying below that value, so the t-value is 2.262. This gives a margin of error of 2.262*4.743 = 10.730. Our 95% confidence interval for the true population mean maximum heart rate for men who shovel snow is 175 plus or minus 10.730, which gives 164.3 to 185.7.

We interpret our confidence interval by saying ?I am y% confident that this interval contains the true population mean.? 

In our example, we could say: 
?I am 95% confident that the interval from 164.3 to 185.7 BPM contains the true mean maximum heart rate for men who shovel snow.?
Or, we could say:
?A plausible range of values for the true mean maximum heart rate for men who shovel snow is from 164.3 to 185.7 BPM.?
Recall that the t-value in the confidence interval equation is determined by the confidence level and the sample size in the study.  So unlike with the z-values presented in a previous lecture, there isn?t one set t-value for a particular confidence level. The table above provides the t-values for the corresponding confidence levels and degrees of freedom. As already mentioned, t-values are larger for larger confidence levels and smaller for smaller confidence levels. As a result, larger confidence levels produce confidence intervals that are wider. You will also notice that as the degrees of freedom increase, the t-values start to approach the z-values presented in a previous lecture (z = 1.645, 1.96, and 2.575 for 90, 95 and 99% confidence levels, respectively). This is also expected since the t-distribution approaches the Standard Normal distribution as the sample size increases.

To find the t-value, use software or a table that displays values of the t-distribution. If using a table and the table does not include the exact number of degrees of freedom for your situation, use the closest value, or, if you want to be more conservative, use the next lower number of degrees of freedom found in the table.

It is important to realize that when one single study is carried out, the confidence interval calculated may miss the true value. This will only happen in roughly 5% of the studies (for a 95% confidence level), but this study might be one of those studies. There is no way of knowing, since we don?t know what the true population mean is. (If we did know it, we wouldn?t need to do the study!)Statistical inference for a mean relies on several assumptions. 

We assume that the sample is a random (or representative) sample from the population of interest. In this way, we are able to generalize the results from this study to the larger population of interest. This assumption could be violated, for example, if we wanted to generalize the maximum heart rates to the population of all adults, both men and women, since our example study only investigated men. The average heart rate for women is higher than for men, so the results from our non-representative (all male) sample would be biased low and would not be generalizable to the population of interest.

We assume that the observations are independent of each other. Selecting one member of the population should not change the chance of selecting any other member. This assumption would be violated, for example, if siblings were included in the sample. Siblings may share genetic factors that affect heart rate and so might be more similar to each other than two people chosen at random would be. This assumption would also be violated if some men?s heart rates were measured twice and both values were included in the data. 

The sampling distribution of sample means should be approximately Normal. How can we check this assumption? We can check to see if the conditions for the Central Limit Theorem (CLT) hold. Recall that if the underlying population distribution is approximately Normal (which we can check to some extent by plotting the sample distribution), then the sampling distribution of sample means is approximately Normal. OR, even if the underlying population distribution is not Normal, if the sample size is ?large enough? (and what is ?large enough? depends on how heavily skewed the population distribution is), then the sampling distribution will still be approximately Normal. If this assumption is not met, use other methods for computing a confidence interval for a mean (such as bootstrapping).

If these assumptions are not met, then the calculated confidence interval cannot be used as an estimate of the population value, because the calculated confidence interval may be too narrow, suggesting a more precise estimate than we actually have, or it might be centered at the wrong place (biased), and hence more likely to ?miss? the true population mean.

<!--chapter:end:06-confidence_intervals_mean.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:06-references.Rmd-->

In this lecture, we will explore a technique known as bootstrapping, which can be used to obtain confidence intervals for parameters (such as medians) for which the statistic does not have a sampling distribution that is Normally distributed. Bootstrapping uses the sample itself as a ?stand-in? for the population, and repeatedly samples from the sample to create an approximate sampling distribution for the statistic of interest. This approximate sampling distribution can then be used to estimate a confidence interval for the population parameter. In earlier lectures, we explored the behavior of the sampling distributions for sample proportions and for sample means. However, any statistic that we calculate from a sample will have a sampling distribution. For example, there is a sampling distribution for sample medians, and a sampling distribution for sample relative risks. The sampling distribution for a statistic simply describes how that particular statistic (such as the median or the relative risk) varies from sample to sample. As a general rule, the sampling distributions of other statistics besides means and proportions are usually NOT Normal and the Central Limit Theorem does NOT apply to them. Some of the other statistics have known parametric sampling distributions (e.g., sample slope), but others have sampling distributions that aren?t known.

But we can still create confidence intervals for the population parameter even if we do not know anything about the sampling distribution for the sample statistic. Let?s explore how to get a confidence interval for other statistics, focusing on the median as the ?other? statistic in this lecture.
Let?s start by thinking about a sample.

We have seen that, if we take a random (representative) sample from a population, the summary statistic from that sample will be a good estimate of the population parameter. If our sample is ?large enough?, the shape and variability of the sample will also be similar to the population. 

Let?s look at an example. 

On the left, we see the population distribution for systolic blood pressure (SBP) in US adults*. On the right, we see a dotplot of the SBP from the Blood Pressure dataset, which contains a sample of 500 people. While the sample distribution isn?t exactly the same as the population distribution, the shape, center, and variability are similar: we see a few people with SBP below 100 mmHg, a peak around 140 mmHg, and then a more gradual decrease until about 200 mmHg, with a few above 200 mmHg.

Suppose that we didn?t have information about the population and we only had information from the 500 individuals in the Blood Pressure dataset. The median SBP in this sample is 140.5 mmHg. This is a point estimate of the population median SBP. If we wanted to better estimate the population median SBP, how can we obtain a confidence interval for this value?

Reference: 
*These SBP measurements were obtained from 5209 people as part of the Framingham Heart Study (http://www.framinghamheartstudy.org). This population may not be exactly like the entire US population in all respects.
Add citation for the data for the sample, if different.
Confidence intervals for a mean or a proportion are based on the assumption that the sampling distribution for the statistic (mean or proportion) is Normally or approximately Normally distributed. For other statistics, such as the median, we cannot make that assumption. If we could only take a very large number of samples repeatedly from the population of interest, calculate the statistic of interest for each sample (e.g., sample median), and obtain the sampling distribution for the statistic, we could understand the sampling variability behavior of the statistics. 

Unfortunately, in most cases we don?t have complete data from the population of interest. (If we did, we wouldn?t need to do statistical inference.) But we do know that a single sample can be a good approximation of the population under certain circumstances: if it is representative of the population, if the observations are independent and measured accurately, and if the sample size is ?large enough?. 

If our sample is a good approximation of the population, we should be able to take a very large number of samples from our sample (a.k.a. our ?approximate? population) to approximate the sampling distribution!

Bradley Efron introduced what is called ?the bootstrap? in 1979, when he formalized the idea of using repeated sampling from the sample itself to approximate the sampling distribution1. The idea behind the bootstrap is simple: we use the sample to approximate the unknown population, and then sample repeatedly from it to obtain an approximate sampling distribution for our statistic. 

Our bootstrap or approximate sampling distribution will probably not have exactly the same center as the true sampling distribution (or the population), since it will be centered at the sample value. It will also probably not have exactly the same variability as the true sampling distribution, but it will be close, provided the original sample is large enough. ?Large enough? for bootstrap confidence intervals means sample sizes larger than about n=50. For smaller samples, the bootstrap distribution may underestimate the true variability, leading to confidence intervals that are too narrow and ?miss? the true value more often than they should.

References:
1 A very accessible (to non-statisticians) description of the bootstrap can be found in Bradley Efron and Robert Tibshirani (1991) ?Statistical Data Analysis in the Computer Age?, Science 253(5018), pp390-395.

A somewhat more statistical description of the bootstrap can be found in Brad Efron and Rob Tibshirani (1986) ?Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy?, Statistical Science 1(1), pp 54-77.



So how exactly is this done? (Note that this is carried out by computer software, not by hand!)

First the computer takes a sample of size n from the original sample (Note: n is the same sample size as the original sample). This new sample is called a ?bootstrap sample?. We sample with replacement, which means that once we have randomly chosen an observation, we put it back in the dataset so that it could be chosen again. This means that in a given bootstrap sample, some observations in the sample will be represented more than once and others may not be present at all. This will result in the bootstrap samples differing slightly from each other, in ways that reflect the variability in the original sample. (If we sampled without replacement, there would be only one way to get a bootstrap sample of size n from an original sample of size n, which wouldn?t give us any information about variability.)

We then calculate the statistic of interest (in our example, the median) for the bootstrap sample.

The computer repeats the process of taking a bootstrap sample and calculating the statistic of interest thousands or tens of thousands of times. (This takes next to no time with modern computing power.) This gives us a "bootstrap distribution? or approximate sampling distribution for the statistic of interest. 

There are a number of ways of obtaining a confidence interval from the bootstrap distribution. A common one is to use the percentiles of the distribution directly. Using this approach, a 95% confidence interval for the median would span the middle 95% of the bootstrapped medians, from the 2.5% value to the 97.5% value. 

Note: Bootstrapping is an example of a nonparametric statistical method. Nonparametric methods rely only on the sample to make inference about the population. They do not require that we know the distribution (and parameters for that distribution) of the population or of the true sampling distribution. 
Let?s look at an example.

As we said a few slides ago, the median SBP level in our Blood Pressure dataset is 140.5 mmHg. This is a point estimate of the median SBP level in the population of all US adults. How can we determine how precise our estimate is? How can we obtain an interval estimate, a confidence interval, for the true population median SBP level?

Using bootstrapping, we sample with replacement repeatedly from our one original sample of size n=500. Three of those bootstrap samples are presented at the top of the slide. As you can see, each of the bootstrap samples have a slightly different distribution and a different median. We are mimicking the process of repeated sampling from the stand-in or ?approximate? population (a.k.a. our sample) to build up the sampling distribution of the median. 

If we obtain 10,000 bootstrap samples of size n= 500 and obtain the median for each sample, we end up with the plot shown at the bottom of the slide. This bootstrap distribution of the sample median has a center (mean) at 140.3 and standard error of 1.35.To find a 95% confidence interval, we have the computer put the 10,000 sample medians in order from smallest to largest, and find the lower and upper values of the middle 95% of the median SBP values. This corresponds to the 2.5th percentile, or 251 values above the minimum value, and the 97.5th percentile, which is 251 values below the maximum value. We obtain a 95% confidence interval for the population median SBP of 138 to 142 mmHg. We interpret this confidence interval the same way as we did for estimating the population proportion and the population mean. 

While the bootstrap technique was designed to obtain confidence intervals when the form of the sampling distribution of the sample statistic is unknown, the bootstrap can be used to find confidence intervals for any parameter, including means and proportions. Let?s see what happens if we calculate a bootstrap confidence interval for the mean SBP in our example. The sample mean from the Blood Pressure example is 144.95 mmHg. 

When we calculate the mean instead of the median for each of our 10,000 bootstrap samples, we obtain the bootstrap distribution for the sample mean shown here. This time, the bootstrap distribution looks quite Normally distributed. This is what we would expect, since we know from the Central Limit Theorem that the sampling distribution for a sample mean is Normal. The bootstrap distribution mirrors the sampling distribution in shape and spread, but is centered on the sample mean instead of the population mean.

If we order the 10,000 sample means and find the 2.5th percentile and the 97.5th percentile, we obtain a bootstrapped 95% confidence interval for the population mean SBP of 142.56 to 147.44 mmHg. 

How does this result compare with a 95% confidence interval calculated the usual way, using the Central Limit Theorem? The sample size is 500, the sample mean is 144.95 mmHg, and the sample standard deviation is 27.99 mmHg. Our degrees of freedom are 499, which results in a t-value of 1.965. Putting these values into the formula, a 95% CI is xbar +/- t*SE = 144.95 +/- (1.965)*(27.99/sqrt(500)) = 144.95 +/- 2.46 . We end up with a 95% confidence interval that ranges from 142.49 to 147.41 mmHg, which is nearly identical to the bootstrap CI above.

We see that when we use the bootstrap method to calculate a confidence interval for the mean, the resulting interval is almost identical to the interval based on the Central Limit Theorem, which is reassuring!

<!--chapter:end:07-bootstrapping.Rmd-->

Comparing proportions is of interest for studies in which the outcome is categorical and there are two or more groups. We will focus on the simplest situation where the outcome is binary (e.g., dead/alive, disease progressed/did not progress) and two groups are being compared (e.g., exposed/not exposed, treatment/control). 

This lecture focuses on comparing proportions from prospective studies: 
cohort studies, in which the two groups of interest are typically those exposed and those not exposed to some risk factor, or 
clinical trials, in which the two groups of interest are participants who received a new treatment and participants who served as controls. 
Have you heard the saying: an aspirin a day keeps heart attacks away? A key study that helped to support this claim was the Physicians Health Study1, which was a double-blind, placebo-controlled, randomized trial that began in the 1980s. 

The researchers in this study recruited male physicians in the US aged 40-84 years old. Of those who were contacted, 22,071 physicians volunteered and were eligible to participate. The men were randomly assigned to one of two groupsdaily low-dose aspirin pill group or daily placebo pill groupand were monitored for cardiovascular endpoints. In particular, the primary outcome of interest for this study was first heart attack. 

After five years, 139 of the 11,037 men in the aspirin group had had a heart attack while 239 of the 11,034 men in the placebo group had had a heart attack. Translating these values into a proportion, 0.013 of the men in the aspirin group and 0.022 in the placebo group had had a heart attack.

Based on this, we can see that the proportion of heart attacks is higher in the placebo group than the aspirin group. How might we obtain a single quantity to compare these two groups? And, could this large of a difference be the result of sampling variability or is this evidence of a difference? Lets use statistical methods to answer these questions.

References: 
1Steering Committee of the Physicians' Health Study Research Group*. "Final report on the aspirin component of the ongoing Physicians' Health Study."New England Journal of Medicine, 321.3 (1989): 129-135.*Note: There was a typo in the transcript. While the transcript has been updated for Spring 2021 (bold and underlined text is the updated information), the audio recording still has the error. This will get corrected once we return back to working in the office.

When we have two binary categorical variables, we can display the counts for each combination of categories in a two by two (2x2) contingency table. The recommended way to display the 2x2 table is to have the two groups that are being compared in the rows and the outcome of interest in the columns, with the event of interest in the first column. The sample size for group 1 and group 2 are denoted as n1 and n2 in the table (the last column in the table) and these would add up to the total sample size, n, for the entire study. We denote the counts in the middle of the table as
a for those in group 1 who had the event of interest; 
b for those in group 1 who did not have the event of interest;
c for those in group 2 who had the event of interest; and, 
d for those in group 2 who did not have the event of interest.
It is very important to set your table up with the rows and columns in the order presented on this slide. If you mix up the rows and columns, then you are mixing up a, b, c, and d and the formulas as presented in this lecture wont be correct. 

One summary measure of interest might be calculating the proportion who had the event in each of the groups. The proportion who had the event in group 1 is a divided by the total in group 1, n1. Similarly for group 2: the proportion who had the event in group 2 is c divided by the total in group 2, n2. Because these proportions are from a sample, we label them p-hat1 and p-hat2, and they are point estimates of the population proportions, p1 and p2. This type of proportion is called a conditional proportion (or a conditional probability), because we are focusing our attention on only one row (or sometimes one column) in the table and finding the proportion with the event for those within that row. In technical terms, we are looking at the proportion of something, conditional on being in a particular group. In notation, we denote this as Pr[U | V], which can be read as the probability that U will occur given V. To make it more concrete, p-hat1 could also be written as Pr[Event | Group 1], or the probability of having the event given you are in group 1. 

There is another term for these proportions: risk. The estimated risk of having an event is another name for the proportion with the event in a prospective study. The risk terminology can only be used in prospective studies (cohort studies or clinical trials) in which participants are followed over time, since it expresses the number who experienced the outcome as a proportion of all those who were at risk at the start of the study. 

In epidemiologic usage, a person is at risk of an outcome if 1) they are capable of having it, and 2) they havent had it yet. A person is at risk of lung cancer if they have a lung and dont have lung cancer already. A person is at risk of death if they are alive. Note that this usage of the phrase at risk differs substantially from the colloquial usage you are likely to see in newspapers or public policy reports. Once we have the risk estimates for each group, how might we create a single quantity to compare the risks in the two groups? Popular comparative measures are differences (subtraction) and ratios (division). The former is used to make absolute comparisons (e.g., 10% higher) and the latter is used to make relative comparisons (e.g., 2 times more). 

When comparing proportions (or risks) using subtraction, the difference between two proportions (or risks) is called a risk difference (RD). It is also sometimes called the attributable risk. The risk difference is defined as the value of the risk of the event in group 1 (p-hat1) minus the risk of the event in the group 2 (p-hat2). In most cases, group 1 is the group of interest and group 2 is the reference or control group. 

If the group of interest has the higher risk, then the risk difference will be a positive value, whereas if the group of interest has the lower risk, then the risk difference will be a negative value.If we compared proportions (or risks) using division, the ratio between two proportions (or risks) is called a relative risk (RR). The relative risk is defined as the risk of the event in group 1 (p-hat1) divided by the risk of the event in group 2 (p-hat2). Similar to risk difference, group 1 is the group of interest and group 2 is the reference or control group. 

If the group of interest has the higher risk, then the relative risk will be a value greater than 1, whereas if the group of interest has the lower risk, then the relative risk will be a value less than 1.

An alternative way of interpreting this ratio is to translate this value into a percent. If the relative risk is less than 1, we translate this into a % decrease (since the risk is less in the group of interest than in the reference group). We do this by taking (1-RR)*100. If the relative risk is greater than 1, we translate this into a % increase (since the risk is more in the group of interest than in the reference group). We do this by taking (RR-1)*100. The data from the Physicians Health Study are shown here again. The risks of heart attack for both groups were presented in an earlier slide, but now the complete details of the calculations are shown here. The risk of having a heart attack for those in the aspirin group was 139 divided by 11,037, which equals 0.013. The risk of having a heart attack for those in the placebo group was 239 divided by 11,034, which equals 0.022. 

The risk difference for this example is 0.013 minus 0.022, which is -0.009. This means that the absolute risk of having a heart attack is 0.009 lower in the aspirin group as compared to the placebo group. 

The relative risk is 0.013 divided by 0.022, which is 0.581. This means those who took aspirin had 0.581 times the risk of having a heart attack compared to those who took the placebo. 

An alternative interpretation of the RR is that, since the RR was less than 1 in the Physicians Health Study, we would calculate the percent reduction in risk as (1-0.581)*100 = 41.9%. This means that those who took aspirin had a 41.9% reduction in risk of heart attack compared to those who took placebo.

We know that if we repeated this study, the estimated risk values in each group would be slightly different, due to sampling variability. Lets use statistical inference (specifically, interval estimation) on these summary measures to understand if what we observed is due to sampling variability or if there is evidence of a real difference. The other inferential method, hypothesis testing, for comparing proportions will be presented in a future lecture.Recall that the general formula for a confidence interval is the point estimate plus or minus the margin of error. Filling in these details for estimating a difference of two population proportions (or risks):

--The point estimate is the difference between the two sample proportions (or risks), that is, the risk difference. 

--The confidence interval is the point estimate plus or minus the degree of confidence, which is the appropriate z-value, times the estimated standard error for the difference in the sample proportions (or risks). 

--The estimated standard error for this situation looks similar to the SE value described in a previous lecture on confidence intervals for a proportion but now applies to a two-group situation: the square root of the sample proportion in group 1 (p-hat1) times 1 minus the sample proportion in group 1, all over the sample size for group 1 (n1), plus, the sample proportion in group 2 (p-hat2) times 1 minus the sample proportion in group 2, all over the sample size for group 2 (n2). 

The confidence interval formula presented on this slide only applies when all of the assumptions are met for comparing two proportions. (These assumptions will be presented in a upcoming slide.) Remember to check the assumptions first before carrying out any inferential method.  Recall that the values in a confidence interval provide plausible values for the true population parameter, but we can also use it to make conclusions about statistical significance. How might we do this for risk difference? When using comparison measures, its good to ask yourself, If the two groups were equal (the null hypothesis were true), what value would result using this comparison measure (the null value)?. In the case of a risk difference, if the two groups were equal and we took the difference between them, then the (null) value would be 0. 

Using similar logic to what was presented in a previous lecture, if the null value does not fall within the confidence interval limits, then this value is not plausible and therefore, we would have evidence that the true population value is different than the null value. That is, the results are statistically significant and there is evidence of a difference in proportions between the two groups. This situation is shown in the first plot above: the confidence interval does not include the null value of (in this case) zero. 

Conversely, if the null value falls within the confidence interval limits, then it is a plausible value (one of many different plausible values), and we would not have enough evidence to say the true population value is different than the null value. That is, the results are not statistically significant and there is not enough evidence of a difference in proportions between the two groups. This situation is shown in the second plot above: the confidence interval DOES include the null value of zero. Now lets calculate the confidence interval for the risk difference for the Physicians Health Study example. 

The sample risk difference of having a heart attack between the aspirin and control groups is -0.009 and the estimated standard error for the difference is 0.002. The z-value for a 95% confidence interval is the value in the Standard Normal distribution with 0.975 area lying below that value, so the z-value is 1.96. Putting all of those values togetherthe point estimate, the z-value, and the standard errora 95% confidence interval for the difference in the population risk of having a heart attack between the aspirin and control groups is between -0.011 and -0.007.

We are 95% confident that the difference in risk of heart attack between the aspirin and control groups is between -0.011 and -0.007. Or we could say, a range of plausible values for the true difference in risk of heart attack between the aspirin and control groups is from -0.011 to -0.007. Because the 95% confidence interval does not include 0, then zero is not a plausible value and we can conclude that the difference in risk of heart attack between the aspirin and control groups is statistically significant. There is evidence of a difference in risk of heart attack between the two groups, with aspirin having between 0.007 to 0.011 (or 0.7% to 1.1%) lower risk than the placebo group. Unlike the usual calculation for a confidence interval as presented for risk difference, computing a confidence interval for relative risk is a two-step procedure. 

We see that the first step looks very similar to the general formula for a confidence interval: point estimate plus or minus the margin of error. But it is for the natural log of the relative risk. This is because the sampling distribution for relative risk does not follow a normal distribution, no matter what the sample size is. But, if we take the natural log of the sample relative risk, then the sampling distribution for this transformed measure is approximately normal. Applying this transformation allows us to use normal-based methods for calculating the confidence interval. 

So, Step 1 is to calculate the CI for the natural log of the relative risk, where: 

--The point estimate is the natural log of the sample relative risk. 

--The degree of confidence is the appropriate z-value, and 

--The estimated standard error of the natural log of relative risk is as shown on the slide. 

Then, Step 2 is to back transform the confidence interval values found for the natural log of RR by taking the antilog: the exponential function. This will produce the lower and upper limits of the confidence interval for relative risk (on the original scale). 

The confidence interval formula presented on this slide only applies when all of the assumptions are met for comparing two proportions. (These assumptions will be presented in a upcoming slide.) Remember to check the assumptions first before carrying out any inferential method.  Similar to the approach used for risk difference, lets figure out how we can make conclusions about statistical significance using relative risk. Lets ask the same question again: If the two groups were equal (the null hypothesis were true), what value would result using this comparison measure (the null value)?. If the two groups were equal and we took the ratio between them, then the (null) value would be 1. 

So if the null value does not fall within the confidence interval limits, then this value is not plausible and therefore, we would have evidence that the true population value is different than the null value. This situation is shown in the first plot above: the confidence interval does not include the null value of (in this case) one. 

Conversely, if the null value falls within the confidence interval limits, then it is a plausible value (one of many different plausible values), and we would not have enough evidence to say the true population value is different than the null value. This situation is shown in the second plot above: the confidence interval DOES include the null value of one. 
Now lets calculate the confidence interval for the relative risk for the Physicians Health Study example. 

The sample relative risk of having a heart attack between the aspirin and control groups is 0.581. Taking the natural log of this value gives us -0.542, and the estimated standard error for the natural log of RR is 0.106. The z-value for a 95% confidence interval is the value in the Standard Normal distribution with 0.975 area lying below that value, so the z-value is 1.96. Putting all of those values togetherthe point estimate, the z-value, and the standard errora 95% confidence interval for the ln(RR) is between -0.750 and -0.335. Now we need to exponentiate those limits to get the 95% CI for the relative risk: e^(-0.750) equals 0.473 and e^(-0.335) = 0.715. 

We are 95% confident that the relative risk of heart attack between the aspirin and control groups is between 0.473 and 0.715. Or we could say, a range of plausible values for the true relative risk of heart attack between the aspirin and control groups is from 0.473 to 0.715. Because the 95% confidence interval does not include 1, then one is not a plausible value and we can conclude that the relative risk of heart attack between the aspirin and control groups is statistically significant. There is evidence of a difference in risk of heart attack between the two groups, with aspirin having between 0.473 to 0.715 times the risk of the placebo group (or 28.5% to 52.7% reduction in risk).Statistical inference (e.g., interval estimation) for comparing risks relies on several assumptions. 

The samples should be random (or representative) samples from the respective populations, to allow us to generalize the results to those populations; 

The observations within each group should be independent of one another and the observations in one group should be independent of the observations in the other groups. 

We assume that the sample is large enough for the sampling distribution of risk difference or the sampling distribution for the natural log of the relative risk to be approximately Normal. For these statistics, large enough requires the counts in the middle of the 2x2 table (a, b, c, and d) to be at least 5. In our example, this assumption would be met if our sample contained at least 5 people in each of the cells in the table. If the sample size is too small, consider using other methods, such as the correction methods, to compute confidence intervals.

If these assumptions are violated, the confidence intervals we calculate may give us faulty information about the true population proportion. Both the risk difference and the relative risk summarize the 2x2 table data using a single number. However, neither the RD nor the RR gives a complete picture of the impact of an exposure or proposed treatment. 

Reporting only the relative risk can be misleading. Reducing the risk of disease by 50% sounds impressive, but if the disease is rare, then such a reduction may have little practical impact. To see why, consider the following situation. Suppose we have developed a vaccine that halves the risk of a particular disease, so the risk of the disease in the vaccinated is half that in the unvaccinated (RR = 0.50). What impact will our new vaccine have? (Refer to the left side of the slide above.)
In Case 1, the disease is quite rare; the risk of disease in the unvaccinated is only 2 in 10 million. Our new vaccine cuts the risk in half, so the risk of disease in the vaccinated is 1 in 10 million. The risk difference, therefore, is 1 in 10 million. 
In Case 2, the disease is more common; the risk of disease in the unvaccinated is 20%. Our new vaccine cuts the risk in half, so the risk of disease in the vaccinated is 10%. The risk difference, therefore is 10%. 
Our new vaccine would have much more impact in Case 2, where the prevalence of the disease is high, than in Case 1, where the disease is rare, even though the relative risk is the same. 

On the other hand, reporting only the risk difference can also be misleading. Lets consider an environmental contaminant, where the exposed people have 1% higher risk than the unexposed people. (Refer to the right side of the slide above.)
In Case 1, the risk of the disease in unexposed people is fairly low at 1%, so the contaminant raises the risk to 2%. The contaminant doubles the number of people who can be expected to acquire the disease. The relative risk, therefore, is 2.0.
In Case 2, the risk of the disease in unexposed people is already quite high at 89% (think common cold), so the contaminant raises the risk to 90%. The contaminant increases by only 1% the number of people who can be expected to acquire the disease, so the relative risk is 1.01. 
The contaminant in Case 1 is likely to raise more concern, since it doubles the risk, even though the absolute number of people harmed is the same in both cases. 

It is important to keep the difference between the risk difference and the relative risk in mind when reading the news and medical literature. Researchers have a tendency to report their results in whichever way makes them sound more impressive, for example, reporting benefits with BIG numbers (e.g., relative risk) and harms with small numbers (e.g., risk difference). This is misleading. A famous researcher who studies how people understand risk, Gerd Gigerenzer1, suggests that researchers report absolute measures over relative measures because its easier to understand them. The key point is that the impact of an exposure or treatment depends on how common the condition or disease is in the first place.

References: 
1Helping doctors and patients make sense of health statistics: towards an evidence-based society, a keynote presentation at the 8th International Conference on Teaching Statistics (ICOTS) in Ljubljana, Slovenia, July 11-16, 2010

<!--chapter:end:10a-RelativeRisk.Rmd-->

This lecture focuses on comparing proportions from case-control studies, in which the two groups of interest are cases (people who have the disease of interest) and controls (people who do not have the disease). Recall that case-control studies are carried out after the fact (or retrospectively). The researchers purposefully select cases who have already experienced the outcome of interest and then purposefully select controls who have not experienced the outcome. The number of controls is chosen in advance to be an integer multiple of the number of cases, and controls are typically chosen to be similar to the cases in age, gender, and other relevant characteristics. The researchers then look retrospectively back in time to see how many of the cases and how many of the controls were exposed to the risk factor in the past. In contrast, prospective studies select participants who do not have the disease and follow them forward in time to see if the risk of experiencing the outcome depends on group or exposure status. However, prospective studies can be time-consuming and expensive to do, especially if it takes years for the outcome or event of interest to occur, or if the outcome is very rare. Case-control studies are useful for exploring possible risk factors for a disease or condition, and are often the preferred design for studying rare diseases or outcomes. 

Ideally, we want to compare two proportions using the risk difference or relative risk (as we can in prospective studies), but when we have a case-control study, these measures are meaningless due to the case-control sampling strategy (that is, due to the fact that the number of controls is chosen in advance to be an integer multiple of the number of cases). Instead, we use use another measure of comparison called the odds ratio. Lets explore an example.

A case-control study1 was carried out to investigate the relationship between antidepressant use during pregnancy and Autism Spectrum Disorder (ASD) in the children. Two-hundred and ninety-eight case children with ASD (and their mothers) and 1,507 randomly selected control children (who dont have ASD, and their mothers) were drawn from a large health care organization in northern California. The researchers chose to have a ratio of about 5 control children per 1 case child. The control children were matched to case children by sex, birth year, and hospital of birth. The researchers retrospectively collected information about maternal use of antidepressants during the year prior to delivery (the exposure of interest). 

Of the children with ASD, 20 of the mothers had used antidepressant drugs during the year prior to delivery (which is 20 out of 298, or 6.7% of the cases). Of the control children, 50 of the mothers had used the drugs (which is 50 out of 1,507, or 3.3% of the controls). 

Based on this, we can see that the proportion of maternal antidepressant use during pregnancy is higher in the cases than in the controls. How might we obtain a single quantity to compare these two groups? And, could this large of a difference be the result of sampling variability or is this evidence of a difference? Lets use statistical methods to answer these questions.

References: 
1Croen, L. A., Grether, J. K., Yoshida, C. K., Odouli, R., & Hendrick, V. (2011). Antidepressant use during pregnancy and childhood autism spectrum disorders.Archives of general psychiatry,68(11), 1104-1112.Before we move on to the statistical methods, lets discuss the arrangement of the table for case-control studies. 

This table looks similar to what you have seen in a previous lecture, but notice that this table does not have a fourth column with totals, as the table for a prospective study would. This is done intentionally to emphasize the fact that case-control studies must be analyzed differently than prospective studies. It is NOT appropriate to say that of the 70 (=20 + 50) children whose mothers used antidepressants during pregnancy, 20 of them (2.8%) developed ASD, for two reasons. First, the study did NOT begin with people who did not have the outcome (ASD) and then follow them over time to see if they experienced the outcome; it started with children who already had ASD. The risk of a healthy child developing ASD therefore cannot be determined from this study. Second, the number of ASD cases relative to the number of children in the study is fixed in advance by the chosen ratio of controls to cases. Risks, risk differences, and relative risks should never be calculated from a case-control study.So, how do we compare two categorical variables when we have a case-control study? Answer: the odds ratio! 

But first, what are odds? In general terms, odds are the probability of having the event (p) divided by the probability of not having the event (1-p). Probabilities always range between 0 and 1 so odds range between 0 and infinity. For example, if a race horse runs 100 races and wins 10 times and loses the other 90 times, the probability of winning is 10/100 = 0.10 or 10%, but the odds of winning are (10/100)/(90/100) = 10/90 = 0.111 or 11%, or 1 win to 9 loses (notice that the total number of races canceled out between the two probabilities and we were left with count for event divided by count for non-event). Or, as another example, if a horse runs 100 races and wins 60 of them, the probability of winning is 60/100 = 60%, but the odds of  winning are 60/40 = 1.5. Notice that when the probability is low, the odds and the probability are more similar. 

Recall that the recommended way to display the 2x2 table is to have the two groups that are being compared in the rows and the outcome of interest in the columns, with the event of interest in the first column. This is mentioned again because it is very important to have rows and columns in the order presented on this slide. If you mix up the rows and columns, then you are mixing up a, b, c, and d and the formulas as presented in this lecture wont be correct. 

To calculate odds, we find the odds of being exposed in group 1 (cases) and the odds of being exposed in group 2 (controls). If the proportion who were exposed in group 1 is a divided by a+c (which is the total number of cases), then the odds of being exposed in group 1 is (a/(a + c)) divided by (c/(a+c)), which turns out to be a/c (because the denominators of a+c cancel out). Similarly, the odds of being exposed in group 2 is b/d. To compare the odds between two different groups using division, we use the odds ratio (abbreviated OR). The odds ratio is defined as the odds of being exposed in group 1 (odds1) divided by the odds of being exposed in group 2 (odds2). As previously stated, group 1 is the cases and group 2 is the reference or control group. 

If the group of interest has higher odds than the reference group, then the odds ratio will have a value greater than 1, whereas if the group of interest has lower odds, then the odds ratio will have a value less than 1. 

An alternative way of interpreting this ratio is to translate this value into a percent. If the odds ratio is less than 1, we translate this into a % decrease (since the odds is less in the group of interest than in the reference group). We do this by taking (1-OR)*100. If the odds ratio is greater than 1, we translate this into a % increase (since the odds is more in the group of interest than in the reference group). We do this by taking (OR-1)*100. 

Note that odds ratio may be used in prospective and cross-sectional studies, but it must be used in case-control studies.The data from the Autism and Maternal Antidepressant Use study is shown here again. Of the children with ASD, the odds of the mothers having used antidepressant drugs during the year prior to delivery is 20 divided by 278, which equals 0.072. Of the control children, the odds of the mothers having used the drugs is 50 divided by 1457, which equals 0.034. 

The odds ratio, then, is 0.072 divided by 0.034, which is 2.096. This means that children with ASD had roughly 2 times higher odds of the mothers having used antidepressant drugs during the year prior to delivery compared to control children. 

Alternatively, since the OR was greater than 1, we could calculate the percent increase in odds as (2.096-1)*100 = 109.6%. The odds of the mothers having used antidepressant drugs are increased by 110% in children with ASD compared to controls. Thus far in this lecture, we have been talking about the exposure odds ratio, which is the odds of being exposed, comparing cases to controls. Did this feel odd to anyone (no pun intended)? The exposure happened first and then the outcome (disease (in general terms) or ASD in our example). It would be great if we could have a summary measure that we interpret as we intuitively want to, that is, the odds of having the outcome (disease) comparing the exposed to the unexposed (called the disease odds ratio). We are in luck! Odds ratios have several useful properties that are almost magical. 

Recall that the exposure odds ratio is a over c, all divided by b over d. Doing a little rearranging gets us to a times d, all divided by b times c. Lets see how to calculate the disease odds ratio.

If we calculate the odds of having the disease for the exposed, we would focus on only the exposed row in the table: those that have the disease (a) divided by those that dont have the disease (b). Similarly, if we calculate the odds of having the disease for the unexposed, we would take those that have the disease (c) divided by those that dont have the disease (d). Putting those two ratios together to get the disease odds ratio results in a over b, all divided by c divided by d. Doing a little rearranging, we get a times d, all divided by b times c. This is exactly the same number as the exposure odds ratio. (Magic!)

As previously noted, it isnt appropriate to calculate the probability of disease in the exposed in a case-control study. The same logic would apply to calculating the odds of disease as we just did for the disease odds ratio. However, since the disease odds ratio is always exactly the same number as the exposure odds ratio, either of them can be used as measures of the strength of association between exposure and disease. You are likely to see both of them in the published literature. Now lets use statistical inference (specifically, interval estimation) to understand if what we observed is due to sampling variability or if there is evidence of a real difference. The other inferential method, hypothesis testing, for comparing two categorical variables will be presented in a future lecture.

The calculation of a confidence interval for an odds ratio is similar to the calculation of a confidence interval for a relative risk, in that its a two-step procedure. 

We see that the first step looks very similar to the general formula for a confidence interval: point estimate plus or minus the margin of error. But it is for the natural log of the odds ratio. This is because the sampling distribution for odds ratio does not follow a Normal distribution, no matter what the sample size is. But, if we take the natural log of the sample odds ratio, then the sampling distribution for this transformed measure is approximately Normal. Applying this transformation allows us to use normal-based methods for calculating the confidence interval. 

So, Step 1 is to calculate the CI for the natural log of odds ratio, where the 

--The point estimate is the natural log of the sample odds ratio. 

--The degree of confidence is the appropriate z-value, and 

--The estimated standard error of the natural log of odds ratio is as shown on the slide. 

Then, Step 2 is to back transform the confidence interval values found for the natural log of OR by taking the antilog: the exponential function. This will produce the lower and upper limits of the confidence interval for odds ratio on the original scale. 

The confidence interval formula presented on this slide only applies when all of the assumptions are met for comparing two odds. (These assumptions will be presented in a upcoming slide.) Remember to check the assumptions first before carrying out any inferential method. Lets figure out how we can make conclusions about statistical significance using odds ratio. Lets ask the same question again that we saw in a previous lecture: If the odds in the two groups were equal (the null hypothesis), what value would result using this comparison measure (the null value)?. Because the odds ratio is a ratio measure of comparability, similar to the relative risk, if the odds in the two groups were equal and we took the ratio between them, then the (null) value would be 1. 

So if the null value does not fall within the confidence interval limits, then this value is not plausible and therefore, we would have evidence that the true population value is different than the null value. This situation is shown in the first plot above: the confidence interval does not include the null value of (in this case) one. 

Conversely, if the null value falls within the confidence interval limits, then it is a plausible value (one of many different plausible values), and we would not have enough evidence to say the true population value is different than the null value. This situation is shown in the second plot above: the confidence interval DOES include the null value of one. Now lets calculate the confidence interval for the odds ratio for the Autism and Maternal Antidepressant Use example. 

The sample odds ratio of having ASD between maternal antidepressant use during pregnancy and no use is 2.096. Taking the natural log of this value gives us 0.740, and the estimated standard error for the natural log of OR is 0.273. The z-value for a 95% confidence interval is the value in the Standard Normal distribution with 0.975 area lying below that value, so the z-value is 1.96. Putting all of those values togetherthe point estimate, the z-value, and the standard errora 95% confidence interval for the ln(OR) is between 0.206 and 1.274. Now we need to exponentiate those limits to get the 95% CI for the odds ratio: e^(0.206) equals 1.229 and e^(1.274) equals 3.577. 

We are 95% confident that the odds ratio of having ASD between maternal antidepressant use during pregnancy and no use is between 1.229 and 3.577. Another way of stating this is that the odds of having ASD for children whose mothers used antidepressants during pregnancy is between 1.229 and 3.577 times higher than for children whose mothers did not. Or we could say, a range of plausible values for the true odds ratio for ASD between maternal antidepressant use and no use is from 1.229 to 3.577. Because the 95% confidence interval does not include 1, it is not a plausible value and we can conclude that the odds ratio comparing the two groups is statistically significant. There is evidence of a difference in odds of ASD between the two groups, with those who were exposed to antidepressants in-utero having between 1.229 to 3.577 times the odds of the those who were not exposed (or 22.9% to 257.7% increase in odds). Statistical inference (e.g., interval estimation) for comparing odds relies on several assumptions. 

The samples should be random (or representative) samples from the respective populations, to allow us to generalize the results to those populations; 

The observations within each group should be independent of one another and the observations in one group should be independent of the observations in the other groups. 

We assume that the sample is large enough for the sampling distribution for the natural log of the odds ratio to be approximately Normal. For this statistic: 
in a prospective cohort or cross-sectional study, large enough  requires both n1*(p-hat1)(1-phat1) and n2*(p-hat2)(1-phat2) to be at least 5, where n1 is the number of exposed individuals, p-hat1 is the sample proportion with the outcome among the exposed individuals, n2 is the number of unexposed individuals, and p-hat2 is the sample proportion with the outcome among the unexposed individuals. 
in a case-control study, large enough requires both m1*(p-hat1)(1-phat1) and m2*(p-hat2)(1-phat2) to be at least 5, where m1 is the number of cases, p-hat1 is the sample proportion of cases that are exposed, m2 is the number of controls, and p-hat2 is the sample proportion of controls that are exposed. 
If the sample size is too small, consider using other methods, such as the correction methods, to compute confidence intervals.

If these assumptions are violated, the confidence intervals we calculate may give us faulty information about the true population odds ratio. While the odds ratio is distinct from the relative risk, there are times when it is a reasonable estimate of the relative risk. This is another magical property of the odds ratio. 

If the disease being studied is relatively rare, then the sum of a and b (the total exposed) would be approximately equal to b because cell a would be a very small number. And a similar argument for the not exposed row: the sum of c plus d would be approximately equal to d. Using these approximations in the relative risk calculations results in the odds ratio formula. So, if the disease is relatively rare, the odds ratio from a case-control study is a good approximation of the relative risk that would have been obtained from a prospective study. What would we consider relatively rare? Answer: A disease with a prevalence below 10%, but the lower the prevalence, the closer the odds ratio will be to the relative risk. When the prevalence is NOT low, however, the odds ratio always overestimates the strength of the association between exposure and disease. 

The fact that you can get a good approximation to the relative risk from the odds ratio is the reason that odds ratios (rather than some other measure) are used to analyze case-control studies. Relative risks are easier to interpret and easier to explain to non-statisticians (or non-gamblers!) than odds ratios, and they directly express what is usually of interest  i.e. how the risk of getting the disease is affected by the treatment or exposure. 

<!--chapter:end:10b-OddsRatio.Rmd-->

Thus far in this course, we have discussed statistical methods for comparing means of a numerical variable between two or more groups (using t-tests, confidence intervals, or ANOVA) and for comparing proportions of a binary variable between two groups (using confidence intervals for relative risks or odds ratios). This week, we will discuss hypothesis tests that can be used for comparing proportions of a binary variable between two groups, but which can also be used more generally for comparing any two categorical variables (binary or not) to determine whether or not they are associated. This lecture will focus on a common/popular test for comparing two categorical variables, Pearsons Chi-square test. Lets start with an example. A randomized controlled trial was carried out to determine whether a handheld transcranial magnetic stimulation (TMS) device was more effective at treating migraine headaches than a sham control device that did not deliver any stimulation1. Two-hundred adults who suffered from migraine headaches were randomly assigned to use the TMS device or the sham device when they felt a migraine coming on. Two hours later, the participants were asked to record their pain status. 

In this study, there are two binary categorical variables: treatment group (TMS or control) and pain status after two hours (pain-free or not). Similar to a previous lecture, the data can be summarized using a two by two (2x2) table or contingency table, as shown above. 

In this study, 39 out of the 100 participants in the TMS group were pain-free after two hours (or 39% of the TMS group were pain-free). (Recall that this type of calculation is called conditional proportion or conditional probability. You may sometimes hear statisticians use language like Conditional on treatment group, the probability of being pain-free after two hours is 39%.) In contrast, 22 out of the 100 in the control group were pain free after two hours (or 22% of the control group were pain free) 

We can see that in this particular study, the chance of being pain-free after two hours is higher in the TMS group (39%) than in the control group (22%).

But, as usual, what we want to know is: Could this large of a difference in pain relief between the two study groups be the result of sampling variability or is this evidence of a real difference? Lets use statistical inference methods to answer this question.

Reference: 
1Lipton, R.B., et al (2010). Single-pulse transcranial magnetic stimulation for acute treatment of Migraine with aura: A randomized, double-blind, parallel-group, sham-controlled trial, The Lancet Neurology, 9(4), 373-380.
Dataset: Cannon, A.R., Cobb, G.W., Hartlaub, B.A., Legler, J.M., Lock, R.H., Moore, T.L., Rossman, A.J., & Witmer, J.A. (2013) Stat2: Building models for a world of data. New York, NY: W. H. Freeman and Company. (p. 464)
For 2x2 tables, such as those seen thus far, there are a number of analysis approaches that could be used. 

We could frame our research question in terms of relative risk and use the methods for relative risk that were presented in a previous lecture. For example, is the risk of being pain-free different between the TMS group and control group? Or we could formulate the question in terms of odds ratios and use the methods for odds ratios that were presented in a previous lecture (although using odds ratios for anything other than a case-control study doesnt really make sense). 

In this lecture, we will frame the question as one of association or relationship between variables. For example, is there an association between study group and pain status OR are the two variables independent of one another? If there were no association between the variables (i.e., if they were independent), then the probability of being pain-free would not depend on study group. In contrast, if there were an association between the variables, then the probability of being pain-free would depend on study group. 

The advantage of framing the question this way is that it is equally applicable to 2x2 tables, or to 2x3 tables, or to tables of any arbitrary size. 

The limitation of framing the question this way is that it only addresses whether the two variables are associated with each other. It does not address how they are associated, that is, it does not provide an effect direction or size (such as a relative risk or risk difference).

We will discuss two tests that can be used to address the question of whether two categorical variables are associated with each other: Pearsons Chi-square test (this lecture) and Fishers exact test (future lecture). As with any study, we begin with the research question. 

The research question for the TMS and Migraine Pain study is: Is study group (TMS vs. control) associated with migraine headache pain status (pain-free or not) after two hours? Before we can carry out any inferential method, we first need to evaluate the assumptions of the method. The assumptions for Pearsons Chi-square test are: 

The sample should be a random (or representative) sample from the population, to allow us to generalize the results to that population; 

The observations should be independent of one another. This assumption would be violated if the participants in the study were related in some way to each other (e.g. siblings) or if the observations in one group were related in some way to those in another group, for example, if the participants in the groups were matched or paired. If the observations are paired, then McNemars test should be used instead.

In previous lectures, we checked to make sure the sample was large enough for the sampling distribution to be approximately Normal. A similar approach is employed to safely utilize the Chi-square test. We have to check to make sure the sample is large enough by making sure the expected number of observations in each cell of the table is at least 5 (more to come on expected counts on the next slide). If the sample size is too small, consider using other methods, such as Fishers exact test or continuity correction methods, to carry out the hypothesis test. 

If these assumptions are not met, then the results of the test will not be valid. What are expected counts? They are the counts we would expect to see assuming the null hypothesis (of no association) is true. They are computed to reflect identical percent distributions of the event for each group that are equal to the overall percent distribution of the event. The overall percent for a row or for a column is called a marginal proportion (or a marginal probability) and is found by taking the total for the row or for the column and dividing by the total sample size of the study. [Note: They are called marginal because they are located around the edges, or margins, of the contingency table.] 

For example, if the overall proportion who have the event is (a+c)/n (suppose this value is 0.28), then we would expect 28% of Group 1 and 28% of Group 2 to have the event. So we would multiply the overall proportion by the sample size of each group to get the expected count of those with the event. We would use a similar method for finding the expected count of those without the event for each group by using the overall proportion who didnt have the event. 

So when dealing with testing for an association between two groups, we distinguish between observed counts, which are the counts we observed in our sample (a, b, c, and d), and expected counts, which are the counts we would expect if the two variables were independent of one another. In the 2x2 table above, the middle cells display the observed counts in red letters and the formula for calculating the expected counts in the black letters. 

In general, we find the expected counts for the cells of a 2x2 table by calculating the (row total * column total) / total sample size (n).Lets find the expected counts for the TMS and Migraine Pain study. 

The overall proportion of those who were pain free after two hours was 0.305, or 30.5%, in the sample. If the two variables were independent, we would expect 30.5% of the TMS group and 30.5% of the control group to be pain free. So of the 100 in the TMS group, 0.305*100 = 30.5 of them would be expected to be pain free (assuming the null is true). A similar calculation for the control group yields the same result (because the two groups have equal sample sizes). 

Similarly, we use 0.695 to find the expected counts by group for those who were not pain free. Lets check the assumptions for the TMS and Migraine Pain example. 

Are the participants a random or representative sample of all adults who suffer from migraines? We dont have enough information to know for certain if the participants were randomly selected from the population, but it seems reasonable to assume that the researchers would likely have chosen the participants to be as representative as possible of all adult migraine-suffers.

Are the observations independent of one another? Again, we dont have enough information to know for sure, but for now, it isnt unreasonable to assume that the participants in each group are independent of each other and that the participants from one group are independent from participants in the other group. (Usually if there was some kind of matching done, it would have been mentioned in the study description.)

Lastly, are all of the expected cell counts greater than 5? Based on the calculations in the previous slide, yes, the sample size condition is met to safely apply the Chi-square distribution for this situation. 

References:
Image: https://www.pinclipart.com/pindetail/owRJbh_file-mw-icon-checkmark-svg-creative-commons-check/The next step is to define the hypotheses for the Chi-square test in terms of the population. Hypotheses when testing an association between two categorical variables tend to be expressed in words rather than through parameter notation (such as those in previous topics). 

Recall that the null hypothesis defines the skeptical perspective or the nothing is going on situation. When we are looking to see if two variables are associated, the skeptical perspective is that they arent associated at all; they are not related in any way; they are independent of one another.

The alternative hypothesis, on the other hand, defines the competing claim, the thing we are interested in finding evidence for. In this case, the alternative hypothesis is that the two variables are indeed associated with or related to each other and therefore, are not independent of one another.Lets define the hypotheses for the TMS and Migraine Pain example. 

The null hypothesis is that the two variables of interest, pain status (pain-free or not) and study group (TMS or sham control), are independent of each other in the population of migraine sufferers.

The alternative hypothesis is that the two variables of interest, pain status (pain-free or not) and study group (TMS or sham control), are NOT independent of each other in the population of migraine sufferers;  that is, that there is an association or relationship of some kind between them.We evaluate the claims in the usual way: 
We collect evidence (data), 
We summarize the data using exploratory data analysis (a contingency table and perhaps marginal and conditional probabilities), and
We calculate a test statistic to measure the compatibility between the data and the null hypothesis. 

The test statistic for comparing two categorical variables (given that the assumptions are met) is the Chi-square statistic (pronounced kai, rhymes with eye). It quantifies the discrepancy between the observed counts from a sample and the expected counts based on the null hypothesis, combining the information from all cells of the table. To put it in words, for each cell in the table, it computes the difference between the observed and expected counts, squares that difference, and divides that squared difference by the expected count. For a 2x2 table, there would be four of these values. Then, to get the Chi-square statistic, we would add up all of those ratio values from each of the cells. This test statistic has a Chi-square distribution with degrees of freedom that is determined by the size of the contingency table: (r-1)*(c-1), where r is the number of rows in the table and c is the number of columns in the table. For a 2x2 table, there are two rows and two columns, so the appropriate degrees of freedom would be (2-1)*(2-1) = 1. 

Lets look at each piece of the Chi-square statistic. If there is a large discrepancy between the observed count and the expected count, the difference in the numerator will be large. This difference is squared to prevent large positive deviations from cancelling large negative deviations when they are summed. Finally, this squared difference is divided by the expected count for that cell as a way of standardizing each cells value. Note that large values of the Chi-square statistic correspond to samples that do not agree with the null hypothesis. What does a Chi-square distribution with parameter df look like? 

Since the Chi-square statistic can only take on positive values in both the numerator and denominator, the Chi-square distribution can only take on positive values. Three different Chi-square distributions, for 1, 2 and 4 degrees of freedom, are shown in the figure above. Notice that the Chi-square distribution is not symmetric, but rather is positively skewed. The shape of the Chi-square distribution varies depending on the degrees of freedom. As the degrees of freedom increase, the Chi-square distribution shifts to the right and becomes less peaked.

For a 2x2 table, the degrees of freedom are (2  1) times (2  1), which equals one, so the distribution with df = 1 is the appropriate one to use. This is shown in blue on the plot above.

For a 3x2 table, the degrees of freedom would be (3-1) times (2-1), which equals two, so the distribution with df = 2 would be the appropriate one to use. This is shown in pink on the plot above.For the TMS and Migraine Pain example, data were collected and summarized via tables earlier in this presentation. We noticed that the conditional proportions for each group were different and that the TMS group had a higher proportion that were pain free after two hours compared to the control group. But do these results provide evidence that there is an association between pain status and study group? 

To answer this question, we compute a Chi-square test statistic using the observed and expected counts. Adding up all of the computed values for each cell gives a Chi-square statistic of 6.817. This test statistic has a Chi-square distribution with 1 degree of freedom. If the null hypothesis were really true and there was no association between the two variables, then the sampling distribution of the Chi-square statistic would follow a Chi-square distribution with 1 degree of freedom, as shown in the plot above. The calculated Chi-square test statistic for the TMS and Migraine Pain study is 6.817. Our test statistic lies in the right tail of this distribution. This value seems fairly unlikely to occur, but how likely or unlikely is it? Lets quantify this probability. Recall that we quantify how unusual the evidence is compared to what is assumed to be true by computing a p-value. The p-value is the probability that you would obtain a sample difference this unusual if the null hypothesis were really true and any observed difference was simply due to sampling variability. To put it another way, its the probability of getting our sample result (or one even more extreme) if the null hypothesis were true. As before, what counts as extreme depends on the alternative hypothesis. The smaller the p-value, the less consistent or compatible the data are with the null hypothesis. 

Once we have the p-value, we make a conclusion about the strength of evidence we have against the claim by comparing the p-value to the significance level, alpha. If the p-value is less than alpha, we reject the null hypothesis and say we have evidence against the null in favor of the alternative hypothesis. Another way of saying this is that the result is statistically significant. Alternatively, if the p-value is greater than alpha, then we do not reject the null hypothesis and we say we lack evidence against the null. This result would not be considered statistically significant. In either case, remember to always state the conclusion in the context of the problem. Lets evaluate the evidence for the TMS and Migraine Pain example. 

With a Chi-square test statistic of 6.817, and an inherently one-sided test, we are interested in the probability of seeing a test statistic of 6.817 or more extreme if the null hypothesis were really true. Therefore, the probability we are looking for is the area in the upper tail of the Chi-square distribution, above the value 6.817. The p-value for this example is 0.009. For a significance level (alpha) set to 0.05, we reject the null hypothesis because the p-value of 0.009 is less than 0.05. Therefore we conclude that there is evidence that pain status (pain free or not) after two hours is associated with study group (TMS or control) in this population of migraine sufferers.

Since we have rejected the null hypothesis, we could go on to say that more participants in the TMS group experienced pain relief after two hours than in the control group (39% vs. 22%). If we wanted an interval estimate for a plausible range of values for how much more, we could calculate a confidence interval for the relative risk or the risk difference. Here are a few additional notes about the Chi-square test. 

Although this lecture used a 2 x 2 table, the Chi-square test can be carried out for any r x c table, where r is the number of rows and c is the number of columns. For example, if we had a 3 x 2 table, then there would be 6 values to add up to get the Chi-square statistic, and this Chi-square statistic would have a Chi-square distribution with (3-1)*(2-1) = 2 degrees of freedom. 

As previously mentioned in this lecture, testing for an association, as is done in using the Chi-square test, only helps us answer the question of whether the two variables are associated with each other. It does not address how they are associated, that is, it does not provide an effect direction or size (such as a relative risk or risk difference). We would need to calculate these statistics and their confidence interval to answer the question of how.

It is recommended to not use Chi-square tests for small samples. The typical rule that is that the Chi-square test is acceptable as long as no cell in the table has an expected count less than 1.0 and not more than 20% of cells have expected counts less than 5.0. If this is not met, then we need to use an exact test (to be presented in a future lecture). 

We discussed the Pearsons Chi-square statistic in this lecture, but there is an upgraded statistic related to the Chi-square statistic called Yates continuity correction statistic. [Note: This is being mentioned here only because you might see it within your software.] It tries to correct for bias that can result from using the Chi-square test on a 2x2 table with small samples by introducing a more conservative statistic. However, there is no consensus on whether this correction is needed, and it has been debated for decades. If you do come across a small sample situation, we recommend that you use Fishers exact test instead.


<!--chapter:end:11a-ChiSquareTest.Rmd-->

Pearsons Chi-square test, discussed in the previous lecture, is probably the most common hypothesis test used for comparing two categorical variables. It is applicable to tables of almost any size, can be (fairly) easily calculated by hand, and requires few assumptions. However, it is not valid if the number of counts in any of the cells are small. The most common alternative test in that case is Fishers exact test. Fishers exact test can in principle be used for tables of any size, just as Pearsons Chi-square test can, but it is most commonly used for 2x2 tables.

Lets work through Fishers exact test, using the same TMS and Migraine Pain example we used in the previous lecture. [Note: Even though there was not a small sample issue with this study, we are using the same example for demonstration purposes only.]Recall that the research question for the TMS and Migraine Pain study is: Is study group (TMS vs. control) associated with migraine headache pain status (pain-free or not) after two hours? 

Fishers exact test can also answer this kind of research question: are the two categorical variables associated with each other or not? Before we can carry out any inferential method, we first need to evaluate the assumptions of the method. There are two assumptions for Fishers exact test. They are: 

The sample should be a random (or representative) sample from the population, to allow us to generalize the results to that population; 

The observations should be independent of one another. This assumption would be violated if the participants in the study were related in some way to each other (e.g., siblings) or if the observations in one group were related in some way to those in another group, for example, if the participants in the groups were matched or paired. If the observations are paired, then McNemars test should be used instead.

If these assumptions are not met, then the results of the test will not be valid and alternative methods should be used instead. 

As we discussed in the previous lecture, the first two assumptions seem reasonable, as far as we can tell, for the TMS and Migraine Pain study.The hypotheses for Fishers exact test are precisely the same as for Pearsons Chi-square test:

The null hypothesis defines the skeptical perspective or the nothing is going on situation. When we are looking to see if two variables are associated, the skeptical perspective is that they arent associated at all; they are not related in any way; they are independent.

For the TMS and Migraine Pain example, the null hypothesis is that pain status (pain-free or not) and study group (TMS or sham control) are independent of each other in the population of migraine sufferers.

The alternative hypothesis, on the other hand, defines the competing claim, the thing we are interested in finding evidence for. In this case, that the two variables are indeed associated with or related to each other and not independent.

For the TMS and Migraine Pain example, the alternative hypothesis is that pain status (pain-free or not) and study group (TMS or sham control) are NOT independent of each other in the population of migraine sufferers; that is, that there is an association or relationship of some kind between them.For Fishers exact test, we evaluate the claims in the usual way up to a point:
We collect evidence (data), and
We summarize the data using exploratory data analysis (a contingency table and perhaps the marginal and conditional probabilities).

However we do NOT calculate a test statistic to measure the compatibility between the data and the null hypothesis. For Fishers exact test, there is no test statistic. A p-value is calculated directly from the table data, using a computer. Lets explain how this works using our TMS and Migraine Pain example.Fishers exact test first assumes that the marginal counts are fixed at the observed values: one-hundred people in each study group (TMS and control), and 61 people who were pain-free and 139 people who were not pain-free after two hours. The computer then determines how many possible ways there are to arrange the 200 people in the study among the four cells (a, b, c and d) and still keep the marginal counts as they are. 

There would be a lot of possible ways! For example, the arrangement we actually observed was a = 39, b = 61, c = 22, and d = 78, as shown in the table above. Another arrangement that has the SAME marginal counts could be obtained by moving one participant from a to b and balance it by moving another participant from d to c, so a = 38, b = 62, c = 23, and d = 77. This pattern could be continued until you reach a = 0, b = 100, c = 61 and d = 39. And this is not all of the possible arrangements!Next, for each arrangement, the computer calculates the probability of seeing that particular combination of values. The resulting p-value is found by adding up the probabilities from the observed table and every other unusual table. (Note: A table is deemed unusual if it has a probability less than or equal to the observed table.) The Fishers exact test p-value tells us how unusual it is to see this particular set of table data, or one more extreme, IF the two variables are not associated. If the total probability of such unusual tables is small, we can reject the null hypothesis and have evidence for the alternative. 

While there is agreement on how to compute one-sided p-values for Fishers exact test, there is not agreement on how to compute two-sided p-values (and this is most often the value of interest). We just described one approach for how to deal with two-sided p-values (one that we think is easy to understand), but be aware that some software programs might use a different method. 

For the TMS and Migraine Pain study, the two-sided Fishers exact p-value is 0.0137.As usual, once we have the p-value, we make a conclusion about the strength of evidence we have against the null hypothesis by comparing the p-value to the significance level, alpha. If the p-value is less than alpha, we reject the null hypothesis and say we have evidence against the null in favor of the alternative hypothesis. 

For the TMS and Migraine Pain study, with a significance level (alpha) set to 0.05, we reject the null hypothesis because the p-value of 0.0137 is less than 0.05. Therefore we conclude that there is evidence that pain status (pain free or not) after two hours is associated with study group (TMS or control) in this population of migraine sufferers. 

Note that our conclusion is the same using Fishers exact test (p-value = 0.0137) as it was previously using Pearsons Chi-square test (p-value = 0.009). The two tests will not always agree, however, particularly when the sample size is small.

Since we have rejected the null hypothesis, we could go on to say that it appears more participants in the TMS group experienced pain relief after two hours than in the control group (39% vs. 22%).We have now discussed two hypothesis tests that can be used for comparing two categorical variables: Pearsons Chi-square test and Fishers exact test. How do we decide which one to use? There are some tradeoffs.

Pearsons Chi-square test is approximate; the Chi-square test statistic only has a perfectly Chi-square sampling distribution as the sample size goes to infinity, but it is close enough as long as there are more than 5 counts in all of the table cells. Fishers exact test does not require a large sample size, and in fact is easier to calculate for small samples. 

Historically, Pearsons Chi-square test was used because the test statistic could be calculated by hand. Fishers exact test didnt become common until computers were readily available to do the calculations but for REALLY large tables or very large sample sizes, even computers will take too long to calculate the Fishers exact p-value. 

So what to do? There are several schools of thought on this. 

One view is that Pearsons Chi-square test should be the default choice, with Fishers exact test used only when any cell count is less than 5.

Another view is that, since all statistical analysis is done using computers now, Fishers exact test should be the default choice, with Pearsons used only when the sample size is so large or the table dimensions so big that Fishers wont calculate.

Both of these views are completely reasonable and you may choose to follow whichever one you wish. Just make sure to report the method you chose. 

What should you NOT do? You should definitely NOT carry out both tests, and if they disagree, choose the one that gives you the result you prefer. This is considered statistically unethical. (The two tests will sometimes disagree, particularly when the sample size is small.) You should decide in advance which one test is appropriate, using either of the views described above, and carry out that test.

<!--chapter:end:11b-FishersExactTest.Rmd-->

We have so far discussed two hypothesis tests for comparing two categorical variables: Pearsons Chi-square test and Fishers exact test. Both of these tests are appropriate only when the observations are independent of one another. What test should we use if the observations are paired, or dependent on one another, and the outcome is binary? Answer: McNemars test! Recall that paired data or observations arise in a number of different situations.

Paired observations arise when measurements are taken on the same participants before and after some intervention or event occurs. The pre- and post- measurements are not independent of each other, since they were obtained on the same person. 

Paired observations also arise when a study is carried out on matched participants, such as twins, siblings, or participants matched as pairs in other ways (by age, gender, and race, for example). The two members of the pair are not independent of one another. They are more like each other than two randomly selected people from a population would be.

Paired observations also arise in medical studies when measurements are conducted on the left and right side of the body. For example, a skin treatment might be tested on the right arm, with the left arm being the control. The two observations are not independent of each other, since they are on the same person.

When the outcome is a binary variable, we can carry out a McNemars test. Another way to think about it is that McNemars test is appropriate for comparing a binary outcome in paired observations, just as the paired t-test is appropriate for comparing a continuous outcome in paired observations. Another name for this test is McNemars Chi-square test because the test statistic has a Chi-square distribution.Lets start with an example of a paired design. 

Researchers1 were interested in whether a surgical procedure called cholecystectomy (CHE), which is removal of the gall bladder, would affect bowel function. They recruited 51 participants who were about to have CHE surgery, and measured their bowel function as number of stools per day (one or fewer vs. more than one) before the surgery and again one month after surgery. 

Since the before and after measurements were done on the same patients, the data are not independent, and summarizing the data using a 2x2 contingency table like the ones weve seen in earlier lectures would not be appropriate. Instead, the data are summarized by pairs as shown above, where each count represents one PAIR of before/after measurements (or one person, in this case). So, for example, 25 of the patients had one or fewer stools per day BOTH before and after the surgery. Another 11 patients had more than one stool per day both before and after surgery. There were 15 patients who had one or fewer stools per day before surgery but more than one stool per day after surgery. No patients in this study went in the other direction.

The researchers wanted to know if the CHE surgery had an effect on stool frequency. We can see that 11 of the 51 patients (22%) had more than one stool per day before CHE surgery (and 40 patients, 78%, did not), while 26 of the 51 patients (51%) had more than one stool per day after the surgery (and 25 patients, 49%, did not). Could this large of a difference in bowl function between before and after surgery be the result of sampling variability or is this evidence of a real difference? Lets use statistical inference methods to answer this question.

Before we can answer this question, we have to understand some terminology specific to paired binary observations. 

Reference: 
1Sauter, G. H., Moussavian, A. C., Meyer, G., Steitz, H. O., Parhofer, K. G., & Jngst, D. (2002). Bowel habits and bile acid malabsorption in the months after cholecystectomy.The American journal of gastroenterology,97(7), 1732.
Data: Dawson, B., & Trapp, R.G. (2004). Research questions about one group. In Basic and clinical biostatistics (4th ed., pp. 121). New York, NY: McGraw-Hill.In our paired contingency table, shown above, there are two cells where the measurement of interest did not change. Cell a shows that 25 of the patients had one or fewer stools per day both before and after CHE surgery and cell d shows that 11 of the patients had more than one stool per day both before and after CHE surgery. Cells a and d are called concordant cells (or agreeing cells). They account for the number of patients whose bowel function was not affected by the surgery. These cells provide no information about the difference between pairs so this data is not used in the calculation of the test statistic.The cells of most interest for paired binary data are cells b and c (called discordant cells or disagreeing cells) because they represent pairs that saw a difference or change. In the CHE and Bowl Function study, cell b shows that 15 of the patients went from having one or fewer stools per day before CHE surgery to having more than one stool per day afterwards (i.e., their bowel function got worse) and cell c shows that zero (0) patients went in the other direction, from more than one to one or fewer stools per day after surgery. Generally speaking, they account for the number of patients whose status was affected by the surgery. Discordant cells are the only values used in the calculation of the test statistic.In defining the research question for McNemars test, we are interested in whether the two paired binary variables are associated with (or related to) each other. 

For the CHE and Bowel Function study, the research question could be phrased in multiple ways, and all are equivalent to each other. For example,
 
Is CHE surgery associated with bowel function?, or,

What is the effect of CHE surgery on bowel function?, or,

Is the proportion of patients with worse bowel function (> 1 stool per day) different before and after CHE surgery?Before we can carry out any inferential method, we first need to evaluate the assumptions of the method. The assumptions for McNemars test are: 

The sample should be a random (or representative) sample from the population, to allow us to generalize the results to that population; and

The observations are paired in some way.

We need to make sure the sample is large enough for the underlying theoretical distribution to hold. For the McNemars test, we check this condition by summing the counts in the two discordant cells. One rule of thumb is checking whether b + c > 10. If this is true, then the McNemars test statistic can be approximated by the Chi-square distribution. Otherwise, if the number of discordant pairs is less than 10, consider using other methods, such as an exact binomial test or continuity correction methods, to carry out the hypothesis test. 

If these assumptions are not met, then the results of the test will not be valid. For the CHE and Bowel Function example, are the assumptions met? 

Are the participants a random or representative sample of all adults who might undergo this surgical procedure? We dont have enough information to know for certain if the participants were randomly selected from the population, but it seems reasonable to assume that the researchers would likely have chosen the participants to be as representative as possible of all people going in for CHE surgery, at least in that region or country. 

Are the observations paired? Yes, because the same measurement (number of stools per day) was done on each participant both before and after CHE surgery.

Lastly, is the sample size large enough to proceed with the McNemars test? Yes, the number of discordant pairs is equal to 15, which is greater than 10, so this condition is met to safely use the Chi-square distribution for this situation.

References:
Image: https://www.pinclipart.com/pindetail/owRJbh_file-mw-icon-checkmark-svg-creative-commons-checkThe next step is to define the hypotheses for the McNemars test in terms of the population. We could state the hypotheses for McNemars test in two different but completely equivalent ways: as a test of independence or as a test of proportions. 

Lets write out the two approaches for the CHE and Bowel Function example. 

As a test of independence: 
The null hypothesis is that the two variables of interest, bowel function (< 1 or > 1) and CHE surgery timing (pre vs. post), are independent of each other.

The alternative hypothesis is that the two variables of interest, bowel function (< 1 or > 1) and CHE surgery timing (pre vs. post), are NOT independent of each other; that is, that there is an association or relationship of some kind between them.

Alternatively, as a test of proportions:
The null hypothesis is that the proportion of patients who have more than one stool per day is the SAME both before and after CHE surgery. 

The alternative hypothesis is that the proportion of patients who have more than one stool per day is DIFFERENT before and after CHE surgery. 

This test of proportions approach is the one that is more commonly used.We evaluate the claims in the usual way: 
We collect evidence (data), 
We summarize the data using exploratory data analysis (a paired contingency table and perhaps marginal or conditional probabilities), and 
We calculate a test statistic to measure the compatibility between the data and the null hypothesis. 

As previously mentioned, the test statistic for comparing paired binary data (given that the assumptions are met) only uses the information from the discordant cells, cells b and c of the table, because they are the only ones that provide us information about the difference in the pairs. The test statistic is called McNemars Chi-square statistic. It quantifies the discrepancy between the two discordant cells. It computes the difference between the counts in cells b and c, squares that value, and then divides by the sum of the counts in those two cells. This test statistic has a Chi-square distribution with 1 degree of freedom (because we can only use McNemars on 2x2 paired table). 

There are several things to notice about this test statistic:

If the null hypothesis were true, then cell b and cell c should be approximately the same (i.e., the same number of people got worse as got better), and the test statistic would be equal to zero. However, if the null were not true, then the difference between the two discordant cells would be large. 

It doesnt matter whether the counts in cell b or in cell c are larger (i.e., whether more people got worse or whether more people got better), since the difference is squared. The only thing that is important is how large the difference between cells b and c is.

Since the McNemars Chi-square statistic can only take on positive values in both the numerator and denominator, the Chi-square distribution can only take on positive values (similar to what we saw in a previous lecture on the Chi-square test). Because of this, McNemars test is inherently one-sided. For the CHE and Bowl Function example, data were collected and summarized via tables earlier in this presentation. We noticed that the there was a higher proportion of participants that had more than one stool per day after surgery than before. But do these results provide evidence that there is an association between bowl function and CHE surgery? 

To answer this question, we compute a McNemars Chi-square test statistic using the counts from the discordant cells, which ends up being a value of 15. This test statistic has a Chi-square distribution with 1 degree of freedom. So how unusual is our McNemars test statistic value? If our null hypothesis were really true and there were no association between the two variables, then the sampling distribution of the McNemars statistic would have a Chi-square distribution with 1 degree of freedom. The calculated test statistic for the CHE and Bowel Function study is 15.0. Our test statistic lies in the right tail of this distribution. This value seems fairly unlikely to occur, but how likely or unlikely is it? Lets quantify this probability. With a McNemars test statistic of 15.0, and an inherently one-sided test, we are interested in the probability of seeing a test statistic of 15.0 or more extreme if the null hypothesis were really true. Therefore, the probability we are looking for is the area in the upper tail of the Chi-square distribution, above the value of 15. The p-value for this example is 0.0001, or < 0.001. For a significance level (alpha) set to 0.05, we reject the null hypothesis because the p-value of 0.0001 is less than 0.05. Therefore we conclude that there is evidence that the proportion of patients who have more than one stool per day is not the same before and after CHE surgery in this population. 

Since we have rejected the null hypothesis, we could go on to say that the proportion of patients who had more than one stool per day increased after CHE surgery, from 11 out of 51 or 22% before surgery, to 26 out of 51 or 51% one month after surgery. The CHE surgery appears to significantly worsen bowel function in this population.Besides testing for an association between paired observations, we can also calculate a confidence interval for the odds ratio of paired data to get an estimate of the true (population) value. Similar to the hypothesis test presented earlier, the only information that contributes to the odds ratio is the discordant cells. Also notice that the procedure looks identical to the one presented in an earlier lecture on odds ratio. 

The first step uses the general formula for a confidence interval: point estimate plus or minus the margin of error. But it is for the natural log of the odds ratio, where the sample odds ratio for paired data is computed as cell b divided by cell c. This is because the sampling distribution for odds ratio does not follow a Normal distribution, no matter what the sample size is. However, if we take the natural log of the sample odds ratio, then the sampling distribution for this transformed measure is approximately Normal. Applying this transformation allows us to use Normal-based methods for calculating the confidence interval. 

So, Step 1 is to calculate the CI for the natural log of the odds ratio, where: 

--The point estimate is the natural log of the sample odds ratio, ln(b/c). 

--The degree of confidence is the appropriate z-value, and 

--The estimated standard error of the natural log of odds ratio is as shown on the slide. 

Then, Step 2 is to back-transform the confidence interval values found for the natural log of OR by taking the antilog: the exponential function. This will produce the lower and upper limits of the confidence interval for odds ratio (on the original scale). 

The confidence interval formula presented on this slide only applies when all of the assumptions are met for comparing paired data on a categorical variable. Now lets calculate the confidence interval for the odds ratio for the CHE and Bowl Function study. 

The sample odds ratio of having more than one stool per day, after compared to before surgery, isuh oh! We cant directly calculate this value because it would b 15 divided by 0, which gives us an undefined value, and we know the odds are different. One recommendation for dealing with tables that have zero counts in cells is to add 0.5 to each cell and then compute the estimates using the adjusted values. This technique is referred to as the Haldane-Anscombe correction. Because we have this situation for our example, lets apply it to get the odds ratio and its confidence interval. 

The sample odds ratio for this example, then, is 15.5 divided by 0.5, which is 31. Taking the natural log of this value gives us 3.434, and the estimated standard error for the natural log of OR is 1.437. The z-value for a 95% confidence interval is the value in the Standard Normal distribution with 0.975 area lying below that value, so the z-value is 1.96. Putting all of those values togetherthe point estimate, the z-value, and the standard errora 95% confidence interval for the ln(OR) is between 0.618 to 6.250. Now we need to exponentiate those limits to get the 95% CI for the odds ratio: e^(0.618) equals 1.855 and e^(6.250) = 518.116. 

We are 95% confident that the odds ratio of having more than one stool per day comparing after to before CHE surgery is between 1.855 and 518.116. Or we could say, a range of plausible values for the true odds ratio of having more than one stool per day comparing after to before surgery is from 1.855 and 518.116. Because the 95% confidence interval does not include 1, then one is not a plausible value and we can conclude that the odds ratio of having more than one stool per day comparing after to before CHE surgery is statistically significant (same conclusion that we came to in the test). There is evidence of a difference in odds of having worsening bowl function, with after surgery having between 1.855 to 518.116 times the odds of before surgery.

<!--chapter:end:11c-McNemarsTest.Rmd-->

Thus far in this course, we have discussed statistical methods for comparing means of a numerical variable between two or more groups (using t-tests, confidence intervals, or ANOVA) and for comparing two categorical variables (using confidence intervals for relative risks or odds ratios, Chi-square tests, or Fishers exact tests). This week, we move on to examining methods for comparing two numerical variables. This lecture will focus on how to graphically depict (via scatterplots) and numerically summarize (via correlation coefficients) the relationship between two numerical variables.Recall the Mercury Content in Fish example from a previous lecture. The average mercury content in fish in a lake is actually rather difficult to measure, in practice. It would be great if researchers could use some other value that is more easily obtained from a water sample, such as acidity, to give information about mercury content. The researchers from that study also collected data on water acidity and alkalinity from the 53 Florida lakes. Is there a relationship between mercury level in fish and acidity? Because the two variables are quantitative, we can display this relationship by creating a scatterplot (as shown above). 

Before presenting the formal approach for evaluating a scatterplot, what do we notice from the plot itself? It appears that as acidity level increases, mercury level in fish tends to decrease for lakes. What other things should we examine when we look at a scatterplot? 

References:
Study: Lange, T. R., Royals, H. E., & Connor, L. L. (1994). Mercury accumulation in largemouth bass (Micropterus salmoides) in a Florida lake. Archives of Environmental Contamination and Toxicology, 27(4), 466-471.
Dataset: Lock, R.H., Lock, P.F., Lock Morgan, K., Lock, E.F., & Lock, D.F. (2013). Statistics: Unlocking the power of data (1st ed.). Hoboken, NJ: John Wiley & Sons, Inc. When evaluating a scatterplot, we assess four features of the plot:

Form. Do the points form a trend? If so, do they appear to follow a straight line (which we call linear association), or some other curve or pattern? 

Direction. Do the points generally have an upward direction or downward direction when we look from left to right? Or, do the points just appear to be randomly scattered (imagine a splat of paint on a canvas)? An upward direction is called a positive association and a downward direction is called a negative association. 

Strength. How closely do the points adhere to some imaginary trend line? If they appear to have a lot of scatter, we say there is a weak association. If they appear to be really close to the trend line, we say there is a strong association. This can be difficult to judge based on visual clues alone, but with practice and experience, you can start to develop a sense for weak vs. strong relationships in plots. 

Outliers. Are there any points that are striking deviations from the overall pattern? Lets apply the four features to the Mercury Content in Fish and Water Acidity scatterplot.

It appears that the relationship between average mercury and acidity for lakes is a moderate, negative, linear association with no obvious outliers. 

After visual inspection of the relationship, if the relationship appears to be roughly linear, we can calculate a correlation coefficient to numerically describe the linear relationship between two numerical variables.
The correlation coefficient quantifies the strength and direction of a linear association between two quantitative variables. The equation for computing the correlation coefficient, r, is presented on the slide. Lets dissect the formula to better understand how it works. 

Lets start with a particular point or observation in the dataset. For that observation, we calculate its z-score for the x-variable and its z-score for the y-variable. Recall that a z-score will be positive if the value is above the average (for X or Y) and will be negative if it is below the average (for X or Y). We then multiply the two z-scores together to see how they co-vary. Three things could happen: 
If the x- and y-values for the observation are both above their averages, both z-scores will be positive and the product of the z-scores will be positive; or, 
If the x- and y-values are both below their averages, both z-scores will be negative and the product of the z-scores will still be positive; or, 
If the x- and y-values are in opposite directions (e.g., higher than average x-value and lower than average y-value), one of the z-scores will be positive and the other will be negative, resulting in a negative product. 

We then (1) calculate the z-scores for X and Y for all observations in the dataset, (2) multiply the z-scores for each observation, (3) add up all of the products, and (4) divide by n-1. Essentially, the correlation coefficient is the average of the product of the z-scores (for X and Y). [Recall that average is just adding up a bunch of somethings, and dividing by how many somethings we have.] To put it another way, on average, how do the z-scores for X and Y co-vary or relate? 

If the two variables, X and Y, relate in the same way for every single observation, then the correlation coefficient will be +1 or -1, with positive indicating that they co-vary in the same direction and negative indicating that they co-vary in the opposite direction. That is, r will equal +1 or -1 when there is an exact linear relationship between x and y. If there is no apparent linear relationship between the variables (i.e., no linear pattern at all), then the correlation will be near zero. The plots on the slide provide a visual reference for how the scatterplot would look for a particular correlation coefficient value. Here are some things to notice as you are examining the plots:

The correlation coefficient, r, can take on values from -1 to +1. 

The sign of r (positive or negative) indicates the direction of the association. 

As previously mentioned, looking at the magnitude of the value (and not the sign) indicates the strength of the association; that is, how tightly the points are clustered around the imaginary linear trend line. Although there is no consensus on what constitutes a strong or weak association, here are some rough guidelines:
If the absolute value of r is greater than or equal to 0.7 (i.e., |r| > 0.7), that indicates a strong association.
If the absolute value of r is between 0.3 and 0.7 (i.e., 0.3 <|r| < 0.7), that indicates a moderate association.
If the absolute value of r is less than 0.3 (i.e., |r| < 0.3), that indicates a weak association.

The correlation coefficient is unit-less. 

The correlation coefficient is symmetric. That is, for correlation, it doesnt make any difference which variable you label as X and which as Y. The correlation between variables X and Y is the same as the correlation between Y and X. (However, this is not the case for regression, as you will see in a future lecture). 

Reference:
Images: https://www.mathsisfun.com/data/correlation.htmlWhat is the correlation coefficient for the Mercury Content in Fish and Water Acidity example? The value turns out to be -0.575. 

The negative sign indicates a negative association between the two variables. A negative association means that as one variable increases, the other one decreases. In this example, as lake acidity increases, the average mercury content in a lakes fish decreases. 

The magnitude of the value tells us that there is a moderate association between the two variables. 

Both of these aspects were also gleaned from our visual inspection of the plot, but the correlation coefficient can be helpful to provide an objective assessment of the relationship, as well as to compare with relationships from other studies. Here are a few additional notes about the correlation coefficient.

The first thing you should do when you have to carry out any type of analysis is plot the data! This is especially true when it comes to correlation. The correlation coefficient only provides information about the linear association between two numerical variables (if any). So even if the relationship between two variables is curved, it is still possible to calculate a correlation coefficient but it will be meaningless. For example, a correlation near 0 does not necessarily mean that the two variables are not associated  they may have a strong nonlinear or curved relationship. Lesson to the wise: Look before you leap when you interpret the r value. 

To emphasize this point, lets look at a famous example, called Anscombes quartet1. All four datasets have nearly identical correlation coefficients (r = 0.816), but their scatterplots, shown on the slide above, look very different. You can see in the upper-right plot that there is a perfect curved relationship between the two variables, yet merely looking at the correlation coefficient, r, gives you no indication of this. The correlation coefficient provides meaningless information regarding this nonlinear relationship. 

For the bottom two plots, we see that the correlation coefficient is being influenced by outliers. In the bottom left plot, there is a nearly perfect linear relationship, but the one outlier is influencing r and lowering the value from 1 to 0.816. In the bottom right plot, the majority of the points indicate there isnt any relationship between the x and y variables, but one high outlier is enough to result in a high correlation coefficient. This brings us to the next note about correlation

Reference: 
Anscombe, F.J. (1973). Graphs in statistical analysis. American Statistician, 27,  17-21. 
Plot: https://www.flickr.com/photos/113716728@N08/11806491624/, courtesy of Eric-Jan Wagenmakers.The correlation coefficient is influenced by outliers. Changing or excluding points that are potential outliers can change the results. Therefore, r is not robust to outliers, especially outliers that lie to the far right or far left relative to the other points. This is another reason why you should look before you leap when interpreting the r value. However, you should not instantly assume that potential outliers are bad points that are just messing up the analysis. Investigate to see if they are actual mistakes (such as typos) in the data. If not, they may be real observations that indicate that the relationship between X and Y is not as simple as you had hoped. Real observations should never be excluded from an analysis without a really good reason. It is possible that the potential outliers are the most interesting and information-filled observations in the study. ?
The correlation coefficient that was discussed in this lecture is called the Pearsons correlation coefficient. When people refer to the correlation (or more appropriately, the correlation coefficient), they are most often referring to this one. However, there are other kinds of correlation coefficients that quantify the relationship between other kinds of variables. For example, the Spearmans rank correlation coefficient measures the relationship between two variables that are ordinal. 

Lastly, a correlation coefficient that indicates a strong association between two variables does not necessarily imply a cause-and-effect relationship. Possible explanations for why two variables strongly co-vary include: 
X causes Y
Y causes X
There is a third variable that causes both X and Y.
X, Y, and all other factors are all part of a complex system and the observed correlation is just a peek at this much more complicated set of relationships. 
X and Y are not related at all, and the observed correlation is just a coincidence. (We would need inference to address this question). 
Most people immediately think of the first two possibilities but ignore the rest. Its worth also considering the other possibilities when interpreting correlation coefficients. 

<!--chapter:end:12a-Correlation.Rmd-->

Similar to correlation analysis, linear regression is a technique that is used to explore the relationship between two numerical variables that have a linear relationship. Unlike correlation, however, regression allows us to do more exploring of the relationship. For one, it allows us to determine the line that best describes the linear relationship between the two numerical variables. It also allows us to estimate the change in one variable (Y) that corresponds to a given change in the other variable (X). Lastly, it allow us to predict the value of one variable (Y) for a specified value of the other (X).  

If only one numerical variable (X) is used to predict the value of the other numerical variable (Y), the analysis is called simple linear regression (often denoted as SLR). When two or more variables (X1, X2, etc.) are used to predict the value of a numerical variable (Y), the analysis is called multiple linear regression. We will cover multiple linear regression in a future lecture.

Lets work through SLR, using the same Mercury Content in Fish and Water Acidity example we used in the previous lecture.We have used several terms throughout the course when we examine the relationship between two variables. For example, in a previous lecture, we talked about exposure and disease, where exposure occurs first and we are interested in its relationship (if any) to the disease. We also talked about factor and outcome: for example, how does the factor affect the outcome. You might have even encountered X and Y or independent and dependent language sometime in your previous coursework. In statistics, you may see the terms explanatory (or predictor) variable and response variable. The table on this slide lists synonyms for these terms that you might encounter in practice or in reading the medical or public health literature. 

Note: Unlike when computing the correlation coefficient, in regression, its important to correctly identify the response (Y) and explanatory (X) variables. The response variable is the variable to be predicted from or explained by the explanatory/predictor variable. 

In our example, mercury in fish is the response variable since we are interested in predicting this from water acidity, the explanatory variable. Lets explore the process of fitting a line to a set of data. 

When two numerical variables have a statistical relationship (as opposed to a deterministic one) the data points will not all fall perfectly on a straight line. Instead, the data will appear to be a cloud of points. So with the variability that is associated with the points around an imaginary line, how do we decide which line is the best fitting? 

A few of the many possible lines through the data points in this study are illustrated in the plot above. Which one do you think is the best line for the data? As you are contemplating the answer, think about the criteria that you are using to decide which line is the best fit to the data. (For example, are you looking for the line to go nearest to the middle of all of the data? Or the line that seems to have the same slope as the general trend of the data? Or something else?)If you chose Line A as the best fitting line, then you would be correct. The reason that Line A is the best fit is because it gets closest to all of the points (in an intuitive sense). This line goes by various names, such as the line of best fit or the linear regression line or the fitted regression line or the least squares regression line. The least squares regression line is the line that minimizes the distances between the line itself and the observed values of the response variable (y). To obtain this line, we only consider the vertical distances from each point to the line, as shown on the plot above. These vertical distances are called residuals. We square these distances (so that points below the line dont cancel out points above it), add them up for all points, and find the line that minimizes this sum of squared distances. From the equation for the best fit line, we can calculate the predicted y value for each x value: the predicted values form the line (shown in blue above).

A formal definition of a residual, e, is the difference between the observed and predicted values of the response variable y; that is, observed y minus predicted y, or y minus y-hat, for a given data point. The predicted values of y are the y-values along the best-fitting line. As depicted in the scatterplot above, a residual is the vertical deviation (or distance) from a data point to the line. Points above the line will have positive residuals and points below the line will have negative residuals. If the predicted values closely match the observed values, the residuals will be small. 

For example, for the data point where x = 7.8 and y = 0.77, the predicted y-value (y-hat) is 0.343 (as indicated by the yellow star on the line at x = 7.8). So the residual for that data point is 0.770  0.343 = 0.427. Since the residual is positive, we know that the observed y-value is above the line (which we can also see from visual inspection of the scatterplot). 

Note that the predicted value of y for a given x-value is not the same as the observed value of y in most cases. The predicted y-value will be the same as the observed value of y only in cases where the data point lies exactly on the line. Thus far, weve been vague about the components that go into the regression line. You may remember from a previous math class that the equation of a line can be written as y = mx + b, where m is the slope and b is the intercept. In statistics, we have the same form but use different letters: b0 for the intercept and b1 for the slope. We write the least squares line as y-hat = b0 + b1x. Lets break down the parts the go into the regression line. 

The y-hat denotes the y-values predicted by the regression model. 

The y-intercept, b0, identifies where the regression line crosses the y-axis, or the value of y when x equals 0. 

The slope, b1, reflects the change in Y for a one-unit change in X, or rise over run, where you rise Y amount for one-run over in X. 

The least squares regression line is obtained by choosing slope and intercept values that minimize the sum of the squared residuals.

In practice, finding the values for the intercept, b0, and slope, b1, is done using a computer. However, these values can also be calculated by hand from summary statistics using the equations given on the slide above, using the  sample mean of x [x-bar], sample mean of y [y-bar], sample standard deviation of x [sdx], sample standard deviation of y [sdy], and the correlation between the variables x and y [r].*Note: There was a typo in the transcript. While the transcript has been updated for Spring 2021 (bold and underlined text is the updated information), the audio recording still has the error. This will get corrected once we return back to working in the office.

The regression line for the Mercury Content in Fish and Water Acidity example is shown in the equation and plot above. The value of the slope, b1, is -0.152 and the value of the intercept, b0, is 1.53. The equation states that for a given value of water acidity, pH (the x  variable), the predicted average mercury content in fish (in ppm) (y-hat) is given by y-hat = 1.53  0.152x. (*minus sign updated in equation) 

The sign of the slope is negative, which means there is a negative linear relationship between water acidity and average mercury content in fish; lakes with higher water acidity will tend to have lower levels of mercury in fish, and vice versa. The magnitude of the slope tells us that for every 1-unit increase in water acidity, we would expect the average mercury content in fish to decrease by 0.152 ppm. 

The intercept describes the expected average mercury content in fish when there is no water acidity in a lake. Intercepts do not always have realistic interpretations, as in this example. If a lake has a pH of 0, it is considered very acidic (think battery acid acidic). This is not a realistic situation.We can use the regression line to predict a value of y for a particular value of x. 

Lets see how this works for our example. What is the expected average mercury content in fish (in ppm) for a lake that has water acidity (pH) of 6.3? Insert 6.3 into the equation in place of x and solve for y-hat: 

y-hat = 1.53 - 0.152*6.3 = 0.572 

The predicted average mercury content in fish for a lake that has water acidity of 6.3 is 0.572 ppm. A question we might have once we obtain the regression line is, How well does the line fit the data? This can be assessed using the coefficient of determination, or R2 value (which is the square of the correlation coefficient, r).  R2  describes the proportion of the variation in y that is explained by the regression line. The larger the value of R2, the better the fit of the regression line to the data. 

For our example, the R2 value is 0.33, which means that 33% of the variation in average mercury content in fish (Y) is explained by the linear relationship with water acidity (X). Since only 33% of the variation in average mercury content in fish is explained by water acidity, there are likely other variables that explain some of the remaining 67% of the variation. Multiple regression analysis uses more than one explanatory variable to predict the response variable, as we will discuss in a future lecture. If there are other explanatory variables significantly related to average mercury content in fish, including them in a multiple regression model will increase the R2 value. Here are a few additional notes about simple linear regression.

The first thing you should do when you have to carry out any type of analysis is plot the data! Similar to correlation, this is also true when it comes to simple linear regression. The regression line is a model describing the linear association between two numerical variables. A relationship that is curved or wavy would need a nonlinear model to appropriately describe the association. 

Predictions using regression line equations are only valid within the range of x-values in the collected data. For the Mercury Content in Fish and Water Acidity example, the range of acidity (pH) is between 3.6 and 9.1. It would not be appropriate to use this regression line equation to predict average mercury in fish for a lake that has a pH of 10 or a lake that has a pH of 2. There may be a different relationship between these two variables beyond the range of the collected data so the relationship identified by the regression line equation should not be extrapolated beyond the range of the x-values. 

Similar to correlation, regression is influenced by outliers. Therefore, regression techniques are not robust to outliers, especially outliers that lie to the far right or far left relative to the other points. Outliers in the Y direction will have large residuals. Outliers in the X direction can have a large influence on changing the regression results. 

As you could see from the formula for finding the slope, there is a close connection between correlation and regression. Because r is used to find the slope, both values will have the same sign. In other words, both values indicate the direction of the relationship. Where they differ is that the magnitude of the correlation coefficient indicates the strength of the linear relationship, whereas the magnitude of the slope indicates how much the response variable changes as a result of a change in the predictor. 

<!--chapter:end:12b-SLR.Rmd-->

In previous lectures, we provided an introduction on how to describe (via summary statistics) the relationship between two numerical variables. The summary statistics that were presented were the correlation coefficient and the regression coefficients from a simple linear regression model. Just as before, we know that summary statistics will vary from sample to sample, so we are interested in knowing: Could the relationship we observe be the result of sampling variability or is this evidence of a real relationship? Lets use statistical inference methods to answer this question. 

The same example we used in previous lectures, Mercury Content in Fish and Water Acidity, will be used throughout this lecture.We have talked about a regression model several times in this lecture and in a previous one. You may be wondering, what does model mean? A model, in a general sense, is a representation of something else. In science, models are created as a simplification and idealized understanding of some physical system. In statistics, a model is an equation or set of equations that describe, represent, or approximate a response variable. This model must specify both the ideal predictions for what we expect will happen and allow for random scatter around the ideal. In crude notation, we could write this out as 

Response = Trend + Random, or 

Data = Signal + Noise. 

For a simple linear regression model, the observed Y value (the response) can be modeled by a line (which is the trend part) and some scatter of the data points above and below the line (which is the random part). The equations presented on the slide have exactly this form, but one describes the simple linear model for a sample and the other for a population.

Similar to previous lectures, we have different notation for the regression coefficients depending on whether we are talking about a sample or a population. If we are talking about a sample, the notation for the regression coefficients uses lower-case Roman letters, b0 and b1, as we saw previously. And if we are talking about a population, the notation uses Greek letters, beta0 and beta1. 

In the population SLR model, we treat the trend as a fixed component, meaning that the intercept (beta0) and slope (beta1) each have one single true value for that population. In contrast, the random component, epsilon, takes on a different value for each data point. We call that random part error, but that doesnt mean there is a mistake; it just refers to any random variability. Because error is random, we can make assumptions to the behavior of the randomness. And because these population coefficients are unknown to us, we use the coefficients determined from the sample (b0 and b1) to make an inference about the population coefficients. Lets see how to do this with our SLR example. Recall that the fitted regression line for the Mercury Content in Fish and Water Acidity example was:

y-hat = 1.53 - 0.152*x

where y-hat is the predicted average mercury content in fish (in ppm) and x is the water acidity.

Y-hat indicates the value predicted by the model and the values for Y-hat for all possible X-values form the regression line. On the other hand, Y indicates the actual, observed value for a given point, which will differ from the model-predicted value (i.e., from the regression line) by some amount, e, where e is the error or residual value for that point. 

Based on the fitted regression equation from this sample, the slope of the line is negative. However, this slope (and the intercept) are only estimates of the population parameters, beta0 and beta1. We know that if we take another sample, these estimates would probably be different. But, as usual, what we want to know is: Could the relationship we observed be the result of sampling variability or is this evidence of a real relationship? Lets use statistical inference methods to answer this question.The research question we will answer for this example is: Is there a relationship between average mercury content in fish and water acidity in Florida lakes?Lets define the hypotheses for the simple linear regression. 

There are two coefficients we could test, but we are only really interested in one of them. The test in simple linear regression is most often about the slope, rather than about the intercept, because testing the slope answers the question of whether there is a relationship between the two variables or not. 

Recall that the null hypothesis defines the skeptical perspective or the nothing is going on situation. When we are looking to see if two variables are associated, the skeptical perspective is that they arent associated at all or they are independent of one another. For simple linear regression, this would occur if the slope equals 0. 

The alternative hypothesis, on the other hand, defines the competing claim, the thing we are interested in finding evidence for. In this case, the alternative hypothesis is that the two variables are indeed associated with or related to each other and therefore, are not independent of one another. We would write the alternative hypothesis for simple linear regression as the slope does not equal 0. Lets define the hypotheses for the Mercury Content in Fish and Water Acidity example. 

The null hypothesis is the slope between mercury content in fish and water acidity in the population is zero. We could also state it as, the true coefficient for water acidity is zero. In notation, beta1 = 0. 

The alternative hypothesis is the slope between mercury content in fish and water acidity in the population is not zero, or the true coefficient for water acidity is not zero. In notation, beta1  0. We evaluate the claims in the usual way: 
We collect evidence (data), 
We summarize the data using exploratory data analysis (scatterplot, correlation coefficient, simple linear regression line), and
We calculate a test statistic as a measure of the compatibility between the result from the data and the null hypothesis.
We can also calculate a confidence interval that provides a plausible range of values the population parameter can take.

In a previous lecture, we used a t-test statistic for testing the mean of a numerical variable. Regression is very similar. Recall that the general t-statistic formula is the sample estimate minus the null value, all divided by the standard error for that estimate. Written out for simple linear regression, it is the sample slope (beta1) minus 0 (the null value), divided by the standard error of the slope. The equation for the standard error of the regression slope is a little ugly (and time-consuming to carry out by hand), so we will rely on software to compute the standard error for us and leave the explanation for a future statistics course. This test statistic has a t-distribution with n-2 degrees of freedom. [Note: The df = n-2 because we are estimating two parameters (coefficients) in this model: the slope and the intercept.] 

Confidence intervals are an alternative method for summarizing the evidence provided by the data. Recall that the general formula for a confidence interval is the point estimate plus or minus the margin of error. Filling in these details for estimating a population slope:

--The point estimate is the slope estimated from the sample. 

--The margin of error is the degree of confidence, which is calculated from the appropriate t-value times the standard error for the slope. The appropriate degrees of freedom for the t-value is n-2 and the standard error for this situation is the SE for the slope (obtained via software). 

The confidence interval formula presented on this slide only applies when all of the assumptions for simple linear regression are met. (These assumptions will be presented in a future slide.) Remember to check the assumptions first before carrying out any inferential method.  For the Mercury Content in Fish and Water Acidity example, data were collected and summarized using plots and summary statistics (presented in a previous lecture). The simple linear regression output that you might see from your software is given above. The slope from this sample was -0.1523. Would this slope be unusual if the true slope were really zero?

To answer this question, we compute a t-test statistic using the data and the null value. The t-test statistic for this example is -0.1523 minus 0, all divided by 0.0303. Solving this out results in a t-test statistic of -5.024 (which we can see in the output from our software). This test statistic has a t-distribution, where the degrees of freedom are 53  2 = 51.

We can also compute a confidence interval for the slope using information in the output. The sample slope is -0.1523 and the standard error for the slope is 0.0303. The t-value for a 95% confidence interval is the value in the t-distribution with 51 degrees of freedom and 0.975 area lying below that value, so the t-value is 2.008. Putting all of those values togetherthe point estimate, the t-value, and the standard errora 95% confidence interval for the true slope is -0.213 to -0.091. The last step in our inferential framework is to make a decision about how usual or unusual the evidence is compared to the claim about the population parameter. Recall that we do this by quantifying how unusual the evidence is compared to what is assumed to be true by computing a p-value. Once we have the p-value, we make a conclusion about the strength of evidence we have against the null hypothesis by comparing the p-value to the significance level, alpha. If the p-value is less than alpha, we reject the null hypothesis and say we have evidence against the null in favor of the alternative hypothesis. Lets evaluate the evidence for the Mercury Content in Fish and Water Acidity example. 

With a t-value of -5.024 and an alternative hypothesis that is two-sided, we are interested in the probability of seeing a t-value of -5.024 or more extreme if the null hypothesis were really true. The t-value could be more extreme, which means further from zero, in either direction: either further above +5.024 or further below -5.024. Therefore, the probability we are looking for is the area in both tails of the t-distribution, highlighted green in the plot above. To find this, we calculate the area under the t-distribution curve to the right of +5.024 and add to it the area to the left of -5.024, which is the mirror image of the upper portion of the distribution. Alternatively, because the t-distribution is symmetric, we can just calculate the area under the curve to the left of -5.024 and multiply this value by 2. The p-value for this example is 0.00000657 (which we saw in the output from the software).For a significance level (alpha) set to 0.05, we reject the null hypothesis because the p-value is less than 0.05. Therefore we conclude that there is evidence of a non-zero slope between mercury content in fish and water acidity in Florida lakes. 

Since we have rejected the null hypothesis, we could go on to say that the slope appears to be negative from this sample, with a plausible range of values for the slope of -0.213 to -0.091. What are the assumptions when carrying out inference for simple linear regression? The assumptions will be briefly described here while leaving a future lecture to describe them in more depth.

The sample should be a random (or representative) sample from the population of interest, to allow us to generalize the results to that population; 

The data should show a linear trend. If there is a nonlinear trend, a more advanced regression method should be applied;

The observations should be independent of one another. That is, whether one point is above or below the line does not influence whether another point is above or below the line. 

The residuals are nearly Normal in their distribution. 

The variability of the points around the best-fit line is roughly constant. 

If these assumptions are not met, then the SLR results will not be valid. Here are a few additional notes about regression.

Use multiple sources of evidence to get a complete picture of the relationship. Each statistic by itself only provides part of the picture. Consider, for example, the p-value for the slope. It answers the question of whether the slope of the line is different than zero (a horizontal line). We can get a p-value that is really small, but then we may look at the R2  value and realize that the model doesnt fit the data well at all (that is, the R2 value is very small). In most fields, this kind of effect (one with a significant p-value but a very small R2 value) would be considered completely trivial, even though it is statistically significant. If we only focused on the p-value, it might lead us to misunderstand the findings. 

Simple linear regression is a stepping stone to more interesting and complex regression methods, but it is rarely used in practice as the sole statistical method. A famous statistician, George Box, once said All models are wrong, but some are useful. This is a good phrase to consider when doing regression (especially regression beyond SLR). What he meant is that all models are approximations of the real thing. So the question you should ask yourself is not Is the model true? (because it never is exactly true), but Is the model good enough for this particular application? (paraphrased from Luceo and Paniagua-Quiones (2009)). 

We presented confidence intervals for estimating the slope in the population, but there are other types of intervals we can compute in regression. These include prediction intervals for a Y value at a particular X value and prediction intervals for a mean Y value at a particular X value. These can be useful when you wish to use a regression model for prediction (such as predicting a persons future risk of stroke based on their age, gender, blood pressures, and cholesterol levels) and want to estimate the uncertainty in your predictions.

We presented the simplest of all regression techniques, but there are many more types of regression beyond simple linear regression. These include multiple linear regression (which will be introduced in a future lecture), logistic regression, proportional hazards regression, Poisson regression, and nonlinear regression. Many of these will be presented in the second course in this sequence, PubH 6451: Biostatistics II.

<!--chapter:end:13a-InferenceSLR.Rmd-->

A previous lecture described how to use statistical inference methods for simple linear regression. What was missing from that lecture was how we evaluate the assumptions. Lets learn how to check the assumptions of the model via diagnostics to help ensure that we can make valid inferences about the relationship using the simple linear regression model. 

The same example we used in previous lectures, Mercury Content in Fish and Water Acidity, will be used throughout this lecture.Checking assumptions is an important step when doing statistical inference because the estimates (point and interval) and hypothesis testing were developed assuming that the model is correct. If the model is incorrect, then we are at risk for making incorrect conclusions. 

As a review from a previous lecture, the assumptions for simple linear regression are:

The sample should be a random (or representative) sample from the population of interest, to allow us to generalize the results to that population; 

The data should show a linear trend between the X and Y variables. 

The observations should be independent of one another. 

The residuals should be nearly Normal in their distribution. 

The variability of the points around the best-fit line should be roughly constant. 

The remainder of this lecture will discuss the LINE assumptions (which stands for Linearity, Independence, Normality, Equal variance) in more detail.The first of the LINE assumptions is that Y and X have a linear relationship to each other, as shown in the linear regression model equation above. The last three assumptions, though, are all related to the underlying distribution of the errors as specified by the error term in the SLR model above. The simple linear regression model assumes that the random errors are independent values and come from a Normal distribution that has a mean of 0 and some fixed variability, denoted as sigma in the notation. Lets unpack what we mean by this through an illustration (as presented on the plot above).
 
The plot indicates that Y is linearly related to X, as shown by the line [Linearity assumption]. It further shows that for any given value of X, the values of Y will not all lie exactly on the line: they will be spread out symmetrically above and below the line, with more of them close to the line and fewer of them further away from the line. The differences between the Y values and the line are called the errors. These errors are independent of one another [Independence assumption]. At any given X value, the errors are roughly Normally distributed with mean zero, as shown in the plot by the little Normal curves which are centered on the line [Normality assumption]. The error variance (sigma) is constant all across the X axis, as shown by the little Normal curves all having the same width [Equal variance assumption].

Since the assumptions relate to the population errors, many of the approaches to evaluating these assumptions involve examining the sample estimated errors, or residuals, via graphs. *Note: There was a typo in the transcript. While the transcript has been updated for Spring 2021 (bold and underlined text is the updated information), the audio recording still has the error. This will get corrected once we return back to working in the office.

First up, the linearity assumption. We can check this assumption by creating a scatterplot of the two numerical variables and evaluating the form, that is, assessing the trend of the points. Do the points appear to following a straight line or some other curve or pattern?

Lets check this assumption using the four plots on the slide. Which plots appear to meet the assumption of linearity? 

If you said Plot A and Plot C, you would be correct. Plot A has a positive linear trend and Plot C has a negative linear trend. It seems reasonable to use a linear regression model to capture the relationship between X and Y.

Plots B and D display a curved relationship between X and Y. The trend is clearly increasing in Plot D but not in a linear way. And the trend is clearly decreasing in Plot B but not in a linear way. This assumption would not be met if we saw either of these scatterplots, and we would need to consider a different model to better capture the relationship between X and Y.Another way we can assess the linearity assumption is by using a residual plot. This plot graphs the residuals from the model (on the y-axis) against the fitted values (i.e., y-hat values) from the model (on the x-axis). 

To check this assumption via a residual plot, we look for random scatter of the points (the author of this lecture tends to think about it as a big blob). If the points are randomly dispersed around the horizontal axis at 0, a linear regression model is appropriate for the data. If the points appear to trend in some way, a different model may be more appropriate.

Looking at the four plots on the slide, we can see that there is no obvious trend for the residual plots A and C, but there is an increasing curvature for residual plot B and a decreasing curvature for residual plot D. This is the same assessment we had made on the previous slide: the linearity assumption appears to be met for Plots A and C but not for B and D. 

Its helpful to use both the scatterplot and the residual plot to evaluate linearity, because the form might be more pronounced in the residual plot if its hard to detect in a scatterplot.Lets check the linearity assumption for the Mercury Content in Fish and Water Acidity example. The slide displays the scatterplot for the data as well as the residual plot. Using both plots, does it appear that a linear model is appropriate to represent the relationship between average mercury in fish and water acidity? 

The scatterplot shows a general decreasing trend, with no obvious curvature, and the residual plot displays random scatter above and below the horizontal axis, or a big blob. It appears that the linearity assumption is met for this example. The next assumption, independence, relates to the how the data were sampled and collected. The observations should be independent of one another. That is, whether one point is above or below the line does not influence whether another point is above or below the line. This assumption would be violated if the observations were related in some way to other observations: for example, if the participants were matched or paired, or if they were related in some way to each other (e.g., members of the same family). Another violation would occur if observations involved repeated measurements: for example, if the participants were measured multiple times on the same outcome (e.g., blood pressure). If the observations are not independent, then methods for correlated data should be used instead.

In the Mercury Content in Fish and Water Acidity example, it is reasonable to assume that the characteristics of one lake do not affect the characteristics of another lake, so we can assume that the 53 lakes are independent of one another.The normality assumption refers to the distribution of the errors or residuals. We can check this assumption by creating a histogram of the residuals and evaluating the shape of the distribution. Do the residuals appear to be approximately Normal or are they skewed?

Lets check this assumption using the four plots on the slide. Density curves of a Normal distribution overlay the histograms so we can get a better sense, visually, about whether the distributions are approximately Normal. Which plots appear to meet the assumption of normality?

Plots A and D appear to be approximately Normal, while Plots B and C appear to have a right skew to them. If this assumption is not met, then other procedures for carrying out inference for simple linear regression (such as transformation or nonparametric methods) should be used instead. 

Histograms can be useful for identifying a highly asymmetric distribution, but they are not as useful in identifying normality specifically unless the sample size is relatively large. This is primarily due to the subjective nature of deciding the number of bins to have in the histogram. Luckily, there is another type of graph that can help us assess normality of the residuals.The plot that is generally used to evaluate the normality of data is called a QQ plot (also known as a normal probability plot). The idea behind the QQ plot is that if the data follow a Normal distribution, the theoretical percentiles of the Normal distribution and the observed percentiles from the data should be approximately equal. Recall that percentile refers to a value such that p% of the observations lie below that value. For example, in a Normal distribution, only 5% of the observations are below 1.64 standard deviations below the mean. If the percent of the observations in the data that are that far below the mean is much higher (or much lower) than this, then the data may not be normally distributed. For another example, in a Normal distribution, 50% of the observations are below the mean. If the percentage of observations in the data that are below the mean is much different than 50%, then the data may not be normally distributed. Similar statements can be made about all of the other percentiles.

If the data are normally distributed, then a plot of the sample percentiles vs. the theoretical percentiles will be linear. The more closely the points follow a linear pattern, the closer the distribution is to a Normal distribution. A diagonal line is often included in the plot as a visual aid to assess how well the points follow a linear trend. 

Because the data we are interested in for assessing normality is the sample residuals, we create a QQ plot of the residuals. Using the tips for how to evaluate QQ plots, which of the plots on the slide appear to follow a linear trend? Note: Try not to be too picky. Dont focus too much on small departures from an ideal pattern. We are not looking for exact normality: we are looking for large departures from normality. Pay attention only when you see a consistent departure from the expected pattern. 

Based on the QQ plots on the slide, Plots A and D appear to generally follow a linear trend, which suggests that the residuals are approximately normally distributed. Plots B and C deviate quite a bit from the diagonal line at the top right end of the plot, which suggests some skewness to the distribution of the residuals. This is the same assessment we had made on the previous slide: the normality assumption appears to be met for plots A and D but not for B and C. Lets evaluate the normality assumption for the Mercury Content in Fish and Water Acidity example. The slide displays the histogram of the residuals as well as the QQ plot of the residuals. Using both plots, does it appear that the residuals are approximately Normal? 

The histogram shows a slight right-skew of the residuals and there is slight curvature in the QQ plot. While the skew is slight, it is probably not severe enough to claim that the assumption is not met. We can conclude that the residuals are approximately Normal.The final assumption we need to evaluate is the equal variance assumption. We assess this assumption by (again) using a residual plot. In particular, we are looking to see if the residuals have consistent variability above and below the horizontal line at 0 as we move across the x-axis. We can think of this as the residuals forming a horizontal or parallel band around the 0 line. An example of unequal variance would be if a plot had either a fanning or funneling trend as we moved across the x-axis. A fanning trend would be if the residuals were close to 0 for small fitted values and were more spread out for large fitted values. A funneling trend would be the opposite of that; the residuals were spread out for small fitted values but were close to 0 for large fitted values. 

Lets check this assumption using the four plots on the slide. Which plots appear to meet the assumption of equal variance? 

The residual plot for C appears to be the only one that satisfies the equal variance assumption. There doesnt appear to be any trend in the residuals, and they appear to be evenly distributed above and below the horizonal line at 0. 

We already know (from an earlier slide) that plots B and D have curvature trends, which makes it difficult to also assess the equal variance assumption. However, the residual plot for A appears to have some fanning going on (that is, the points appear to be a bit more spread out vertically at the right side of the plot than at the left side). So the equal variance assumption does not appear to be met for Plot A. If this assumption is not met, then other procedures for carrying out inference for simple linear regression (such as transformation of X and/or Y data values or weighted least squares linear regression) should be used instead. 

If we put all of the evaluations of the assumptions together, it appears that none of these cases meets all of the assumptions needed for linear regression. B and D fail the linearity assumption, B and C do not appear to have normally distributed residuals, and A does not appear to have equal variances.How does the variability of the residuals appear for the Mercury Content in Fish and Water Acidity example? 

The residuals roughly form a horizontal band around the 0 line. There is no obvious fanning or funneling so it is reasonable to assume that the equal variance assumption is met for this example. 

Putting it all together, for the Mercury Content in Fish and Water Acidity example, all of the assumptions for linear regression appear to be met, so using a SLR model to determine the relationship between average mercury content in fish and water acidity is appropriate. You might be thinking at this point in the lecture that evaluation of the assumptions using graphical diagnostics seems subjective, and you would be right. With time and experience, you will get better at this kind of evaluation. Keep in mind, though, that linear regression is what is called robust to small deviations from the assumptions, meaning that the results will be valid as long as the data are pretty close to meeting the assumptions. When we check the assumptions, we are not looking for tiny little differences; we are looking for major deviations. Dont focus on any one or two points. Humans love to try to find patterns in randomness when there isnt one. Think of it this way: Its like looking up at the clouds in the sky1. If you look long enough, you start to see images of animals in the random scatter of the clouds. Try to not do this when evaluating the regression diagnostics. Pay attention only when you see a consistent departure from the expected pattern. 

A topic that is not covered in this course is checking for outliers or influential points. Outliers are points that fall far from the cloud or trend of points. They are important to consider because they may have strong influence on the position of the simple linear regression line. There are many measures that have been created to assess which points are potential outliers, or potential influential points, such as Cooks distance and studentized residuals. These will be covered next semester, in PubH 6451. 

A very common misconception is that the linear regression assumptions are about the predictor variable(s). This is a friendly reminder that the assumptions are about the errors (or the residuals) and not the predictor variables themselves. If the X variable is non-normal and we fit a simple linear regression model, that is totally ok. (The X variable does not even need to be continuous: it could be categorical or even binary. We will see this more in multiple linear regression.) Only the residuals need to be approximately Normal.

Lastly, some violations of the model assumptions are more important than others. Regression inferential methods (i.e., hypothesis tests and confidence intervals) are sensitive to departures from independence and to moderate (but not too tiny) departures from equal variance. But they are fairly robust to departures from normality.

Reference: 
Adapted from https://onlinecourses.science.psu.edu/stat501/book/export/html/910.

<!--chapter:end:13b-SLRDiagnostics.Rmd-->

Simple linear regression, which was covered in a previous lecture, allows us to describe and test the linear relationship between a numerical outcome and one numerical predictor. It also allows us to predict a value of the outcome based on a single numerical predictor. However, we know that the world is a complex place and multiple variables could serve as potential predictors. Enter multiple linear regression!

Multiple linear regression (MLR) is an extension of simple linear regression (SLR), and many of the ideas from SLR carry over into MLR. Most often, the goal of multiple linear regression is to understand how combinations of several variables are simultaneously related to the outcome. The underlying idea behind adding predictors to the model is to try to explain the part of Y that has not already been explained by the single numerical predictor. This lecture is to serve as an introduction to multiple linear regression. The second course in this sequence, PubH 6451: Biostatistics II, will provide a much thorough presentation of this topic. 

We are aware that the example used in this lecture, the BetterBirth study on perinatal morbidity and mortality, may be sensitive to some. This particular example was chosen because it is part of an important, and needed, area of research in public health and medicine. We, as biostatisticians, may also have emotional moments when we deal with data on a difficult topic, but we feel called to help people by using our gifts of being analytical and precise and careful in our work to further research that will improve their lives.Recall the conceptual notation that was presented in a previous lecture: 

Response = Trend + Random. 

In simple linear regression, we wrote this as: the value of Y for a particular subject from the population is given by an intercept (beta0) plus a slope (beta1) times the X value for that subject, plus some random normally-distributed error or random noise, denoted by epsilon (?). Remember that the error term is added to describe the random scatter of the points about the true line: it tells us that the Y values for individuals in the population deviate from the regression line by the distance ?. Because the error is random, we made assumptions about the behavior of the randomness.

For multiple linear regression, the model notation is similar, but now there are several predictors. To properly denote this, the regression coefficients and the predictors (Xs) are given subscripts. The first predictor is denoted X1, the second predictor is denoted X2, and so on, up to k predictors. [Note: Order of predictors doesnt matter.] Up until this point, we have only considered numerical predictors. Fortunately, regression models can accommodate both numerical and categorical predictors. Categorical predictors are typically incorporated into the model by recoding the categories as 0 or 1. These kind of variables are called indicator or dummy variables. In this lecture, we will only deal with binary predictors. 

As before, we say that the value of Y for a particular subject in the population is given by an intercept (beta0) plus a coefficient, beta1, times the X1 value for that subject, plus a coefficient, beta2, times the X2 value for that subject, and so forth for all of the predictors (k of them), plus some random normally-distributed error or random noise, denoted by epsilon (?). The error term is interpreted in the same manner as in SLR, that is, as the random scatter of the points about the true line. We again make assumptions about the behavior of the randomness of the error term. Because these population coefficients are unknown to us, we use the coefficients determined from the sample (b0, b1,, bk) to make an inference about the population coefficients (beta0, beta1,, betak). 

Just as in SLR, least-squares regression is used in MLR to find values for all of the coefficients that result in the best fit for the observed data, that is, that minimize the sum of the squared residuals. The math behind the multiple regression model involves solving systems of equations in matrix algebra. In this course, you do not need to worry about how this works, but only about how to interpret the results obtained from statistical software.For simple linear regression, we can easily plot and visualize the (X, Y) data and the regression line in a two-dimensional (2D) scatterplot. It becomes more difficult to plot the data for multiple linear regression, since there are more than two dimensions. 

In the case of two predictors, where one predictor is numerical and the other is binary, we could plot a 2D scatterplot, and denote the value of the categorical variable by changing the color or shape or size of the points (see the plot on the left-hand-side of the slide). In this situation, the regression model would allow us to obtain two separate lines, one for each group. 

Or, in the case of two predictors that are both numerical, we could conceivably plot the data in three dimensions (X1, X2, and Y), and the regression model would be a plane (instead of a line) in three-dimensional space (see the plot on the right-hand-side of the slide). The least-squares estimation approach in this case involves minimizing the distance of the observed data points from the best-fit plane. 

If there are more than two predictors, we would need to plot the data in k + 1 dimensional space, which is beyond human capacity. So we often dont create scatterplots in multiple linear regression, beyond the numerical + binary predictor situation. The example we will use in this lecture is the BetterBirth Study. 

An important research area in public health is improving maternal and child health locally and globally. One approach is to study ways to improve the quality of care provided at health-care facilities. One study investigated using a checklist-based strategy in an area that was largely rural and had one of the highest rates of maternal and newborn mortality in India and globally. 

A matched-pair, cluster-randomized controlled study was carried out to try to reduce severe maternal, fetal, and newborn harm in Uttar Pradesh, India. Sixty primary and community health-care facilities were assigned to the intervention arm, which involved coaching-based implementation of a checklist-based program, and 60 matched control sites were also enrolled at the same time. The results of the study indicated that the intervention did not reduce perinatal mortality, maternal morbidity, or maternal mortality. A post-hoc analysis1 was carried out to further investigate any relationships of the outcomes (perinatal mortality, maternal morbidity, or maternal mortality) with facility-level attributes. [Note: We are going to be examining a very small subset of the predictors that were actually used in the post-hoc analysis, for educational purposes.]

Suppose we were interested in examining the relationship between female literacy, at the district-level, and perinatal mortality, after accounting for study arm. A scatterplot of the relationship, with the points colored by study arm (coaching-based intervention is orange and control is blue), is shown on the slide. What do we notice from the plot? It appears that as female literacy at the district-level increases, the perinatal mortality tends to decrease for facilities. Also, the study arm dots appear to be mixed together in the plot, suggesting that there isnt a difference between the study arms when it comes to the relationship of perinatal mortality and female literacy. 

Lets now fit a model to understand the impact of female literacy at the district-level on perinatal mortality, after accounting for study arm. 

References: 
1Delaney, M. M., Miller, K. A., Bobanski, L., Singh, S., Kumar, V., Karlage, A., Tuller, D.E., Gawande, A.A., & Semrau, K. E. (2019). Unpacking the null: a post-hoc analysis of a cluster-randomised controlled trial of the WHO Safe Childbirth Checklist in Uttar Pradesh, India (BetterBirth).The Lancet Global Health,7(8), e1088-e1096.
Dataset: https://doi.org/10.7910/DVN/GNJBAO The MLR model can be defined as shown in the equation on the slide. For the BetterBirth Study, the outcome Y is the perinatal mortality for a facility (which will be denoted as PM), with predictors X1 for proportion of female literacy (at the district-level) (which will be denoted as FL) and X2 for study arm (X2 = 1 for coaching-based intervention site and 0 for control site). Lets figure out what the parameters of this model tell us. 
?Beta0 is the intercept; that is, it is the average value of Y when all of the Xs equal 0. For the BetterBirth Study, the intercept can be interpreted as the model-predicted average value of perinatal mortality when female literacy is 0 and when study arm is 0 (control group). This is not very useful because a female literacy value of 0 was not observed in our data (and its not a very realistic number). 
?Beta1 is how much higher (or lower) the average of Y is for each one-unit increase in X1, keeping X2 fixed or constant. For the BetterBirth Study, this would be how much higher (or lower) the average of perinatal mortality is for each one-unit increase in female literacy, holding study arm constant (or after accounting for study arm). 

Notice that this is a similar slope interpretation as in SLR, but this time, we have to add an extra part to denote that we are accounting for or adjusting for the other variables in the model. We have to do this with all of the regression coefficients and can do so using a variety of phrases. We could say after adjusting for or after accounting for or holding all other variables constant.
?Beta2 has the same interpretation as beta1; that is, how much higher (or lower) the average of Y is for each one-unit increase in X2, keeping X1 fixed or constant. However, since X2 is a categorical variable coded as 0 or 1, a one-unit increase represents switching from one category to the other. So it would be how much higher (or lower) the average of Y is when comparing X2 = 1 to X2 = 0. For the BetterBirth Study, this would be how much higher (or lower) the average of perinatal mortality is for intervention facilities compared to control facilities, after accounting for female literacy . 

Because there is a binary predictor in the model, we can obtain the regression line for each of the binary categories, one for control arm and one for the intervention arm. We do this by substituting X2 = 1 in the regression equation for the intervention arm and X2 = 0 for the control arm. When we plug X2 = 0 into the equation, we obtain a regression line for the control facilities with an intercept of beta0 and a slope of beta1 times female literacy (see box on the left-hand-side of the slide). When we plug X2 = 1 into the equation, we obtain a regression line for the intervention facilities with an intercept of beta0 plus beta2  and a slope of beta1 times female literacy (see box on the right-hand-side of the slide). Notice that the regression lines for the two arms have different intercepts, but the same slope for female literacy. 

It should be noted that this model assumes that the association of female literacy with perinatal mortality (that is, the slope for female literacy) is the same for both study arms, and that the association of study arms with perinatal mortality (that is, the offset due to study arm) is the same for all levels of female literacy. In other words, this model assumes that there is no interaction between female literacy and study arms. We will come back to this point later in this lecture. The results from fitting the MLR model to our data are shown on the slide. The estimated regression coefficients are in the column labeled Estimate. Lets interpret the values of the regression coefficients. 

For facilities in the same study arm, for every 1-unit increase in female literacy, perinatal mortality is expected to decrease by 0.13. Similarly, for facilities with the same female literacy value, perinatal mortality is expected to be 0.0007 less in the intervention arm compared to the control arm. 

For the BetterBirth Study post-hoc analyses, the interpretation of the study arm coefficient was not of primary importance. The study arm predictor was added to the model to take that variable into account or adjust for it when examining other predictors. Focusing on one or a few predictors in regression models is very common in research, unless you are doing an exploratory analysis. Some predictors are placed in the model so the analysis can properly adjust for the effects of them, while focusing on the relationship between a subset of the predictors and the response variable.A plot of this model is shown above. Now there are two regression lines: the orange line is for the intervention arm and the blue line is for the control arm. 

The two lines have the same slope, -0.1331, which tells us that average perinatal mortality declines by about 0.13 for every additional unit of female literacy. 

The two lines are offset from each other: the line for the intervention arm is 0.0007 below the line for the control arm at any female literacy value, which tells us that for any given female literacy value, an intervention facility will have about 0.0007 lower perinatal mortality than a control facility, on average. Just as with SLR, a question we might have once we obtain the MLR regression results is, How well does the model fit the data? This can again be assessed using the coefficient of determination, but rather than using the multiple R2 value (which is analogous to the R2 value reported in SLR), we use the adjusted R2 value. Why? Well, the multiple R2 value will always increase (get closer to 1.0) as each new predictor is added to the model, even if the new predictor isnt related to the outcome at all. (This is called overfitting or fitting noise.) In contrast, the adjusted R2 value takes the number of predictors into account and only increases if the newly added predictor is expected to make the model fit future data better than the model without it. The adjusted R2 value is always less than the multiple R2 value.

In the BetterBirth Study example, how much of the variability in perinatal mortality is explained by the predictors after adjusting for the number of predictors in the model? The adjusted R2 value is 15.77%. Lets add another predictor to the model to see if we can explain some of that remaining variation. The results from fitting an MLR model with three predictors is shown on the slide. The other predictor that was added to the model was the average years of experience by birth attendants at the facilities (BA Experience). Lets interpret the regression coefficient for this new predictor. 

For facilities in the same study arm and with the same female literacy value, for every 1-year increase in average years of experience of the birth-attendants, average perinatal mortality is expected to decrease by 0.0005. 

We can see that this model fits the data slightly better than when BA Experience wasnt in the model, as the adjusted R2 increased from roughly 16% to 17%, but the increase probably isnt enough to say that it fits MUCH better. 

Just as before, we know that summary statistics will vary from sample to sample, so we are interested in knowing: Could the relationships we observe be the result of sampling variability or is this evidence of real relationships? Lets use statistical inference methods to answer this question. 

For the remainder of the analysis, we will just be focusing on the coefficients for female literacy and BA experience, as those were of primary interest to the researchers. The study arm predictor was placed in the model to properly adjust for the effects of it but was not of primary interest.The hypotheses for multiple linear regression are similar to simple linear regression, except we have one for each regression coefficient. There are k+1 coefficients we could test (one for the intercept and k coefficients, one for each predictor), but, as previously mentioned, we are often really interested in one or a few of them (depending on our research question). The test(s) of interest are most likely about the coefficient(s) for the predictors and not about the intercept. Also, the difference between MLR and SLR is that the individual tests for each coefficient assess the importance/effectiveness of each predictor after the other predictors are in the model. 

Recall that the null hypothesis defines the skeptical perspective or the nothing is going on situation. When we are looking to see if a predictor is associated with the outcome (conditional on the other variables in the model), the skeptical perspective is that they arent associated at all or they are independent of one another (given the other predictors in the model). For multiple linear regression, this would occur if the regression coefficient for that predictor equals 0. 

The alternative hypothesis, on the other hand, defines the competing claim, the thing we are interested in finding evidence for. In this case, the alternative hypothesis is that the predictor and the outcome are indeed associated with or related to each other (given the other predictors in the model) and therefore, are not independent of one another. We would write the alternative hypothesis for multiple linear regression as the regression coefficient for that predictor does not equal 0. We evaluate the claims in the usual way: 
We collect evidence (data), 
We summarize the data using exploratory data analysis (multiple linear regression line, adjusted R2),
We calculate a test statistic as a measure of the compatibility between the result from the data and the null hypothesis, and
We can also calculate a confidence interval that provides a plausible range of values the population parameter can take.

We calculate the test statistic and confidence interval for a regression coefficient in the same way as in SLR. Recall that the general t-statistic formula is the sample estimate minus the null value, all divided by the standard error for that estimate. Written out for multiple linear regression, it is the sample coefficient (betaj) minus 0 (the null value), divided by the standard error of that coefficient. We can compute a test statistic for each coefficient in the model. The equation for the standard error of the regression coefficient is a little ugly (and time-consuming to carry out by hand), so we will rely on software to compute the standard error for us and leave the explanation for a future statistics course. This test statistic has a t-distribution with n  k  1 degrees of freedom. [Note: The df = n k 1 because we are estimating k + 1 parameters (coefficients) in this model: the k coefficients and the intercept.] 

Confidence intervals are an alternative method for summarizing the evidence provided by the data. Recall that the general formula for a confidence interval is the point estimate plus or minus the margin of error. Filling in these details for estimating a population regression coefficient:

--The point estimate is the coefficient estimated from the sample. 

--The margin of error is the degree of confidence, which is calculated from the appropriate t-value (which is NOT the same as the t-test statistic) times the standard error for the coefficient. The appropriate degrees of freedom for the t-value is n  k  1 and the standard error for this situation is the SE for the coefficient (obtained via software). For the BetterBirth Study example, data were collected and summarized using an MLR model and adjusted R2 (shown on a previous slide). Lets answer the question of: Would the individual regression coefficient that we observed be unusual if the true coefficient was really zero in this model?

To answer this question, we compute a t-test statistic for each coefficient using the model-estimated coefficient, its standard error, and the null value. The t-test statistic for female literacy in this model is -0.1275 minus 0, all divided by 0.0270. Solving this out results in a t-test statistic of -4.73 (which we can see in the output from our software). The t-test statistic for BA experience in this model is -0.0005 divided by 0.0003, which equals -1.81. These test statistics each have a t-distribution, where the degrees of freedom are 120  3  1 = 116.

We can also compute a confidence interval for the coefficients in this model using information in the output. The sample coefficient for female literacy in this model is -0.1275 and the standard error for the slope is 0.0270. The t-value for a 95% confidence interval is the value in the t-distribution with 116 degrees of freedom and 0.975 area lying below that value, so the t-value is 1.981. Putting all of those values togetherthe point estimate, the t-value, and the standard errora 95% confidence interval for the true coefficient for female literacy in this model is -0.181 to -0.074. We could carry out a similar calculation for the the coefficient for BA experience. The last step in our inferential framework is to make a decision about how usual or unusual the evidence is compared to the claim about the population parameter. Recall that we do this by quantifying how unusual the evidence is compared to what is assumed to be true by computing a p-value. Once we have the p-value, we make a conclusion about the strength of evidence we have against the null hypothesis by comparing the p-value to the significance level, alpha. If the p-value is less than alpha, we reject the null hypothesis and say we have evidence against the null in favor of the alternative hypothesis. For a significance level (alpha) set to 0.05, we reject the null hypothesis for testing the coefficient for female literacy because the p-value is less than 0.05, but fail to reject the null hypothesis for the coefficient for BA experience because the p-value is greater than 0.05. Therefore we conclude that female literacy is useful in this model to predict perinatal mortality, but BA experience is not an effective predictor in this model. What are the assumptions when carrying out inference for multiple linear regression? They are the same as in simple linear regression, and even use the same diagnostic plots to assess them. As review, they are:

The sample should be a random (or representative) from the population of interest, to allow us to generalize the results to that population; 

The data should show a linear trend. We assess this by looking at a residuals vs. fitted values plot and determining if there is random scatter of the points (or a big blob). If the points are randomly dispersed around the horizontal axis at 0, a linear regression model is appropriate for the data. If the points appear to trend in some way, a different model may be more appropriate.

The observations should be independent of one another. That is, whether one point is above or below the line does not influence whether another point is above or below the line. We assess this through how the data were sampled and collected. This assumption would be violated if the observations were related in some way to other observations: for example, if the participants were matched or paired, or if they were related in some way to each other (e.g., members of the same family). If the observations are not independent, then methods for correlated data should be used instead.

The residuals are nearly Normal in their distribution. We can check this assumption by creating a histogram of the residuals and evaluating the shape of the distribution (Normal? Skewed?). We can also used a QQ plot of the residuals and examine how close the points follow a linear pattern. The more closely the points follow a linear pattern, the closer the distribution is to a Normal distribution. A diagonal line is often included in the plot as a visual aid to assess how well the points follow a linear trend. If this assumption is not met, then other procedures for carrying out inference for multiple linear regression (such as transformation or nonparametric methods) should be used instead. 

The variability of the points around the best-fit line is roughly constant. We assess this assumption by using a residual vs. fitted values plot. We look to see if the residuals have consistent variability above and below the horizontal line at 0 as we move across the x-axis, and there doesnt appear to be any fanning or funneling trend. If this assumption is not met, then other procedures for carrying out inference for multiple linear regression (such as transformation of X and/or Y data values or weighted least squares linear regression) should be used instead. 

If these assumptions are not met, then the MLR results will not be valid. Lets use diagnostic plots and the description of the data to check the assumptions for the BetterBirth Study example. The slide displays the residual plot and the QQ plot. 

Are the health care facilities a random or representative sample of all facilities in that area in India? Based on the brief description of the dataset, the facilities were not a random sample of all facilities in that area. Could they be considered representative of all health care facilities in that area? It seems reasonable to argue that the researchers would likely have chosen the facilities to be representative of all facilities in that area.

Examining the residual plot, the points seem to be a random scatter above and below the horizontal axis, or a big blob, with no obvious fanning or funneling trend. Based on this, it appears that the linearity and equal variance assumptions are met for this example. 

Based on the brief description of the dataset, it is reasonable to assume that the facilities are independent of each other.

Lastly, examining the QQ plot, the residual values appear to generally follow a linear trend, which suggests that the residuals are approximately normally distributed. 

Putting it all together, for the BetterBirth Study example, all of the assumptions for linear regression appear to be met, so using an MLR model to determine the relationship between perinatal mortality and female literacy, BA experience, and study arm, simultaneously, is appropriate. This lecture on multiple linear regression was just a taste of the topic. For a more in-depth understanding of using multiple linear regression, take the second course in this sequence: PubH 6451: Biostatistics II. For those who are only taking this course, here are some notes to be aware of when using MLR.

As previously mentioned, the implication of the model that was presented (the additive model) is that the impact of each predictor on the outcome is the SAME for every value of all other predictors. But, what if the intervention has a bigger impact on perinatal function in districts with higher female literacy than in districts with lower female literacy? If this were the case, there would be an interaction between female literacy and study arm. It is possible to include an interaction term in the model (and the researchers did just that in their article) but this topic is left for the second course in this sequence. 

Multiple linear regression can incorporate any number of predictors, and their interactions, within limits dictated by the sample size of patients available for the analysis. Larger studies have more participants and can therefore test more scientific questions (more model parameters) and will have more power to detect differences. There are many statistical methods for helping us determining what is the best model, and it may not always be the most complicated one. These methods are under the umbrella of model selection. Choosing a final regression model is somewhat of an art that often requires a good deal of experience. This topic will be covered in the second course in this sequence. 

Avoid adding predictors to the model that are highly associated with other predictors, a phenomenon known as multicollinearity. When this is done, the correlated predictors convey essentially the same information, so one of the correlated terms becomes unnecessary and redundant and the model results can often become unstable. For example, if we were trying to predict blood pressure and entered weight and height as predictors, we would have encountered multicollinearity, because people who are taller also tend to be heavier. Once one is known (e.g., height), also knowing the other (e.g., weight) does not contribute that much additional information. This is why we emphasized that the interpretations and significance of the coefficients in MLR are dependent on the other predictors in the model. 

Lastly, we can add categorical predictors to the model that have more than two categories. Essentially, a categorical variable with c categories will be converted to c-1 binary variables. For example, suppose there were three study groups: two interventions (A and B) and a control group (C). If this predictor was added to a model, it would typically be recoded into two binary (or dummy) variables: one comparing A (1) to C (0) and another one comparing B (1) to C (0). This topic will be covered in the second course in this sequence. 

And thus concludes the learning material for PubH 6450: Biostatistics I. Enjoy answering the hard questions that permeate our complex world through the lens of biostatistics/data analysis! 

<!--chapter:end:14-MLR.Rmd-->

In this lecture, we will explore a technique known as bootstrapping, which can be used to obtain confidence intervals for parameters (such as medians) for which the statistic does not have a sampling distribution that is Normally distributed. Bootstrapping uses the sample itself as a stand-in for the population, and repeatedly samples from the sample to create an approximate sampling distribution for the statistic of interest. This approximate sampling distribution can then be used to estimate a confidence interval for the population parameter. In earlier lectures, we explored the behavior of the sampling distributions for sample proportions and for sample means. However, any statistic that we calculate from a sample will have a sampling distribution. For example, there is a sampling distribution for sample medians, and a sampling distribution for sample relative risks. The sampling distribution for a statistic simply describes how that particular statistic (such as the median or the relative risk) varies from sample to sample. As a general rule, the sampling distributions of other statistics besides means and proportions are usually NOT Normal and the Central Limit Theorem does NOT apply to them. Some of the other statistics have known parametric sampling distributions (e.g., sample slope), but others have sampling distributions that arent known.

But we can still create confidence intervals for the population parameter even if we do not know anything about the sampling distribution for the sample statistic. Lets explore how to get a confidence interval for other statistics, focusing on the median as the other statistic in this lecture.
Lets start by thinking about a sample.

We have seen that, if we take a random (representative) sample from a population, the summary statistic from that sample will be a good estimate of the population parameter. If our sample is large enough, the shape and variability of the sample will also be similar to the population. 

Lets look at an example. 

On the left, we see the population distribution for systolic blood pressure (SBP) in US adults*. On the right, we see a dotplot of the SBP from the Blood Pressure dataset, which contains a sample of 500 people. While the sample distribution isnt exactly the same as the population distribution, the shape, center, and variability are similar: we see a few people with SBP below 100 mmHg, a peak around 140 mmHg, and then a more gradual decrease until about 200 mmHg, with a few above 200 mmHg.

Suppose that we didnt have information about the population and we only had information from the 500 individuals in the Blood Pressure dataset. The median SBP in this sample is 140.5 mmHg. This is a point estimate of the population median SBP. If we wanted to better estimate the population median SBP, how can we obtain a confidence interval for this value?

Reference: 
*These SBP measurements were obtained from 5209 people as part of the Framingham Heart Study (http://www.framinghamheartstudy.org). This population may not be exactly like the entire US population in all respects.
Add citation for the data for the sample, if different.
Confidence intervals for a mean or a proportion are based on the assumption that the sampling distribution for the statistic (mean or proportion) is Normally or approximately Normally distributed. For other statistics, such as the median, we cannot make that assumption. If we could only take a very large number of samples repeatedly from the population of interest, calculate the statistic of interest for each sample (e.g., sample median), and obtain the sampling distribution for the statistic, we could understand the sampling variability behavior of the statistics. 

Unfortunately, in most cases we dont have complete data from the population of interest. (If we did, we wouldnt need to do statistical inference.) But we do know that a single sample can be a good approximation of the population under certain circumstances: if it is representative of the population, if the observations are independent and measured accurately, and if the sample size is large enough. 

If our sample is a good approximation of the population, we should be able to take a very large number of samples from our sample (a.k.a. our approximate population) to approximate the sampling distribution!

Bradley Efron introduced what is called the bootstrap in 1979, when he formalized the idea of using repeated sampling from the sample itself to approximate the sampling distribution1. The idea behind the bootstrap is simple: we use the sample to approximate the unknown population, and then sample repeatedly from it to obtain an approximate sampling distribution for our statistic. 

Our bootstrap or approximate sampling distribution will probably not have exactly the same center as the true sampling distribution (or the population), since it will be centered at the sample value. It will also probably not have exactly the same variability as the true sampling distribution, but it will be close, provided the original sample is large enough. Large enough for bootstrap confidence intervals means sample sizes larger than about n=50. For smaller samples, the bootstrap distribution may underestimate the true variability, leading to confidence intervals that are too narrow and miss the true value more often than they should.

References:
1 A very accessible (to non-statisticians) description of the bootstrap can be found in Bradley Efron and Robert Tibshirani (1991) Statistical Data Analysis in the Computer Age, Science 253(5018), pp390-395.

A somewhat more statistical description of the bootstrap can be found in Brad Efron and Rob Tibshirani (1986) Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy, Statistical Science 1(1), pp 54-77.



So how exactly is this done? (Note that this is carried out by computer software, not by hand!)

First the computer takes a sample of size n from the original sample (Note: n is the same sample size as the original sample). This new sample is called a bootstrap sample. We sample with replacement, which means that once we have randomly chosen an observation, we put it back in the dataset so that it could be chosen again. This means that in a given bootstrap sample, some observations in the sample will be represented more than once and others may not be present at all. This will result in the bootstrap samples differing slightly from each other, in ways that reflect the variability in the original sample. (If we sampled without replacement, there would be only one way to get a bootstrap sample of size n from an original sample of size n, which wouldnt give us any information about variability.)

We then calculate the statistic of interest (in our example, the median) for the bootstrap sample.

The computer repeats the process of taking a bootstrap sample and calculating the statistic of interest thousands or tens of thousands of times. (This takes next to no time with modern computing power.) This gives us a "bootstrap distribution or approximate sampling distribution for the statistic of interest. 

There are a number of ways of obtaining a confidence interval from the bootstrap distribution. A common one is to use the percentiles of the distribution directly. Using this approach, a 95% confidence interval for the median would span the middle 95% of the bootstrapped medians, from the 2.5% value to the 97.5% value. 

Note: Bootstrapping is an example of a nonparametric statistical method. Nonparametric methods rely only on the sample to make inference about the population. They do not require that we know the distribution (and parameters for that distribution) of the population or of the true sampling distribution. 
Lets look at an example.

As we said a few slides ago, the median SBP level in our Blood Pressure dataset is 140.5 mmHg. This is a point estimate of the median SBP level in the population of all US adults. How can we determine how precise our estimate is? How can we obtain an interval estimate, a confidence interval, for the true population median SBP level?

Using bootstrapping, we sample with replacement repeatedly from our one original sample of size n=500. Three of those bootstrap samples are presented at the top of the slide. As you can see, each of the bootstrap samples have a slightly different distribution and a different median. We are mimicking the process of repeated sampling from the stand-in or approximate population (a.k.a. our sample) to build up the sampling distribution of the median. 

If we obtain 10,000 bootstrap samples of size n= 500 and obtain the median for each sample, we end up with the plot shown at the bottom of the slide. This bootstrap distribution of the sample median has a center (mean) at 140.3 and standard error of 1.35.To find a 95% confidence interval, we have the computer put the 10,000 sample medians in order from smallest to largest, and find the lower and upper values of the middle 95% of the median SBP values. This corresponds to the 2.5th percentile, or 251 values above the minimum value, and the 97.5th percentile, which is 251 values below the maximum value. We obtain a 95% confidence interval for the population median SBP of 138 to 142 mmHg. We interpret this confidence interval the same way as we did for estimating the population proportion and the population mean. 

While the bootstrap technique was designed to obtain confidence intervals when the form of the sampling distribution of the sample statistic is unknown, the bootstrap can be used to find confidence intervals for any parameter, including means and proportions. Lets see what happens if we calculate a bootstrap confidence interval for the mean SBP in our example. The sample mean from the Blood Pressure example is 144.95 mmHg. 

When we calculate the mean instead of the median for each of our 10,000 bootstrap samples, we obtain the bootstrap distribution for the sample mean shown here. This time, the bootstrap distribution looks quite Normally distributed. This is what we would expect, since we know from the Central Limit Theorem that the sampling distribution for a sample mean is Normal. The bootstrap distribution mirrors the sampling distribution in shape and spread, but is centered on the sample mean instead of the population mean.

If we order the 10,000 sample means and find the 2.5th percentile and the 97.5th percentile, we obtain a bootstrapped 95% confidence interval for the population mean SBP of 142.56 to 147.44 mmHg. 

How does this result compare with a 95% confidence interval calculated the usual way, using the Central Limit Theorem? The sample size is 500, the sample mean is 144.95 mmHg, and the sample standard deviation is 27.99 mmHg. Our degrees of freedom are 499, which results in a t-value of 1.965. Putting these values into the formula, a 95% CI is xbar +/- t*SE = 144.95 +/- (1.965)*(27.99/sqrt(500)) = 144.95 +/- 2.46 . We end up with a 95% confidence interval that ranges from 142.49 to 147.41 mmHg, which is nearly identical to the bootstrap CI above.

We see that when we use the bootstrap method to calculate a confidence interval for the mean, the resulting interval is almost identical to the interval based on the Central Limit Theorem, which is reassuring!

<!--chapter:end:4c-BootstrappingCIs.Rmd-->

Hypothesis testing provides a standardized decision-making process for scientific research. It can be used in many situations: to compare one groups result to a standard, to compare two groups to each other, or to compare among many groups. Hypothesis testing is used to compare means, to compare proportions, to compare odds ratios, to compare hazard ratios, and so on. 

In this lecture, we will introduce the logic behind hypothesis testing. In the next lecture, we will formally present a specific hypothesis test, the one sample t-test, which is used to compare one sample mean to a standard.
To review, statistical inference is the process of concluding something about the value of an unknown population parameter (such as the true mean or true proportion) based on information from a known sample (such as the sample mean or sample proportion). 

There are two approaches to doing statistical inference. One approach is through estimating the unknown population parameter via confidence intervals using the known result from our sample (which we learned in an earlier lecture). The other approach is through hypothesis testing. So how is hypothesis testing different from using confidence intervals? 

In hypothesis testing, we first assume some claim about the population parameter and then use the result from our sample to gain insight into that statement. When we use confidence intervals, on the other hand, we make no assumption about what the population parameter is. The purpose of constructing confidence intervals is to estimate the population value from the result of our sample, whereas, the purpose of hypothesis testing is to determine whether there is enough evidence (from the result from our sample) to refute the assumed claim about the population value.

You might be thinking, it seems odd to start with an assumed claim about the population value. Why would we start with that?  To understand this, we have to understand the logic of hypothesis testing. 
The logic of hypothesis testing involves three big pieces: 
Hypotheses, which are statements or claims about the population parameter;
Evidence, which involves summarizing sample data to provide evidence about the claims; and 
Evaluation, which involves making decisions about how usual or unusual the evidence is compared to the claim about the population parameter. 

A commonly used analogy for the logic of hypothesis testing is the US criminal justice system. Lets use this analogy to better understand the pieces in the framework. A key principle of the US criminal justice system is A defendant is presumed innocent until proven guilty. In this statement, there are two competing hypotheses about the truth: 
The defendant is innocent. 
The defendant is guilty. 

The first hypothesis (in this case, innocence) is assumed to be true. This assumed claim about the truth is also known as the null hypothesis, denoted by H-sub-zero (H0), or H-naught. It is the claim that is to be tested. In general, the null hypothesis describes the default situation (the status quo or the nothing interesting is happening situation). It is the thing that we are going to initially assume is true about the population(s). When we are comparing one population to a standard, the null hypothesis is that the value of the parameter of interest in the population is equal to some hypothesized value. When we are comparing two populations (for example, a treatment group and a control group), the null hypothesis is that there isnt any difference: the parameter has the same value in both populations. (Null means amounting to nothing, none, absent, insignificant.)

In contrast, the second hypothesis (in this case, guilt) is an alternative claim, a claim for which we seek significant evidence. The alternative hypothesis describes what we hope to demonstrate about the population(s) using the data. It is closely aligned with the research question. When we are comparing one population to a standard, the alternative hypothesis is that the value of the parameter of interest in the population is different than the hypothesized value. When we are comparing two populations (for example, a treatment group and a control group), the alternative hypothesis is that there IS a difference: the parameter does NOT have the same value in both populations. 

These two statements should be made prior to examining any evidence. It would be considered cheating if we used our evidence to determine the hypotheses. We need to start with some assumed truth in order to compare the evidence to the claim. Using knowledge of research design and detective skills, evidence is collected and evaluated against the claims (hypotheses) that have been made. In the US justice system, this might involve interviewing witnesses, gathering physical evidence, analyzing forensic evidence, etc. 

In statistics, our evidence is our sample of data, which we may have obtained through a survey, or through a clinical trial, or through some other kind of study. The data are summarized to provide a clearer picture of what evidence we have on hand. 

References:
Image: https://www.kisspng.com/png-magnifying-glass-fingerprint-clip-art-kisspng-1155964/The last piece of the framework is to evaluate how usual or unusual the evidence is compared to the claim about the truth. From this evaluation, one of two situations could occur: 
We find that the evidence is unusual (is not likely to occur) compared to what is assumed to be true.
We find that the evidence is usual (is likely to occur) compared to what is assumed to be true. 

In the US justice system, if the evidence is not consistent with (or is unusual compared to) the assumed claim of innocence, then we conclude there is evidence against the claim that the defendant is innocent (i.e., against the null hypothesis) and evidence for the claim that the defendant is guilty (i.e.,  in support of the alternative hypothesis). On the other hand, if the evidence is consistent with (or not unusual compared to) the assumed claim of innocence, then what we conclude is that there is no evidence for the claim that the defendant is guilty. 

CAUTION: We need to be careful with our wording with hypothesis testing. If the evidence is consistent with (or not unusual compared to) the assumed claim of innocence, then we do NOT conclude that there is evidence for the claim that the defendant is innocent (i.e., we do NOT accept the null hypothesis). Lack of evidence against the null hypothesis is not evidence for the null hypothesis. For example, a defendant may indeed have exceeded the speed limit, but if there were no witnesses and no speed measurements, then there wont be any evidence of it. Lack of evidence doesnt necessarily mean the defendant is innocent: it just means there isnt any evidence of guilt. This is why the US justice system verdict is either guilty (if there is sufficient evidence against innocence) or not guilty (if there isnt enough evidence against innocence); the verdict is never innocent. 

Evidence is used in a similar manner in statistics. We evaluate evidence by calculating the probability of observing the result we obtained given the assumption that the null hypothesis is true. That is, IF we assume the null hypothesis is true, how likely or unlikely would it be to see the result we obtained from our data just by random chance? We compare this value against some threshold to make a conclusion about the population. If the result we obtained from our data is NOT likely to occur if the null hypothesis statement is true, then something else must be going on. In this case, we say we have evidence against the null hypothesis and for the alternative hypothesis. Another common phrase for this is we reject the null hypothesis. On the other hand, if the result we obtained from our data IS likely to occur if the null hypothesis statement is true, then we say we lack evidence against the null hypothesis or for the alternative hypothesis, or we do not reject the null hypothesis.

These three pieces are the framework for all hypothesis testing and will be used throughout this course. 

References:
Image: https://www.pixcove.com/scales-balances-fairness-weighing-considering-justice-tilted-slanted-symbol-sign/

<!--chapter:end:5a-IntroHypTesting.Rmd-->

In this lecture, we will formally present how to carry out a hypothesis test, by comparing a single mean to a standard value. 
The context that will be used to formally learn about hypothesis testing is Mercury Content in Fish. 

High levels of mercury are harmful to ecosystems and humans because it is known to be highly toxic. As a result, many groups (including state health departments) monitor mercury levels in lakes. In particular, they monitor mercury content in fish, as high mercury levels can be a health hazard to humans who consume this food item (especially to women who are pregnant, nursing mothers, and young children). 

A study was carried out on 53 lakes in Florida to understand the mercury concentration in Florida sport fish. Samples of fish (largemouth bass) were collected from each of 53 lakes in Florida and the average mercury level in the fish for each lake was recorded (in parts per million; ppm). For simplicitys sake, mercury level will be used instead of average mercury level for the remainder of this lecture, but remember that the mercury level in a lake is an aggregate measure from ALL of the fish sampled from that lake, and not for ONE fish. Summary statistics and plots of this data are presented above.

Recall that when we examine a distribution, we focus on the shape, center, and spread. Multiple summary measures and displays are presented here to give as complete a picture about the data as a researcher might need (and to review previous topics). The mean mercury level in the 53 lakes was 0.53 ppm, and the average deviation away from the mean (the standard deviation, SD) was 0.34 ppm. The data appears to have a right, or positive, skew. 

References:
Study: Lange, T. R., Royals, H. E., & Connor, L. L. (1994). Mercury accumulation in largemouth bass (Micropterus salmoides) in a Florida lake. Archives of Environmental Contamination and Toxicology, 27(4), 466-471.
Dataset: Lock, R.H., Lock, P.F., Lock Morgan, K., Lock, E.F., & Lock, D.F. (2013). Statistics: Unlocking the power of data (1st ed.). Hoboken, NJ: John Wiley & Sons, Inc. 
Guidelines for safe limits of mercury content in fish are set by the Food and Drug Administration (FDA) or its equivalent. The FDA in the US has determined that a safe limit for mercury content in fish is 1.0 ppm (whereas, the limit set by the Canadian FDA is 0.5 ppm). 

Because the data were collected in the US, the researchers were interested in determining whether the average mercury level in largemouth bass in Florida lakes is acceptable by the US FDA standard. That is, is the average mercury level in largemouth bass in Florida lakes less than 1.0 ppm?

We saw that the study sample had a mean mercury level of 0.53 ppm, which is less than 1.0 ppm, but this may be due to sampling variability. The key question is: Is it a lot lower than what we would expect, taking sampling variability into account, if the population from which this sample was drawn truly had a mean mercury level of 1.0 ppm? We will use hypothesis testing to help us answer this question. Before we can carry out a hypothesis test, we need to first evaluate the assumptions about the test. These assumptions are the same as those weve seen before, in calculating a confidence interval for a mean. That is: 

The sample should be a random (or representative) sample from the population, to allow us to generalize the results from this sample to the population it came from;

The observations should be independent of one another (otherwise, we would use a different statistical inferential method that took dependent or correlated data into account); and

The sampling distribution of sample means should be approximately Normal. How can we check this assumption? We can check to see if the conditions for the Central Limit Theorem (CLT) hold. Recall that if the underlying population distribution is approximately Normal (which we can check by plotting the sample distribution), then the sampling distribution of sample means is approximately Normal. OR, even if the underlying population distribution is not Normal, if the sample size is large enough (and what is large enough depends on how heavily skewed the population distribution is), then the sampling distribution will still be approximately Normal. If this assumption is not met, use other methods for carrying out a hypothesis test for a mean (such as re-randomization tests).

If these assumptions are not met, then the results of a hypothesis test will not be valid. Lets check the assumptions for the Mercury Content in Fish example. 

Are the lakes a random or representative sample of all Florida lakes? Based on the brief description of the dataset, it doesnt seem that the lakes were randomly selected. Are they nevertheless representative of all lakes in Florida? It seems reasonable to argue that the study authors would likely have chosen the 53 lakes to be representative of all lakes in Florida. 

Are the observations independent of one another? Again, without more information, it is reasonable to assume that the mercury content in a given lake is not affected by the mercury level in the other lakes, so the 53 lakes will be independent of one another.

Lastly, are the conditions met for the sampling distribution of sample means to be approximately Normally distributed? Recall that the sample distribution of mercury levels in fish in the 53 lakes appeared to be skewed to the right. However, the sample size is 53, which is large enough. Thus, we can reasonably assume that the sampling distribution of sample means is approximately Normal. 

References:
Image: https://www.pinclipart.com/pindetail/owRJbh_file-mw-icon-checkmark-svg-creative-commons-check/Now that we have examined the data and checked the assumptions for the test, we can move on to formally defining the hypotheses: the null and the alternative. We write them in terms of a population parameter of interestthe population mean, , in this case.

As mentioned in a previous lecture, the null hypothesis defines the skeptical perspective or the no difference situation. It is written as: the population mean, , is equal to some specified value, 0 (mu-naught), the null value. Where does this null value come from? It is typically defined by the research question. 

The alternative hypothesis, on the other hand, is the competing claim. It is typically the statement we are interested in demonstrating. 

There are three different forms the alternative hypothesis can take. It can be written as either:
 is not equal to 0 (mu-naught), or 
 is greater than 0 (mu-naught), or 
 is less than 0 (mu-naught). 

The last two statements, greater than and less than, result in one-tailed tests, because we are only interested in differences in one particular direction. In contrast, the not equal to hypothesis results in a two-tailed test, because we are interested in differences in either direction. The majority of the tests in research articles are two-tailed tests. 

How do we know which alternative hypothesis to choose for our hypothesis test? Again, it is defined by the research question. 
In the Mercury Content in Fish example, recall that the research question is Is the mean mercury level in largemouth bass in Florida lakes less than 1.0 ppm? Lets use information from this research question to define the null value and the direction of the alternative hypothesis.

We can infer from the wording of the research question that we are interested in comparing a mean to a standard value, and that the standard or null value of interest is 1.0 ppm. Therefore, the null hypothesis for this example is that the true mean mercury level in largemouth bass in all Florida lakes is equal to 1.0 ppm. Or, using notation, mu = 1.0 ppm. 

Because the researchers were interested in determining whether there is evidence for less than 1.0 ppm, the alternative hypothesis is that the true mean mercury level in largemouth bass in all Florida lakes is less than 1.0 ppm. Or, using notation, mu < 1.0 ppm. To evaluate the claims in our hypotheses, we need to gather evidence (data). We first summarize the data using exploratory data analysis (summary statistics, tables, graphs) and then calculate a test statistic to measure the compatibility between the result from the data and the null hypothesis. In general, this test statistic standardizes the result from the study to a known statistical distribution. 

When carrying out a test for a single mean (given that the assumptions are met), the appropriate test statistic is the t-statistic, which has a t-distribution with n  1 degrees of freedom. It is found by calculating the difference between the sample mean and the null value and dividing by the standard error (which is the sample standard deviation, s, divided by the square root of the sample size, n). We can think of this value as measuring how far the result we found from our sample data is from what we would expect (given sampling variability) IF the null hypothesis was really true. It is expressed in units of standard error: for example, a t-statistic of +2.0 means that the observed sample mean is 2 standard errors above the null hypothesized population mean. Large test statistic values represent large (relative) differences between the sample result and the null value and small test statistic values represent small (relative) differences between the sample result and the null value. Because we are using the t-distribution to carry out this hypothesis test, it is often referred to to as a t-test (specifically, it is a one-sample t-test when a single mean is being tested). 

Does this t formula look similar to something we have seen before? Remember the formula Z = (value  mean)/SD? The two distributions, the t- and z- (or Standard Normal) distributions are very similar. When we take a value, subtract a mean from that value, and divide the whole thing by some standard deviation measure, we are standardizing the original value. That is, we are measuring how far a value is from the mean, in SD units (for z-scores) or SE units (for t-statistics). So why the t-distribution? 

In a previous lecture about confidence intervals for a mean, the t-distribution was used because the sample standard deviation, s, was used in the equation instead of the population standard deviation, ?? (sigma). And using s as an estimate of sigma adds extra uncertainty because it is a statistic and will vary from sample to sample. Recall that the distribution that accounts for this extra uncertainty is the t-distribution. 

A similar argument is made when carrying out hypothesis testing for a mean. When we are trying to carry out inference for a single mean (using either confidence intervals or hypothesis testing), the t-distribution with n  1 degrees of freedom is the appropriate statistical distribution.
Returning to the Mercury Content in Fish example, lets walk through all of the aspects within the evidence piece. 

Data were collected and we explored the data via summary statistics and various plots. We observed that the mean mercury level in largemouth bass in our sample of n=53 lakes was 0.53 ppm. Then we wanted to test whether this value is unusual if we assume that the true mean mercury level in Florida lakes is 1.0 ppm (the claim we are testing against).

To answer this question, we compute a t-test statistic using the data and the null value. The t-test statistic for this example is 0.53 minus 1, all divided by 0.34 divided by the square root of 53, which gives -10.06. This test statistic has a t-distribution with n-1 = 52 degrees of freedom. This test statistic tells us that the mean mercury level in our sample of lakes (0.53 ppm) is about 10 standard errors lower than the hypothesized null value of 1.0 ppm. When we plot the t-distribution with 52 degrees of freedom to see where our test statistic value falls, we see that a t-value of -10.06 is very far out in the left tail of the distribution. It appears that this sample result is very unlikely, or very unusual, if we assume that 1.0 ppm is the true population mean mercury level in fish in all Florida lakes. But, how unlikely is it? Can we quantify this value?To quantify how unusual the evidence is compared to what is assumed to be true, we calculate a p-value. The p-value is the probability that you would obtain a sample result this unusual if the null hypothesis were really true and any observed difference was simply due to sampling variability. To put it another way, its the probability of getting our sample result (or one even more extreme) if the null hypothesis were true. As with any probability, the value can take on numbers between 0.0 and 1.0. A sample result would be deemed unusual or unlikely if the probability of it occurring (under the assumption that the null hypothesis was really true) is small (that is, the p-value is small) and a sample result would be deemed likely if the probability is large (that is, the p-value is large). The smaller the p-value, the less consistent or compatible the data are with the null hypothesis. 

But, how do we figure out what would be considered small? By using something called the significance level, which is denoted by the Greek letter, alpha. The significance level, or level of unusualness, is typically set at a probability of alpha = 0.05 or 5%. So if something has a chance of occurring 5% of the time or less, then we would consider it to be unusual. The significance level for a study is chosen by the researcher at the beginning of the study, before collecting any data. Note that there is nothing magical about the typical significance level of 0.05. We could just as well set it at alpha = 0.01, or even alpha = 0.0001. So how do we decide what alpha should be? Some research areas have standard accepted values of alpha in their field, but most studies will have a significance level of 0.05.

Putting together the evaluation pieces, the last piece of the framework is to make a conclusion about the strength of evidence we have against the claim by comparing the p-value to the significance level, alpha.Lets visualize how to find the p-value for a test statistic using its distribution. Recall that for continuous population distributions (such as the Normal or t-distribution), the area under the curve tells us the probability that a value will occur within that specific range of values. We determine the appropriate area under the curve by the test statistic (and its distribution) and the alternative hypothesis. 

For a t-test, if the alternative hypothesis is that  (mu) is less than the null value (a one-sided test), then we focus our interest in the lower (left) tail beyond our test statistic. So we locate the test statistic on the t-distribution and calculate the area under the curve to the left of that test statistic. This provides the probability of seeing the result we saw in our study (or less) if the null were really true. (Keep in mind that the test statistic might occasionally happen to be larger than the null value but we still want the area to the left of our test statistic.)

If the alternative hypothesis is that  (mu) is not equal to the null value (a two-sided test), then we are interested in both tails beyond our test statistic. A common way to calculate this is to take the absolute value of our test statistic, calculate the area under the curve to the right of that test statistic, and then multiple this value by two. This provides the probability of seeing the result we observed (or more extreme, in either direction) if the null were really true. 

Lastly, if the alternative hypothesis is that  (mu) is greater than the null value (again, a one-sided test), then we are interested in the upper tail beyond our test statistic. So a similar procedure is carried out. We calculate the area under the curve to the right of the test statistic, and this is the probability of seeing the result we saw (or more) if the null were really true. (Again, keep in mind that the test statistic might occasionally happen to be smaller than the null value but we still want the area to the right of our test statistic.)

Note that we will always use software to calculate p-values from our test statistic and the appropriate t-distribution. Once the p-value is found, then we can make a formal decision about the test. 

If the sample result is unlikely to occur (taking sampling variability into account) if the null hypothesis is true, then the p-value will be less than alpha and we say we reject the null hypothesis. That is, we have evidence against the null hypothesis and in favor of the alternative hypothesis. The sample data provide statistically significant evidence in support of the alternative hypothesis. 

If the sample result is likely to occur (taking sampling variability into account) if the null hypothesis is true, then the p-value will be greater than alpha and we say we do not reject the null hypothesis. That is, we lack evidence against the null. We do not have sufficient evidence to discard the null hypothesis. 

In either case, remember to always state the conclusion in the context of the problem and not just conclude reject the null or result is statistically significant.

By the way, note that it is a quirk of statistical practice to never accept the null hypothesis, but only fail to reject it. This is because lacking strong evidence against the null hypothesis is not the same as having strong evidence for the null hypothesis. In the trial by jury example, the jury does not find the defendant innocent but only not guilty. As the colloquial saying goes, Absence of evidence is not evidence of absence. Lets evaluate the evidence for the Mercury Content in Fish example. 

Recall that the test statistic for this example was -10.06. We noticed that it was in the extreme left tail of the t-distribution. So what is the probability of seeing a t-value of -10.06 (or less, because of the alternative hypothesis) if the null hypothesis were really true? To find this, we calculate the area under the t-distribution curve to the left of -10.06. This value turns out to be 4 x 10^-14 (in scientific notation), or really, really, really unlikely. Whenever you find a p-value that is really, really small, its best to report the p-value as p < 0.001. 

Assuming we set the significance level, alpha, to 0.05, our p-value of <0.001 means that we will reject the null hypothesis and conclude that there is evidence that the mean mercury level in largemouth bass in all Florida lakes is less than the guideline mercury level of 1.0 ppm. This is good news: the evidence suggests that the fish in Florida lakes are safe to eat.
Testing for a single mean against a standard or skeptical claim is not as common in practice as comparing two groups or comparing two measurements on one group (paired data). Comparing two groups occurs when we have two distinct groups and the observations in each group are independent of one another (to be discussed in more detail in a future lecture). In contrast, paired data arises when we have two measurements of something on all members of a single group and those measurements are dependent on one another. Examples of paired data include:
 
Before and after (or pre and post) measurements are taken on each participant in a study, 
A new treatment is tested on one side of the body and the placebo is tested on the opposite side of the body of the same participant, 
Study participants are matched based on demographic variables (such as age and gender) and one of each pair is assigned to the treatment group and the other is assigned to the control group, and
Study participants are twins or siblings recruited as pairs. 

In all of these cases, the two measurements are likely to be related to one another, or dependent on one another, and are not independent. (For example, twins are likely to be more similar to each other than to an unrelated person.) When the variable of interest (the outcome variable) is a continuous variable, we can carry out a paired t-test. However, a paired t-test is essentially a one-sample test on a variable that is a paired difference. Lets look at an example to understand how this is the case.A study on patients with cystic fibrosis looked to see if patients pulmonary function (measured as FEV1 (% of predicted)) improved while on a new treatment during the course of their hospital stay. The researchers measured the FEV1 for 18 patients at admission (Pre FEV1), provided all of them the new treatment, and measured the FEV1 again on those same 18 patients at discharge (Post FEV1). They were really interested in the mean difference or mean change in FEV1 values from admission to discharge. So they calculated the difference (Post FEV1  Pre FEV1) for each patient (so that positive differences indicate improved lung function) and then calculated summary statistics (sample mean and sample standard deviation) for that difference or change variable. They found that the mean change in FEV1 was 12.2 (% of predicted) and the standard deviation of the change in FEV1 was 9.1 (% of predicted).

References: 
Data: Pezzulo, A. A., Stoltz, D. A., Hornick, D. B., & Durairaj, L. (2012). Inhaled hypertonic saline in adults hospitalised for exacerbation of cystic fibrosis lung disease: a retrospective study. BMJ open, 2(2), e000407.Because the researchers were interested in whether there was a mean change in FEV1 from admission to discharge, the null hypothesis for this example is  (mu) is equal to 0, or the true mean change in FEV1 is equal to 0 (because remember the null hypothesis is always stated as no effect or nothing going on). The alternative hypothesis in this case would be that the true mean change in FEV1 is not equal to 0, because the researchers are interested in any change (positive or negative) and didnt specify a direction. 

Carrying out the evidence and evaluation pieces of the hypothesis testing framework, it turns out that the mean change of 12.2 is 5.7 standard errors above the hypothesized mean of 0. Using a t-distribution with 17 degrees of freedom, the probability of seeing a result like this (or more extreme) if there really was no change is p = 1 x 10^-5 or p<0.001. Because the p-value is less than the significance level of 0.05, we reject the null hypothesis and we can say there is evidence that the mean change is different from 0. It appears, based on the data, that the mean change is greater than 0, meaning the new treatment appears to improve lung function in cystic fibrosis patients. 

To close out the formal lecture on hypothesis testing, here are few words of advice: 

Use two-tailed tests in most situations, rather than one-tailed tests. Why? As previously mentioned, to define a one-tailed test, a researcher must predict which direction the data will go prior to collecting the data, when planning a study. A one-tailed test can sometimes be useful because it gives a more focused hypothesis and reduces the necessary sample size. However, if the data end up going the opposite direction than expected, then one would end up with a very large p-value and not be able to reject the null hypothesis, even if the difference was very large. For this reason, we encourage using two-tailed tests in nearly all cases, except in the rare case when the researchers truly have no interest at all in one of the directions.
Report the actual p-value. Avoid reporting p-values as an inequality (e.g., p < 0.05), unless the value is really, really small. In that case, report the p-value as < 0.001. 
Also, dont report p-values to more than 3 decimal places. Being that precise in our estimate does not add any relevant information to the reader.
There are no sharp distinctions between p-value increments. For example, a p-value of 0.06 provides about the same degree of evidence against the null hypothesis as a p-value of 0.05. One should use multiple sources of evidence to make decisions and not just solely rely on the p-value to make a statistical (and practical) conclusion. The p-value does not indicate the magnitude, direction or clinical importance of an observed result. It merely estimates the role of just by chance as an explanation for the observed result in comparison to the hypothesized value. 
Remember that we dont conclude we accept the null when we obtain a large p-value. A large p-value just means that our sample result is consistent (given sampling variability) with the null hypothesized value. To provide a better picture of other plausible values that the true population value can take, we recommend you also supply a confidence interval. The null value would then be one of many plausible values that the truth can take (based on our sample result as the best guess). 

<!--chapter:end:5b-HypTestingMean.Rmd-->

Hypothesis testing is a very useful tool for making a decision about the population based on the data from a sample. However, hypothesis tests are not flawless: they do not always lead us to the correct conclusion 100% of the time. For example, using the US criminal justice system analogy, it sometimes happens that an innocent person is found guilty or that a guilty person walks free. (Perhaps youve seen news articles or movies about cases like these.) In statistics, we can quantify how often we are likely to make these types of errors. We can even quantify how often we are likely to make the correct conclusion about a real effect (this is called statistical power). This lecture will present the concepts of errors and power in hypothesis testing. When we do hypothesis testing, there are two things that could really be true: 
The null hypothesis could really be true: there is no difference or effect. (This is shown in the left column in the table.) 
The null hypothesis could really be false: there is a difference or effect. (Right column.)
Keep in mind, though, that we never know what the truth is. (If we did, we wouldnt need to do a hypothesis test!)

There are also two decisions that we might make based on our data:
1) We could decide that we lack evidence against the null (i.e., we fail to reject the null hypothesis (H0) because the p-value > ??). In other words, we decide that there is no difference or effect. (This is shown in the top row in the table.)
2)  We could decide that we have evidence against the null and in support of the alternative hypothesis (i.e., we reject H0 because the p-value < ??).  In other words, we decide that there is a difference or an effect. (Bottom row in table.)

Since we dont know the truth, our decisions might be wrong. There are two ways for our decisions to be right and two ways (called errors) for our decisions to be wrong. 

The two ways for our decisions to be right are highlighted in yellow in the table:
1) We failed to reject the null hypothesis when it was in fact true. (This is the upper left yellow box.) In other words, we concluded correctly that there was no difference or effect.
2) We rejected the null hypothesis when it was in fact false. (This is the lower right yellow box.) In other words, we concluded correctly that there was a difference or effect.

The two ways for our decisions to be wrong are:
1) We rejected the null hypothesis when it was in fact true. (Lower left box.) In other words, we concluded INCORRECTLY that there was a difference or effect. This is called a Type I (Type One) error or a false positive result.
2) We failed to reject the null hypothesis when it was in fact false. (Upper right box.) In other words, we concluded FALSELY that there was no difference or effect. This is called a Type II (Type Two) error or a false negative result. This type of error can be thought of as missing a real difference or failing to see a real effect.Since we do not know the truth, we can never know if we made an error or not in a given study. However, hypothesis testing is designed so that we can set how likely it is that we will make an error over the long term, over many studies: that is, we can set what the long-term probability of making an error is.
 
The probability of making a Type I error is defined as the probability that we reject the null hypothesis when it is in fact true. (In the statistical notation given on the slide, Pr means the probability of, and the vertical line between reject H0 and H0 true is read as given that.) The probability of making a Type I error is called alpha (?). This is the same alpha that you saw in a previous lecture: the significance level for a hypothesis test. The significance level, alpha, is set by the researcher prior to the study. Typically alpha is set to 0.05 or 5%. Setting alpha at 5% means that, over the long term and many studies, we will have a 5% chance of making a Type I error. In other words, we will have a 5% chance over the long term of saying there is an effect when there really isnt.

The probability of making a Type II error is defined as the probability that we fail to reject the null hypothesis when it is in fact false. The probability of making a Type II error is called beta (?). As we will see shortly, the value of beta is related to the statistical power of the study, and is set by the researcher prior to the study. Typically beta is set to 10% or 20% (corresponding to power of 90% or 80%, respectively).  Setting beta to 10% means that, over the long term and many studies, we will have a 10% chance of making a Type II error. In other words, we will have a 10% chance over the long term of missing a real effect.

Notice that the typical value for alpha (5%) is smaller than the typical values for beta (10% or 20%). This is because in a typical health-related study, a Type I error is seen as less tolerable than a Type II error. In a typical study, it is seen as worse to say there is an effect (of a new treatment, for example) when there isnt, than to miss an effect when it is there, so alpha is set small and beta is allowed to be higher. There is a direct trade-off between Type I and Type II errors, as we will see shortly: we can only increase the one by decreasing the other. The optimal trade-off between alpha and beta will depend on the field and the goals of the study. In some situations, it might actually be preferable to make a lot of false positive decisions (high alpha) in order to avoid making any false negative decisions (very low beta).
One of the values that we typically want to be as large as possible is called statistical power. 

Power is the probability of correctly rejecting the null hypothesis, or the probability of rejecting the null hypothesis given that it is in fact false. Its the ability to see a difference or an effect when there really is one. (This is the yellow box in the table above.) Power is defined as one minus the Type II error rate, beta. By setting the Type II error rate, beta, for our study at a small value, we can make the power (1 minus beta) of our study large. 

Research studies in medicine and public health have in the past typically been designed to have a power of 80%, but it is increasingly common these days to design for 90% power. A study with 90% power will have a 10% Type II error rate. This means that if the same study were repeated over and over, and a hypothesis test was carried out each time, about 90% of the tests would correctly reject the null hypothesis and conclude there is a significant difference when there really is one, and 10% of the tests would incorrectly fail to reject the null hypothesis and conclude there is not a significant difference when there really is one. 

Keep in mind that we never know which of our hypothesis test results are correct and which are errors, since we dont know the truth!

Power can also be thought of as sensitivity. A more powerful test is more sensitive to small differences or effects.As we mentioned earlier, there is a trade-off between Type I and Type II error rates, and between the Type II error rate and power. In addition, all three are affected by the size of the effect and by the sample size. Lets investigate the relationship between power, errors, sample size, and size of the effect visually. 

This plot shows a typical case. The distribution on the left-hand side of the plot (solid black line curve) represents the sampling distribution of the sample mean assuming that the null hypothesis was really true. This is the same distribution that we saw in a previous lecture. The center of the distribution is at the null hypothesized population mean, 0. However, suppose that the truth is actually that the population mean  equals some alternative value (denoted A in the plot). The sampling distribution of the sample mean in this case is on the right-hand side of the plot (blue dashed line curve). The difference between the null value, 0 , and the alternative value, A, is called the effect size.  

The Type I error rate for our hypothesis test is set at alpha=0.05 (the vertical black line on the plot). When we carry out a hypothesis test, we either reject the null hypothesis if our sample mean result is to the right of the vertical black line, or we fail to reject it if our sample mean result is to the left of that line.

If the null hypothesis is really true, then we need to focus on the left-hand sampling distribution. The region shaded in red represents alpha, the Type I error rate, which is typically 5% of the area under the null hypothesis sampling distribution. [We are assuming a one-sided test here, for simplicity.] As we conduct a hypothesis test, we look to see if the sample mean lies in the region that is shaded in red (sometimes called the rejection region), which would indicate that the sample result is unusual under the null hypothesis and the p-value will be less than alpha. If so, we reject the null hypothesis and conclude that there is evidence in support of the alternative hypothesis. However, IF the null hypothesis was really true, then this conclusion would be a Type I error. This would occur in only about 5% of the sample results over the long term. 

If the alternative hypothesis is really true, then we need to focus on the right-hand sampling distribution. The region shaded in yellow represents beta, the Type II error rate, which is typically 10% of the area under the alternative hypothesis sampling distribution (assuming the study power was set at 90%). As we conduct a hypothesis test, if the sample mean lies to the left of the vertical black alpha=0.05 line, this would indicate that the sample result is NOT unusual under the null hypothesis and the p-value will be greater than alpha. If so, we fail to reject the null hypothesis and conclude that there is no evidence in support of the alternative hypothesis. However, IF the alternative hypothesis was really true, then this conclusion would be a Type II error. This would occur in only about 10% of the sample results over the long term. 

Power is the chance that we obtain a sample result that is to the right of the alpha=0.05 line, when in fact, the true population mean is A. Power would thus be the area that is under the blue dashed-line curve AND to the right of the alpha=0.05 line. If beta was set at 10%, then the power will be 90%. In this case, nearly all of the area under the blue dashed-line curve is to the right of the alpha=0.05 line.

We would like this area for power to be as large as possible and the areas for the errors to be as small as possible. However, as already mentioned, there are unavoidable trade-offs.As previously mentioned, the researcher sets the significance level, the Type I error rate, in advance. You might think that the best approach in a hypothesis test would be to set the significance level, alpha, at a very low level, so that the risk of a Type I error would be very low. That is, there will be a small chance of mistakenly concluding that a result is statistically significant. Unfortunately, there is a direct trade-off between the Type I error rate and the Type II error rate. Lowering the Type I error rate (making alpha smaller) necessarily increases the Type II error rate (beta) and therefore decreases the power of the test. Lowering alpha means that you are requiring stronger and stronger evidence of unusualness from your data before you will conclude that there is a differenceor in other words, you are making your test less and less sensitive so it will only see really BIG effects. This makes it harder and harder to mistakenly conclude that there is a difference when there isnt, but it correspondingly makes it easier and easier to miss a real difference that happens to be not big enough.

This can be seen visually in the plot above. When alpha is decreased to 0.01 (holding everything else the samesame hypotheses, same variability, same sample size), the vertical line shifts to the right, making the chance of a sample result falling into the red region tiny (alpha has decreased) and thereby making the chance of falling into the yellow region bigger (beta has increased). As a result, power also decreases when alpha decreases, other things being equal.What would happen if we increased alpha? The converse of the situation described on the previous slide happens. When alpha is increased from 0.05 to 0.10 (holding everything else the samesame hypotheses, same variability, same sample size), the vertical line shifts to the left, making the chance of a sample result falling into the red region larger (alpha has increased) and thereby making the chance of falling into the yellow region smaller (beta has decreased). 

Stated differently, the risk of making a Type I error would be higher, but the risk of making a Type II error would be lower (and power would be higher). If the null hypothesis was true, there is a larger chance that we will mistakenly conclude there is evidence of an effect when in fact there isnt one (i.e. make a Type I error). But if the alternative hypothesis was true, there is a smaller chance that we will miss a real difference (i.e. make a Type II error) and a larger chance that we will catch a real difference. So increasing alpha also increases power, other things being equal. We just looked at the trade-off between Type I error and Type II error (and power), but lets look at two other features that affect the distributions: effect size and sample size. 

What would happen if the true effect size, or difference between the null value and the alternative value, was larger? If everything else stayed the same (same alpha level of 0.05, same variability, and same sample size) and the only thing that is different is that the centers of the two distributions are farther apart, then the chance of making a Type I error (red region) stays the same (since alpha was kept at 0.05), but the chance of making a Type II error (yellow region) is smaller, and the chance of correctly concluding there is an effect (power) gets larger. It is easier to see larger effects than smaller ones. 

Of course, we wont ever know what the true population mean is, but if the true population mean is farther away from the hypothesized null value, then there is a larger chance we will correctly conclude that there is a difference when there really is one.What about increasing the sample size? How will this affect errors and power? Recall from a previous lecture that as the sample size increases, the variability in the sampling distributions decreases. That is, the sampling distributions get narrower. 

So what is the effect of sample size on errors and power (holding all else constant)? If the sample size is increased, both sampling distributions (the one under the null hypothesis and the one under the alternative hypothesis) become narrower. Since we have fixed the Type I error at 5%, as the distributions get narrower, the vertical alpha=0.05 line has to move to the left so that the rejection region (red area) will still contain 5% of the area under the null hypothesis sampling distribution. Thus the chance of making a Type I error (red region) stays the same, but the chance of making a Type II error (yellow region) gets smaller, and the chance of correctly concluding there is an effect (power) gets larger. 

Increasing the sample size has the same effect as increasing the effect size: both (assuming all else is held constant) reduce the Type II error rate and increase the power of the study. The difference is that the true effect size is usually not under the control of the study investigator while the sample size usually is. So in general, increasing the sample size is usually the only practical way to increase the power of a study.
What are the implications if we reject the null hypothesis and conclude that there is a statistically significant difference or effect? Does that imply that the effect is large enough to be interesting or clinically significant? The answer is no. A small p-value just means the sample results would be surprising if the null hypothesis was true. So what are some possible explanations for why one might get a p-value less than 0.05?
 
Explanation #1: It is possible that there really is a meaningful effect or difference. This is what everyone thinks about and hopes for when they see a small p-value. The effect is statistically significant AND clinically significant.

Explanation #2: The effect is statistically significant but not clinically significant. In other words, there is a small (but not meaningful) effect or difference. So while the p-value may be small, it could be that we had a large sample size and low variability so it was easy to detect a difference, even if that difference isnt large. But this small difference might not be practically or clinically significant. It is good to look at multiple sources of evidence to make a conclusion and not just rely on only the p-value.

Explanation #3: There is really no effect or difference, but we just happened to get an extreme sample result, just by chance: the result is a false positive. If alpha is set at 0.05, then we would expect to get a Type I error or false positive result about 5% of the time in the long run. 

Explanation #4: The result arose due to p-hacking. P-hacking refers to fishing in the data to try to find a statistically significant result, when in fact, there is none. Some examples of how researchers p-hack include:
Slicing and dicing data by analyzing various subsets and only reporting the results that produced low p-values, or 
Tweaking analyses in various ways and only reporting the results that give p-values less than 0.05, or 
Peeking at the data before analyzing it to help define their alternative hypothesis. 
P-hacking or fishing is one of the most damaging things researchers can do to the integrity of a study. It fills the scientific literature with irreproducible results and makes people skeptical about research findings. It is essential for researchers to be transparent in reporting how a study was conducted and how the data were analyzed.

In sum: be cautious when interpreting small p-values or statistically significant results. Dont automatically default to Explanation #1 without also considering the possibility that the true explanation is one of the others listed. Consider multiple sources of evidence (e.g., point estimates, interval estimates, sampling method, study design) before making a conclusion about the population.What are the implications if we fail to reject the null hypothesis and conclude that there is not a statistically significant difference or effect? Does that imply that the effect is not interesting or clinically significant? The answer is again no. A large p-value just means the results would not be surprising if the null hypothesis was true. So what are some possible explanations for the why one might get a p-value greater than 0.05?

Explanation #1: It is possible that there really is not a meaningful effect or difference. This is what everyone thinks about when they see a large p-value.

Explanation #2: It is possible that there really is a meaningful effect, but it was too small to be detected in the study. The study did not have enough power to detect the effect. (Perhaps the researchers thought the effect would be larger and powered their study to detect that larger effect. Their study would then have been underpowered for the smaller effect. Or perhaps the variability in their study was much larger than expected and kept them from being able to see a small difference.)

Explanation #3: There is really an effect or a difference, but we just happened to get a sample result that is not extreme given the null hypothesis, just by chance: the result is a false negative. If the study was powered at 90%, then we would expect to get a Type II error or false negative result about 10% of the time in the long run. 

Remember, a high p-value does not prove that the null hypothesis is true: it just means that we dont have sufficient evidence to reject the null hypothesis. Just as in the previous slide, consider multiple sources of evidence (e.g., point estimates, interval estimates, sampling method, study design) before making a conclusion about the population.You may be wondering at this point, how does one go about choosing the significance, or alpha, level? The default is alpha=0.05. To use a different alpha value, you will need to thoroughly justify your choice when you report the study. Here are some guidelines: 

If the goal of the study is to answer exploratory questions of interest, particularly in pilot studies that are generating data for planning future studies, then people sometimes use a larger value of alpha such as perhaps 0.10 or 0.20. 

If the goal of the study is to overturn a long-standing theory or assumption, then the evidence needs to be stronger to convince people that the long-standing theory is questionable, so a smaller value of alpha may be helpful, such as perhaps 0.01. (Using a smaller value of alpha is analogous to setting a higher bar for considering a result to be significant.)

Or, if the consequences of rejecting the null hypothesis are going to be substantialmaybe it results in an expensive change in policy or practiceit might be helpful to use a smaller value of alpha. 

However, it is also important to consider the trade-offs between the Type I and Type II error rates when designing a study. As previously discussed, setting alpha at a smaller value necessarily increases beta and decreases power.

<!--chapter:end:5c-ErrorsPower.Rmd-->

In previous lectures, we discussed using confidence intervals and hypothesis testing to make inferences about a single groups mean. 

This lecture focuses on using hypothesis testing to compare means from two independent groups (for example to compare treatment versus control groups, or males versus females). As before, we recognize that in a given study, the difference in means between two groups, such as treatment and control groups, is unlikely to be exactly zero even if there is no treatment effect in the population. The difference will vary a little bit around zero from one studys sample to the next studys sample simply due to sampling variability. The question is, is the difference we observe in this study sufficiently large that it is unlikely to be due merely to sampling variability when there is actually no difference between the two groups? Heres an interesting two group scenario. The health benefits of drinking tea have been known for centuries. However, research on the immunologic effect of tea was missing. In 2003, researchers published an article1 to try to fill that gap in scientific knowledge. They studied the effect of drinking tea on the production of interferon gamma, which is a molecule that helps the immune system fight bacteria, viruses, and tumors. 

The researchers recruited 21 healthy individuals who did not normally drink tea or coffee and randomly assigned them to either the tea group (n=11) or the coffee group (n=10). The tea group was asked to drink five to six cups of black Lipton tea per day for two weeks (equivalent to roughly 600 ml/day) and the coffee group was asked to drink five to six cups of instant Nescafe coffee per day for two weeks. Coffee was chosen as the comparison treatment because it also has caffeine but does not have the L-theanine that is in tea. Volunteers were allowed to add sugar, lemon, or cream to suit their taste. After two weeks, blood samples were collected and the interferon gamma production was measured. Plots of this data are presented above. 

The summary statistics for the production in the coffee group are: 
Mean = 17.7, SD = 16.7, Median = 15.5, Q1 = 5.0 , Q3 = 21.0

The summary statistics for the production in the tea group are: 
Mean = 34.8, SD = 21.1, Median = 47, Q1 = 15.5, Q3 = 53.5

Based on this, we can see that the mean interferon gamma production is higher in the tea group than the coffee group. But could this large of a difference be the result of sampling variability or is this evidence of a difference? Lets use statistical inference methods to answer this question.

References: 
1Kamath, A. B., Wang, L., Das, H., Li, L., Reinhold, V. N., & Bukowski, J. F. (2003). Antigens in tea-beverage prime human V?2V?2 T cells in vitro and in vivo for memory and nonmemory antibacterial cytokine responses.Proceedings of the National Academy of Sciences,100(10), 6009-6014.
Dataset: Lock, R.H., Lock, P.F., Lock Morgan, K., Lock, E.F., & Lock, D.F. (2013). Statistics: Unlocking the power of data (1st ed.). Hoboken, NJ: John Wiley & Sons, Inc. 
As with any study, we begin with a research question. Since the research question drives the entire research process, it is essential to define it clearly at the outset of a study. 

The research question for the Tea, Coffee, and the Immune System study is: is there an effect of drinking tea (compared to coffee) on the production of interferon gamma?  Before we can carry out any inferential method, we first need to evaluate the assumptions of the method. The assumptions behind hypothesis tests and confidence intervals for comparing means in two groups are the same as those weve seen before, except now we need to evaluate them for each group. That is: 

The samples should be random (or representative) samples from the respective populations, to allow us to generalize the results to those populations; 

The observations within each group should be independent of one another (as discussed in earlier lectures). IN ADDITION, the observations in one group should be independent of the observations in the other group. This assumption would be violated if the observations in one group were related in some way to those in the other group: for example, if the participants in the two groups were matched or paired. If the observations are not independent, then methods for correlated data should be used instead.

The sampling distribution of the difference in sample means between the two groups should be approximately Normal. Statistical theory (which is beyond the scope of this course) tells us that if two sample means are normally distributed, then the difference of the two means will also be normally distributed. So this assumption simply requires us to be sure that the sampling distributions of the sample means in each group are approximately Normal. How can we check this assumption? We can check to see if the conditions for the Central Limit Theorem (CLT) hold. If the underlying population distributions for each group are approximately Normal (which we can check by plotting two sample distributions, one for each group), then the sampling distribution of the sample means will be approximately Normal. OR, if the underlying population distributions are not Normal but the sample sizes are large enough (and what is large enough depends on how heavily skewed the population distributions are), then the sampling distribution of the sample means will still be approximately Normal. If this assumption is not met, then other methods for carrying out inference for comparing two means (such as re-randomization tests) should be used instead. 

If these assumptions are not met, then the results of the inferential methods (CIs and hypothesis tests) will not be valid. 
Lets check the assumptions for the Tea, Coffee, and the Immune System example. 

Are the healthy participants a random or representative sample of all healthy adults? Based on the brief description of the dataset, it doesnt seem that the participants were randomly selected. Could they be considered representative of all healthy adults? It seems reasonable to argue that the researchers would likely have chosen the participants to be representative of all healthy adults. 

Are the observations independent of one another? Again, without more information, it is reasonable to assume that the participants in each group are independent of each other and that the participants from one group are independent from participants in the other group. 

Lastly, are the conditions met for the sampling distribution of the difference in sample means to be approximately Normally distributed? The sample sizes are small so its hard to tell from the sample statistics and plots whether the data for each group are bell-shaped or not. Because the shapes of the sample distributions for each group do not appear to be severely skewed, the sample sizes of 10 and 11 may be large enough to assume the sampling distribution of the difference in means is approximately Normal. 




References:
Image: https://www.pinclipart.com/pindetail/owRJbh_file-mw-icon-checkmark-svg-creative-commons-check/The next step is to define the hypotheses for the test. We write them in terms of the population parameters of interest  the population mean for group 1, 1, and the population mean for group 2, 2.

Recall that the null hypothesis defines the skeptical perspective or the no difference situation. When comparing two groups on a continuous variable, it is written as: the population mean for group 1, mu1, is equal to the population mean for group 2, mu2, or with a little rearranging, the difference in the two population means (mu1  mu2) equals 0. 

The alternative hypothesis, on the other hand, defines the competing claim, the thing we are interested in finding evidence for. 

Just as we saw before, there are three different forms the alternative hypothesis can take. 
If we have written our null hypothesis as 1 = 2, then our alternative hypothesis can be written as either:
1 is not equal to 2 (two-tailed test), or
1 is greater than 2 (one-tailed test), or
1 is less than 2 (also a one-tailed test). 

If we have written our null hypothesis as 1 - 2 = 0, then our alternative hypothesis can be written as either:
1 - 2 is not equal to zero (two-tailed test), or
1 - 2 is greater than zero (one-tailed test), or
1 - 2 is less than zero (also a one-tailed test). 

As always, the alternative hypothesis is defined by the research question.Lets define the hypotheses for the Tea, Coffee, and the Immune System example. 

The null hypothesis is that the true difference in mean interferon gamma production between the tea and coffee groups is 0, or, to put it another way, the true mean production is the same for the tea and coffee groups. Using notation, we can write T (subscript T for the tea group) equals C (subscript C for the coffee group), or T minus C equals 0. 

Because the researchers were interested in whether there was a difference, with no indication of the direction of the difference, the appropriate alternative hypothesis is that the true difference in mean interferon gamma production between the tea and coffee groups is not 0, or, the true mean production is not the same for the tea and coffee groups. Using notation, we can write T does not equal C, or T minus C does not equal 0. This is a two-sided alternative hypothesis.We evaluate the claims in a similar fashion as presented in a previous lecture: 
We collect evidence (data), 
We summarize the data using exploratory data analysis (summary statistics, tables, graphs), and 
We calculate a test statistic to measure the compatibility between the result from the data and the null hypothesis. 

The test statistic when comparing two means (given that the assumptions are met) is similar to the one used to carry out a test for a single mean: it is a t-test statistic. This test is often referred to as a two-sample t-test or unpaired t-test. The formula for the two-sample t-test statistic looks similar to the one for the one-sample t-test statistic we used before: we use the sample estimate (this time, its the difference in the two sample means) minus the null value, all divided by the standard error for that estimate. Written out, it is the difference between the two sample means (x-bar in group 1 minus x-bar in group 2) minus 0 (the null value), divided by the square root of the standard deviation in group one (s1), squared, over the sample size in group 1 (n1) , plus the standard deviation in group 2 (s2), squared, over the sample size in group 2 (n2). Again, this value measures how far the sample result from the study is from what we would expect (given sampling variability) IF the null hypothesis was really true. Large values of the t-statistic represent large (relative) differences between the sample result and the null value and small values represent small (relative) differences. 

The formula for the degrees of freedom for this two-sample t-test statistic, shown on the slide above, is slightly more complex than that for a one-sample t-test. If we were carrying out this analysis in software, we would just let the software calculate the more precise degrees of freedom. Without software, we can calculate the approximate degrees of freedom by using the smaller of the two values: n1  1 or n2  1.For the Tea, Coffee, and the Immune System example, data were collected and summarized using plots and summary statistics earlier in this presentation. The sample mean, sample standard deviation, and sample size for the tea and coffee groups are presented again here for reference. The difference in mean interferon gamma production between the tea group and the coffee group was 17.1. Would this difference in means be unusual if the true difference between the groups were really zero?

To answer this question, we compute a t-test statistic using the data and the null value. The t-test statistic for this example is 34.8 minus 17.7, all divided by the square root of 21.1 squared over 11 plus 16.7 squared over 10. Solving this out results in a t-test statistic of 2.07. This test statistic has a t-distribution, where the degrees of freedom is the smaller of nT  1 = 11  1 = 10 or nC  1 = 10  1 = 9 degrees of freedom: df = 9. If our null hypothesis were really true and there was no difference between the population means of the two groups, then the sampling distribution of the t-test statistic would follow a t-distribution with 9 degrees of freedom, as shown in the plot above. Our t-test statistic value (or t-value) of 2.07 lies in the upper (right) tail of this distribution. This value seems fairly unlikely to occur, but how likely or unlikely is it? Lets quantify this probability. Recall that we quantify how unusual the evidence is compared to what is assumed to be true by computing a p-value. The p-value is the probability that you would obtain a sample difference this unusual if the null hypothesis were really true and any observed difference was simply due to sampling variability. To put it another way, its the probability of getting our sample result (or one even more extreme) if the null hypothesis were true. As before, what counts as extreme depends on the alternative hypothesis. The smaller the p-value, the less consistent or compatible the data are with the null hypothesis. 

Once we have the p-value, we make a conclusion about the strength of evidence we have against the claim by comparing the p-value to the significance level, alpha. If the p-value is less than alpha, we reject the null hypothesis and say we have evidence against the null in favor of the alternative hypothesis. Another way of saying this is that the result is statistically significant. Alternatively, if the p-value is greater than alpha, then we do not reject the null hypothesis and we say we lack evidence against the null. This result would not be considered statistically significant. In either case, remember to always state the conclusion in the context of the problem. Lets evaluate the evidence for the Tea, Coffee, and the Immune System example. 

With a t-value of 2.07 and an alternative hypothesis that is two-sided, we are interested in the probability of seeing a t-value of 2.07 or more extreme if the null hypothesis were really true. The t-value could be more extreme, which means further from zero, in either direction: either further above +2.07 or further below -2.07. Therefore, the probability we are looking for is the area in both tails of the t-distribution, highlighted green in the plot above. To find this, we calculate the area under the t-distribution curve to the right of +2.07 and add to it the area to the left of -2.07, which is the mirror image of the upper portion of the distribution. Alternatively, because the t-distribution is symmetric, we can just calculate the area under the curve to the right of +2.07 and multiply this value by 2. The p-value for this example is 0.068. For a significance level (alpha) set to 0.05, we fail to reject the null hypothesis because the p-value of 0.068 is greater than 0.05. Therefore we conclude that we did not find evidence of a significant difference in mean interferon gamma production between the tea and coffee groups. The data from this sample showed some evidence that tea drinkers might have higher mean interferon gamma production than coffee drinkers, but the evidence is not very strong. The information presented on two-sample t-tests thus far has assumed that the variability in the two groups is different. However, there may be situations when one might reasonably assume that the two populations have the same, or nearly the same, variances (or standard deviations), even if their means are distinct. If there is a reasonable argument for this strong assumption of equal variances, then a pooled variance can be calculated as shown above. {The term pooled refers to the fact that we are using the data from both groups pooled together to estimate the variance.} If the assumption is valid, then the pooled variance provides a better estimate of the true variance because it uses a larger sample of data (from both groups combined).

This pooled variance would be substituted into the standard error formula shown earlier in this presentation, by replacing both (s1)2 and (s2)2 with (spooled)2 and then simplifying to give the SEpooled formula shown above. The associated degrees of freedom for the updated t-test statistic is n1 + n2  2. 
As we wrap up the topic of t-tests, here are some additional notes about these tests. 

To pool or not to pool the standard deviations? This topic is controversial in statistics. There are hypothesis tests to test whether the population variances are equal, but they are not very powerful; in addition, if the assumptions of those tests are not met, then the results will not be valid. There are also rules of thumb to gauge whether the two variances are approximately equal, but these are based on experience and not on theory. One textbook1 claims that pooling offers almost no advantage over the unequal variance case. Therefore, we recommend using the more conservative approach of not pooling the standard deviations.

Recall that one of the assumptions of the t-test is that the data come from a population which is approximately Normal. Furthermore, one way to check this assumption is to plot the data and judge by eye whether the distribution looks approximately bell-shaped. (Note that this check doesnt work very well if the sample size is small.) What should we do if we find that the data dont look very normal? Good news! The t-test procedure is robust to even fairly large deviations from normality and the results will still be valid in most cases, provided the sample sizes arent too small. 

Lastly, heres a fun historical fact2 about the t-test procedure. It was devised by William Gosset in the early 1900s to monitor the quality of Guinness beer. Because Guinness didnt allow their employees to publish their findings, he published under the pseudonym Student and so the procedure was known for many years as Students t-test.

References: 
1Lock, R.H., Lock, P.F., Lock Morgan, K., Lock, E.F., & Lock, D.F. (2013). Statistics: Unlocking the power of data (1st ed.). Hoboken, NJ: John Wiley & Sons, Inc. 
2Wikipedia contributors. (2019, July 29). Student's t-test. In Wikipedia, The Free Encyclopedia. Retrieved 16:52, July 30, 2019, from https://en.wikipedia.org/w/index.php?title=Student%27s_t-test&oldid=908375594

<!--chapter:end:6a-HypTestComparingMeans.Rmd-->

This lecture focuses on using confidence intervals to compare means from two independent groups (for example to compare treatment versus control groups, or males versus females). Recall that confidence intervals are used to estimate the true population value: in this case, the true difference in means between the two groups. We will use the same example that we used earlier, in the lecture on hypothesis testing for comparing two means: the Tea, Coffee and the Immune System study.  This lecture will also discuss the relationship between hypothesis testing and confidence intervals. 

When our goal is to compare means from two independent groups, we can use hypothesis testing, as described in a previous lecture, or we can use confidence intervals. Confidence intervals are an alternative method for summarizing the evidence provided by the data. In the case of comparing the population means in two groups, the confidence interval of interest is the one that estimates the true difference between the two population means.

Recall that the general formula for a confidence interval is the point estimate plus or minus the margin of error. Filling in these details for estimating a difference of two population means:

--The point estimate is the difference between the two sample means. 

--The confidence interval is the point estimate plus or minus the degree of confidence, which is the appropriate t-value, times the estimated standard error for the difference in the sample means. 

--The estimated standard error for this situation is the same SE value described in the previous lecture on hypothesis testing for comparing two means: the square root of the standard deviation in group one (s1), squared, over the sample size in group 1 (n1) , plus the standard deviation in group 2 (s2), squared, over the sample size in group 2 (n2). 

--The appropriate degrees of freedom for this t-value is also the same as described in the previous lecture on hypothesis testing for comparing two means. If we were using software, we would just let the software calculate the more precise degrees of freedom for the t-value. Without software, we would use the approximate degrees of freedom by using the smaller of n1  1 or n2  1. 

The confidence interval formula presented on this slide only applies when all of the assumptions are met for comparing two means. (These assumptions were presented in the previous lecture on hypothesis testing for comparing two means.) Remember to check the assumptions first before carrying out any inferential method.  Now lets calculate the confidence interval for the Tea, Coffee, and the Immune System example. 

The difference in the sample mean interferon gamma production between the tea and coffee groups is 17.1 and the estimated standard error for the difference is 8.291. The t-value for a 95% confidence interval is the value in the t-distribution with nc - 1 = 9 degrees of freedom and 0.975 area lying below that value, so the t-value is 2.262. Putting all of those values togetherthe point estimate, the t-value, and the standard errora 95% confidence interval for the difference in the population mean interferon gamma production between the tea and coffee groups is -1.7 to 35.9.

We are 95% confident that the difference in mean production between the tea and coffee groups is between -1.7 and 35.9. Or we could say, a range of plausible values for the true difference in mean production between the tea and coffee groups is from -1.7 to 35.9. 
Now that the two primary procedures for statistical inference  hypothesis testing and estimation via confidence intervals  have been presented for both a single mean and comparing two means, lets look at the connection between these two. 

Recall that the values in a confidence interval provide plausible values for the true population parameter. If the null value from hypothesis testing (a two-sided test) does not fall within the confidence interval limits, then this value is not plausible and therefore, we would have evidence that the true population value is different than the null value. That is, the results are statistically significant. This situation is shown in the first plot above: the confidence interval does not include the null value of (in this case) zero. 

Conversely, if the null value falls within the confidence interval limits, then it is a plausible value (one of many different plausible values), and we would not have enough evidence to say the true population value is different than the null value. That is, the results are not statistically significant. This situation is shown in the second plot above: the confidence interval DOES include the null value of zero. 

Reviewing the results from the Tea, Coffee, and the Immune System study, the confidence interval for the difference of means was -1.7 to 35.9. Because 0, the null value, is within that confidence interval, it is a plausible value and we therefore conclude that there is a lack of evidence that the tea and coffee groups have differing mean interferon gamma production. This is the same conclusion that we reached in a previous lecture, when we carried out a hypothesis test for this example and found a p-value of 0.068.

You might be wondering, why use hypothesis testing at all if we could just use a confidence interval to make a conclusion about the population? If we only presented confidence intervals, we would lose information about how incompatible the observed data may be with the null hypothesis. On the other hand, if we only provided p-values, then we would lose information about the magnitude of the difference. Recently, many journals have been modifying their statistical guidelines for authors by requiring them to provide confidence intervals in addition to (or in extreme cases, in place of) p-values. We recommend that you report key study results using a four-number summary: the point estimate for the statistic of interest (for example, the difference in means between the treatment and control groups), a confidence interval, and a p-value.We end this lecture with a caution.

In publications, data comparing two or more groups on a continuous outcome are sometimes presented as a bar plot showing the mean for each group (i.e., the height of the bar) and an error bar. The error bars might represent plus or minus the standard deviation for each group, or plus or minus the standard error for each group; or they might represent a confidence interval for the mean for each group. Two of these types of plots from the Tea, Coffee and the Immune System study are presented above, as an example.

It might be tempting to look at whether the error bars overlap in order to make a conclusion about whether the difference in means is statistically significant. But resist the temptation! In reality, you cant learn much by asking if the two error bars overlap. 

To help you in interpreting these kinds of graphs when reading research articles, a table with the rules of thumb is presented on the slide. Note that these rules only apply when comparing two means and when the sample sizes are equal or nearly equal. 

If the type of error bar is the standard deviation, we cant actually make any inferential conclusion about whether the group means are significantly different from one another by just looking to see if the error bars overlap. This is because the plot doesnt display any information about the sample size, which is taken into account when using the t-distribution to compare two means. 

If the type of error bar is the standard error, then we can only conclude the difference is not statistically significant (i.e., the p-value is greater than 0.05) if the two SE error bars overlap. Unfortunately, the opposite does not apply. If two SEM error bars do not overlap, then the p-value could be less than 0.05 or greater than 0.05. We would not know which one it is unless we carried out a hypothesis test or compute a confidence interval for the difference in means. 

Lastly, if the type of error bar is a 95% confidence interval for each groups mean (plot not displayed on this slide), then we can conclude the difference is statistically significant (i.e., the p-value is less than 0.05) if the two 95% CI error bars do not overlap. The opposite of this is not true. If the two 95% CI error bars do overlap, then the p-value could be less than 0.05 or greater than 0.05. We would not know which it is unless we carried out a hypothesis test or compute a confidence interval for the difference in means.

Rather than memorizing the chart above, we highly recommend that you only make inferences from hypothesis tests or confidence intervals for the actual statistic of interest (in this case, the difference in two group means).

<!--chapter:end:6b-ConfIntervalsComparingMeans.Rmd-->

Thus far, we have discussed the use of the t-distribution (via t-tests or confidence intervals) to make conclusions about a single population mean or to compare two population means. Recall that an assumption of this method is that the data values are sampled from a population that is approximately normally distributed, or that the sample size is large enough to invoke the Central Limit Theorem. What approaches are available if it isnt reasonable to assume normality? What if the data are severely skewed, or contain extreme outliers? This lecture will discuss alternatives to the t-distribution when the normality assumption isnt met. 
There are two options for analyzing data when the normality assumption is not reasonable.

The first option is to transform the severely skewed data to see if the transformed data appears approximately Normal. If this does the trick, we then carry out inferential procedures using the t-distribution on the transformed data. A common transformation is to take the log of the variable. When a t-test is carried out on the log-scale, we are no longer comparing the usual mean values, but rather comparing the geometric means between two groups.

The second option is to use a method that doesnt require the data to fit a parametric (e.g., Normal) distribution. These types of methods are called nonparametric methods, or sometimes distribution-free methods. Two common nonparametric approaches are simulation-based techniques and rank-based tests. 
For simulation-based techniques, we use the data to construct/simulate an approximate sampling distribution of the statistic. These techniques do not require any assumptions about the shape of the population distribution and can be used for any sample statistic. 

Bootstrapping is used to obtain confidence intervals for any statistic, such as a mean, a difference in means, a median, a standard deviation, etc. This procedure involves repeatedly taking samples with replacement from the original sample, calculating some statistic for each sample, and collecting the results in a dotplot or histogram to approximate what the sampling distribution of the sample statistic would look like. If this sounds familiar, it is! In an earlier lecture we discussed repeatedly taking samples from a population, calculating a statistic of interest for each sample, and collecting the results in a dotplot or histogram: this gives the sampling distribution of the sample statistic. The only difference is in where we are drawing the samples from. If we repeatedly sample from the population, we obtain the sampling distribution of the sample statistic, as discussed in earlier lectures. If we instead repeatedly sample from the original sample itself, assuming that the original sample is representative of the population it came from, we obtain an approximate sampling distribution, called a bootstrap distribution. From this bootstrap distribution, we can obtain a 95% confidence interval by simply taking the center 95% of the bootstrap statistics. 

Re-randomization testing is used to carry out hypothesis testing for any statistic. We again use the data to create an approximate sampling distribution of the statistic, but this time we are interested in the sort of statistics we would observe if the null hypothesis were true. So we simulate repeated samples in a way that is consistent with the null hypothesis. For example, if we are comparing two groups, we would randomly reshuffle the group labels (such as treatment and control) on the observations in our study, calculate the group difference statistic (such as the difference in medians) for each re-randomization sample, repeat the reshuffling many times, and collect the differences in a dotplot or histogram. This histogram shows the approximate sampling distribution of the group difference statistic under the null hypothesis and gives us an idea of how much the treatment difference would be expected to vary if the treatment had no effect. We would compare the observed difference from the actual study to this approximate sampling distribution simulated under the null to make a conclusion about whether there is a significant difference due to treatment. If the observed difference was much different (in either direction) from the null distribution, then we would have evidence that there is a difference between the groups.
The other nonparametric approach is rank-based tests. These methods use the rank or order number of the observations rather than the raw value of the observation. A rank-based alternative to the paired t-test is the Wilcoxon test (also known as the Wilcoxon matched-pairs signed-rank test, or the signed-rank test, or even just the sign test). A rank-based alternative to the two-sample t-test is the Mann-Whitney-Wilcoxon test (also known as the Wilcoxon test or the Wilcoxon-Mann-Whitney test, or the Mann-Whitney test, or the rank-sum test or even just the ranks test). We will focus here on the rank-sum test.

The Mann-Whitney or rank-sum test, as described in many textbooks, technically compares the distributions of a continuous variable in two samples, rather than comparing the two group means as the t-test does. The null hypothesis of the Mann-Whitney test is that the two distributions are the same (same shape, same center, same everything), and the alternative hypothesis is that they are different in some way. 

In principle, this test could be used to see if two distributions differ in shape (but have the same center and spread), or to see if they differ in spread (but have the same shape and center). However, for practical purposes, for the kinds of situations that we are likely to encounter in our careers in medicine and public health, the Mann-Whitney test only has high power (high ability to tell the difference between the two groups) when the two distributions are shifted left or right relative to each other. For that reason, the rank-sum test is in practice a test of medians. The null hypothesis for the rank-sum test is that the two distributions have the same median, and the alternative hypothesis is that one distribution has a different median than the other.

The rank-sum test statistic is calculated from the ranks or order number of the combined data instead of from the actual values. If the null hypothesis is true, we expect the sums of the ranks for the two groups to be equal or approximately equal. 
Lets walk through the steps for the rank-sum test to understand how rank-based tests work. The Tea, Coffee, and the Immune System study will again be used as the context for the test. 

The sample median gamma interferon production in the coffee group is 15.5 and the sample median in the tea group is much larger, at 47. We want to know if this difference in sample medians is sufficiently large to reject the null hypothesis that the medians in the two populations are the same, or whether a difference of this size could be due to random sampling variability.

To conduct the Mann-Whitney rank-sum test, we first order all of the measured values, regardless of group. The lowest value in the Tea, Coffee, and the Immune System dataset is 0, and there are two of them in the coffee group (known as tied observations). Because of this tie, the two values are both given the average rank of 1.5 (instead of ranks 1 and 2). The next lowest value is 3 (in the coffee group), so that value gets rank=3. The next lowest value is 5 (in the tea group), so that value gets rank=4. This is continued until all of the values have been ranked. If there were no difference in interferon gamma production between the tea and coffee groups, the ranks would end up fairly evenly split (for example, rank 1 might be in the coffee group, rank 2 in the tea group, etc.) and the sums of the ranks in each group would be roughly the same. In this example, the sum of the ranks in the coffee group (85) is lower than the sum of the ranks in the tea group (146): more of the low interferon gamma production values are in the coffee group compared to the tea group. If the goal was to have high amounts of production, then the tea group won! There are various methods proposed to calculate the Mann-Whitney test statistic, but only one method will be discussed here. The test statistic, which we will call W, is calculated by summing the ranks of the group with the smaller sample size (which we will call Rs) and then subtracting a continuity correction which is related to the sample size, ns, of the smaller group. Once we have this sample statistic, we compare it to the sampling distribution of the rank-sum statistic under the null hypothesis, which is known from statistical theory (but we wont discuss it here), in order to obtain a p-value. The p-value quantifies the unusualness of the sample statistic if the null hypothesis of equal medians were true. If there are no ties, the exact p-value is found using the Wilcoxon rank-sum statistic distribution (via tables or software). If there are ties, then an approximate p-value is found using a Normal approximation (again, via tables or software). We will not go into details of the p-value calculation for this test in this course.

The evaluation piece of the hypothesis testing framework is to make a conclusion by comparing the p-value to the significance level, as usual. Lets apply the rank-sum test to the Tea, Coffee, and the Immune System example. 

Since the coffee group had the smaller sample size, we will calculate W for that group. The sum of the ranks for the coffee group was 85 and there were 10 participants in that group. This results in a W-statistic of 85 minus 10 times 10 plus 1, all over 2, which equals 30. 

There were ties in the data set, so an approximate p-value will be calculated (using software). This value turns out to be 0.084, so we would fail to reject the null hypothesis. There isnt sufficient evidence to conclude that the median interferon gamma production differs between tea and coffee drinkers. 

In this case, the p-value for the rank-sum test (p = 0.084) is close to the p-value from the t-test (p = 0.068), so the two tests agree that the groups dont differ significantly. Since the two tests are looking at different things (medians vs. means), however, they may not always agree. It is important to decide which test is appropriate to run before seeing the data. It is considered statistically shady to run both tests and choose the p-value that best fits your pre-conceived idea of what the test result should be. There are a few considerations for rank-based tests to keep in mind.  

Rank-based tests do not require us to assume anything about the population distribution, but they do require a few other assumptions: they require a random or representative sample, and they require that the observations are independent of one another. 

You may have noticed that the names of the rank-based tests are not nearly as standardized as the names of (for example) the t-test or the ANOVA F-test (which will be presented in a future lecture). This can be confusing. It is probably more helpful to focus on what the test is being used for than on its name(s). 

Studies that use rank-based tests usually only report p-values and not confidence intervals. This is because some rank-based tests requires additional assumptions in order for the confidence interval to be valid. For example, the rank-sum test to compare two medians can be extended to provide a confidence interval for the difference in medians, but this requires that we assume the distributions have the same shape. This assumption is not needed to interpret the p-value from a rank-sum test. As a result, some statisticians consider this a serious limitation to these types of methods. 

Finally, keep in mind that rank-based tests are not magic. They can be used in situations where parametric tests such as the t-test cannot be used, but they come with a cost: the rank-based test will typically have lower power to detect a difference than the parametric test would have had, particularly for small samples. They have lower power because they are only considering ranks and not the actual data values, which essentially is throwing away information. However, with large samples, these tests are nearly as powerful as the parametric tests when the data really do come from a normally distributed population. 

<!--chapter:end:6C-AlternativesTtest.Rmd-->

The design of a study is an extremely important part of doing research. The study design is essentially a blueprint for the collection, measurement, and analysis of the data. It is often argued that the design of a study is more important than the analysis of the results, because if the study design is poor, then the study may not be able to achieve its objective at all, whereas if the study is poorly analyzed, the data can be reanalyzed to reach meaningful conclusions. Different study designs are needed to answer different research questions. The purpose of this lecture is to describe several common types of study designs. We have adapted a taxonomy developed by Grimes and Shultz1 to illustrate a few common study designs used in medical and public health research. Research designs can be divided into two broad categories: observational studies and experimental studies. 

In an observational study, the researcher does not assign exposures to participants. The exposure is the agent under investigation that is thought to be related to the outcome being measured. For example, we may be interested in exposure to heavy metals and its relationship with sinus cancer. Or we may be interested in exposure to repeated cell phone use and its relationship with brain tumors. In all these examples, the exposure is not, or cannot be, assigned by the researcher, for practical or ethical reasons. Examples of observational studies include case series studies, cross-sectional studies, case-control studies, and longitudinal or cohort studies. 

In an experimental study, the researcher assigns exposures, often called treatments in this setting, to participants. For example, we may be interested in the relationship of Laetrile treatment and human cancer. Or we may be interested in the relationship between regular aspirin use and heart attack or death. In these examples, the researcher can decide which participants will receive which treatments. Several subtypes of experimental studies can be defined by whether they include a comparison group or not, or whether they randomly assign the participants to the exposure or not. 

We will discuss observational studies first. 

References:
1Grimes, D. A., & Schulz, K. F. (2002). An overview of clinical research: the lay of the land. The lancet, 359(9300), 57-61.A case series study is an observational study in which a series of participants are observed and the course of their disease or other participant characteristics and outcomes are described. It is similar to a report on a single case (i.e. a case study), but it describes more than one related case. Case series studies typically involve a small number of participants. There is no comparison group. There is no sampling. Case series studies are purely descriptive  no information about the association between the exposure and the outcome can be obtained, and no inference can be made to a larger population. 

Case series studies are typically not planned in advance, but arise due to a researcher or doctor noticing something unusual or unexpected. Publication of observations from a case series may lead to research questions for more rigorously designed studies.Here is an example of a case series study. 

In 1981, Michael Gottlieb and his colleagues at the UCLA School of Medicine reported a rare form of pneumonia and unusual multiple viral infections in four previously healthy young men1. Part of the abstract for this landmark paper is shown here. 

Other similar case series studies followed, and by 1982 the condition was named Acquired Immune Deficiency Syndrome, now commonly known as AIDS.

Reference:
1Gottlieb, M. S., Schroff, R., Schanker, H. M., Weisman, J. D., Fan, P. T., Wolf, R. A., & Saxon, A. (1981). Pneumocystis carinii pneumonia and mucosal candidiasis in previously healthy homosexual men: evidence of a new acquired cellular immunodeficiency.New England Journal of Medicine,305(24), 1425-1431.The main advantage of case series studies is that they can be comparatively easy and inexpensive. They are useful as a descriptive tool, and can also be useful in designing further studies to provide a better quality of evidence. 

The disadvantages to case series studies include no comparison group. The sample consists of either only exposed participants with the outcome or all persons with the outcome. As a result, it is not possible to determine whether there is an association between exposure and outcome; nor to assess whether there might be a cause-and-effect relationship between exposure and outcome. Another disadvantage is that this type of design is usually retrospective, due to the typical unplanned nature of the study. This can result in problems due to use of historical data.A cross-sectional study is an observational study in which data are collected from participants at a single point in time. This provides a snapshot in time of the characteristics of interest. Data for cross-sectional studies are often collected using surveys or polls and can involve a large number of participants. The participants may or may not be randomly selected from the population of interest.

The collected data could include outcome variables of interest, such as disease status, diagnostic test results, or biomarker levels. The data could also include possible explanatory variables, such as exposures to suspected risk factors. Finally the data could include demographic characteristics, such as age, gender, or ethnicity. All of these variables are collected at the same time. The collected data can be used to explore relationships between variables at a single point in time. Cross-sectional study designs are used in prevalence studies to evaluate the prevalence of risk factors, or the prevalence of a disease or condition, in a given population at a certain time. For example, researchers used a cross-sectional study to measure the prevalence of serious eye disease in a north London elderly population in 19981. They can also be used repeatedly to monitor changes in the prevalence of a condition over time. An example of this was a prevalence study of skull fractures in children admitted to a hospital in Edinburgh from 1983 to 19892. Although the study period was seven years, the information about each subject was recorded at a single point in time, which makes it a cross-sectional study. 

Cross-sectional studies are also used to identify associations between a condition and potential risk factors. These associations can be further explored in future, more rigorous, studies. An example of this type of use of a cross-sectional design is a study that administered a questionnaire to elderly people to explore the relationship between alcohol consumption and emergency room visits3.

Cross-sectional studies are also used to establish norms for diagnostic or screening tests, to compare the agreement between a new test and an established test, or to compare the sensitivity and specificity of a test to the known gold-standard diagnosis. An example of this was a study that compared the accuracy of detecting gonorrhea in women between self-taken swab and clinician-taken swab cultures4.

References:
1Reidy, A., Minassian, D. C., Vafidis, G., Joseph, J., Farrow, S., Wu, J., Desai, P., & Connolly, A. (1998). Prevalence of serious eye disease and visual impairment in a north London population: population based, cross sectional study.Bmj,316(7145), 1643-1646.
2Johnstone, A. J., Zuberi, S. H., & Scobie, W. G. (1996). Skull fractures in children: a population study. Emergency Medicine Journal, 13(6), 386-389.
3van der Pol, V., Rodgers, H., Aitken, P., James, O., & Curless, R. (1996). Does alcohol contribute to accident and emergency department attendance in elderly people?.Emergency Medicine Journal,13(4), 258-260.
4Stewart, C. M., Schoeman, S. A., Booth, R. A., Smith, S. D., Wilcox, M. H., & Wilson, J. D. (2012). Assessment of self taken swabs versus clinician taken swab cultures for diagnosing gonorrhoea in women: single centre, diagnostic accuracy study.Bmj,345, e8107.The major advantage of cross-sectional studies is that they can be comparatively quick and inexpensive. There is no follow-up of participants, so fewer resources are required to carry out the study. Another advantage is that they are useful for identifying potential relationships that can then be studied more rigorously using a cohort or experimental study design.

There are three main disadvantages to cross-sectional studies. The first is that cross-sectional studies do not provide any support for cause-and-effect relationships between variables. Since the data are collected at a single point in time, it is not possible to determine whether exposure to a risk factor preceded the outcome of interest. Cross-sectional data give no information about the disease process or natural history.

The second disadvantage is that rare diseases or conditions cant efficiently be studied because there may be no one with the condition even in large samples. 

The last disadvantage is the potential for bias in survey data. There may be volunteer bias. In a study where the participants are selected by volunteering (as in a radio call-in show, for example), those who volunteer may be different in some ways from those who did not volunteer. The volunteers are a self-selected non-random sample which may not be representative of the population of interest. There may also be non-response bias. In a survey that uses random sampling, some of the randomly chosen subjects may refuse to participate in the study. The non-responders may differ from the responders in some way, resulting in a sample which is not representative of the population of interest. Non-response bias is considered a particular problem if a large percentage of those contacted refuse to participate.
A case-control study is an observational study in which participants are selected based on their disease status. Cases are people who already have the disease or condition of interest. Controls are individuals from a similar population who do not have the disease or condition. The cases and controls may or may not be randomly selected. Researchers then look backward in time for past exposures to suspected risk factors that might have resulted in the condition. Past exposure may be ascertained from interviews with the participants themselves, from interviews with family members (particularly if the participant is deceased), or from medical records.

Case-control studies are often called retrospective studies because the outcome already occurred before the participant was included in the study.

Case-control studies are useful for exploring the relationship between a disease (or some other outcome) and possible risk factors, especially if the disease is rare.An example of a case-control study is an investigation into the potential association between cell phone use and brain tumors, published in 2001 in the New England Journal of Medicine1. The researchers identified 782 people with various types of brain tumors (the cases), and 799 people with various non-malignant conditions (the controls). They assessed exposure to cell phone use by interviewing the participants. They did not find any evidence for an association between cell phone use and brain tumors.

Reference:
1Inskip, P. D., Tarone, R. E., Hatch, E. E., Wilcosky, T. C., Shapiro, W. R., Selker, R. G., Fine, H.A., Black, P.M., Loeffler, J.S., & Linet, M. S. (2001). Cellular-telephone use and brain tumors.New England Journal of Medicine,344(2), 79-86.Case-control studies are invaluable for studying rare diseases, because you can obtain an adequate number of cases for the study and not have to wait for the condition or disease to occur as you would in prospective studies. They are also relatively quick and inexpensive in comparison to prospective studies.

However, case-control studies have a number of disadvantages. The first is that case-control studies do not provide any evidence for cause-and-effect relationships between variables. In many cases it is not possible to determine whether exposure to a risk factor preceded the outcome of interest. Case-control data also give no information about the disease process or natural history.

Another disadvantage is the potential for bias. One potential source of bias is misclassification bias, which occurs when a case is misclassified as a control, or vice versa. It isnt always easy to determine disease status. Misclassifying a case or a control may result in a biased (inaccurate) measure of association. Another potential source of bias is recall bias. People do not always accurately remember their exposure history, and cases may be more likely than controls to remember exposure to something they believe may have caused their disease. This also may result in a biased measure of association.

Another disadvantage is that case-control studies cannot be used to estimate the prevalence of a disease or condition in the population. The ratio of cases to controls is chosen in advance by the researcher: common ratios are 1 control for every 1 case, or 2 controls for every 1 case. If a ratio of 1:1 is used, then half of the study sample will be cases and half will be controls. This does not mean that the prevalence of the disease in that population is 50%. The proportion of cases in the study sample is not a measure of the prevalence of the disease in that population.

A final disadvantage is that selection of appropriate controls can be difficult. The ideal control subject is a person who is identical in every possible way to the case subject except that they do not have the disease of interest. This situation cannot be perfectly attained in real life.A cohort study is an observational study in which a group of participants (a cohort) is followed over time to determine how many develop the disease or condition of interest. For this reason, cohort studies are sometimes called follow-up studies or longitudinal studies. The study participants may or may not be randomly selected. All participants in a cohort study are initially healthy in the sense that they have not yet had the outcome of interest (for example, a primary heart attack). 

Members of the cohort may be classified as exposed or unexposed to one or more risk factors at the beginning of the study. For example, study participants could be identified as smokers and non-smokers, or as overweight and normal weight.

Both the exposed and the unexposed participants in the cohort are followed over time, and the incidence of the disease or condition of interest in each group is determined.

Cohort studies are useful for exploring the relationship between risk factors and disease.Most cohort studies are prospective or forward-looking. In a prospective cohort study, the exposure has already occurred but the outcome has not yet happened. The risk factor exposure is determined in the present, when the participants are enrolled in the study, and information about the outcome is collected at a future time, after the specified length of follow-up.

Cohort studies can also be retrospective or backward-looking. In a retrospective cohort study, both the exposure and the outcome have already occurred. Historical records (such as medical records) are used to assess the risk factor exposure at some point in the past (say, for example, in 1950). The historical records are then used to follow the participants forward over time and collect information about the outcome after the specified length of follow-up time (say, for example, in 1970 after 20 years of follow-up). Retrospective studies are also called historical cohort studies. They require complete and accurate historical medical records. 

In both prospective and retrospective cohort studies, though, the participants are initially healthy  they do not yet have the disease or condition of interest.An example of a prospective cohort study is the Nurses Health Study1. It was started in 1976 and is now in the third generation of the study with over 275,000 participants. The participants fill out a questionnaire once every two years. The original goal of the study was to investigate the risk factors for major chronic diseases in women. The most current study includes both men and women from a variety of health-related fields and examines how dietary patterns, lifestyle, environment, and nursing occupational exposures impact their health. 

Reference:
1https://www.nurseshealthstudy.orgA major advantage of cohort studies, compared to other observational studies, is that they can provide evidence for possible cause-and-effect relationships between exposures and outcomes. This is because in a cohort study, exposure to the risk factors always precedes the outcome of interest, thereby avoiding the debate about which is cause and which is effect. Cohort studies can also provide information about the disease process or natural history.

Cohort studies have a number of disadvantages, however. The major disadvantage is that they tend to involve large numbers of participants over multiple sites, and may involve follow-up times of years or decades, and can therefore be time-consuming and expensive. Another major disadvantage is that the long follow-up times can result in large numbers of participants dropping out or being lost to follow-up, which can introduce bias.

Another disadvantage is that cohort designs are not suitable for very rare outcomes. If the outcome is extremely rare, then an extremely large cohort will need to be followed in order to obtain enough outcome events. This is prohibitively expensive.

Retrospective cohort studies, as mentioned previously, cannot be conducted at all unless complete and accurate medical records covering the full time-period of interest are available.

A last disadvantage is the potential for selection bias, that is, obtaining a sample that is not representative of the population of interest. For example, the researchers from the original Nurses Health Study wanted to make inferences about women, in general. However, their sample only contained women who were nurses, and we should not presume that women nurses are representative of all women. In contrast to observational studies, experimental studies involve the investigator controlling some intervention, such as a treatment, for the study participants. Examples of interventions of interest in medical or public health research include a new drug, a new surgical procedure, an education or counseling program, or an exercise program. The study participants are followed over time in order to determine the effect of the treatment on the disease or condition of interest. Experimental studies can involve one or more comparison (or control) groups or no control groups (which would be called an uncontrolled study). They also may involve random assignment (known as randomization) of participants to treatments. 

Experimental studies include laboratory studies, pre-clinical or animal studies, and clinical trials. We will focus here on clinical trials.

References:
Grimes, D. A., & Schulz, K. F. (2002). An overview of clinical research: the lay of the land. The lancet, 359(9300), 57-61.As previously mentioned, experimental studies can be designed with or without a comparison group. 

A controlled trial involves an internal comparison or control group. The control group consists of participants who do not receive the intervention or treatment of interest. Instead, they may receive no treatment at all, a placebo, the current standard-of-care treatment (if there is one), or even the same treatment but delayed until a later time. The presence of a control group that is similar in every other way to the treatment group, except that it did not receive the treatment, allows the investigators to attribute any group differences in outcomes or responses to the treatment itself.

An uncontrolled trial does not involve an internal comparison group. It is still considered an experimental study, though, because the investigator directly administers some intervention or treatment to the study participants. This method of investigation has some similarities with both case-control and cohort studies. However, unlike in case-control studies, there is follow-up of participants over a period of time until the outcome is observed or the study ends. And unlike in a cohort study, the intervention is planned and administered to participants, rather than just observed. Sometimes, in the early stages of understanding a new intervention, an uncontrolled design may be used. The gold standard for medical and public health research is the randomized controlled clinical trial (RCCT or RCT). For the purposes of this class, we define an RCT as a prospective study where assignment of participants to an intervention group or a control group is random. Other study designs lack either randomization or a comparison group or both. An RCT need not have a placebo group. Frequently, an RCT compares two or more active treatment groups. In some cases, the control group is the current standard-of-care, which is then compared to one or more experimental treatments.  Many people use the term clinical trial loosely when referencing a non-randomized study design; thus you cannot assume that a reference to a clinical trial is to a randomized controlled clinical trial. The distinctions between random sampling and random assignment can sometimes be difficult to grasp. 

Random sampling is the process of selecting study participants randomly from the population of interest. (To give a very old-fashioned example, random sampling could be done by drawing names out of a hat). Each person has the same chance of being selected for the study. This ensures that the study sample is representative of the population of interest. In randomized controlled clinical trials, however, random sampling is seldom used and convenience sampling is much more common. Participants are typically selected based on pre-defined inclusion and exclusion criteria and this selection process does not necessarily involve random sampling. 

Random assignment (also called randomization) is the process of assigning study participants randomly to treatment or control groups. (To give another very old-fashioned example, random assignment could be done by tossing a coin for each participant and assigning heads to treatment and tails to control). This ensures that the treatment and control groups are balanced and comparable in all characteristics (both known and unknown), so that any differences observed can be correctly attributed to the treatment.
Controlled trials are often conducted using some degree of blinding. Blinding is the act of keeping study personnel (participants, investigators, or assessors) unaware of the assigned treatment to promote objective assessment of outcomes and therefore reduce bias. In a placebo-controlled trial with blinding, the control participants receive a placebo that is designed to be indistinguishable from the active treatment.

Here are the typical categories of blinding:

In a single-blind study, the study participants do not know whether they are in the treatment or the control group.

In a double-blind study, both the study participants and the physicians/investigators are unaware of the treatment assignments. 

In a triple-blind study, the study participants, the investigators, and the data safety and monitoring board (DSMB) are all unaware of the treatment assignments. (Note that this study design usually involves having at least two statisticians: a DSMB statistician who is blinded and who assists the DSMB in making decisions about continuing the study, and a study statistician who created the randomization plan for the study and thus is not blinded.)An example of a randomized controlled clinical trial is the Multiple Risk Factor Intervention Trial, often abbreviated MRFIT and pronounced Mister Fit. In this trial, which began in 1971, 12,866 men with three specific risk factors for coronary heart disease (high blood pressure, high cholesterol and smoking) were randomly assigned to two groups. The special intervention group received special interventions intended to reduce the levels of the risk factors (for example, smoking cessation counseling) in addition to usual physician care, while the control or usual care group received usual physician care alone. Both groups were followed for six to eight years. The outcome or endpoint of interest was death from coronary heart disease.

The trial results appeared in the Journal of the American Medical Association in 19821. The investigators found that the risk factor levels declined in both groups, but they declined slightly more in the special intervention group. However, mortality due to coronary heart disease, as well as all-cause mortality, did not differ significantly between the two groups.

References:
1Multiple Risk Factor Intervention Trial:Risk Factor Changes and Mortality Results.JAMA.1982;248(12):14651477. doi:10.1001/jama.1982.03330120023025The primary advantage of randomized controlled clinical trials over observational studies is that RCTs provide the strongest available evidence for a cause-and-effect relationship between the intervention and the outcome, because the random assignment minimizes or eliminates any possible confounding. The RCT design minimizes confounding from both known and measured factors and from unknown factors, since the random assignment ensures that the treatment groups are comparable: any potential confounding variables, whether measured in the study or not, will be evenly distributed between the groups. It is extremely difficult or impossible to completely control for confounding in an observational study.

What is confounding, you say? Confounding is when the relationship between the treatment (or exposure) and the outcome is interfered with (or confounded by) a third variable. An example would be a (fictional) study of the relationship between coffee drinking and cancer. The study might observe that coffee drinkers are at higher risk of cancer, but this apparent relationship might really be due to smoking. If coffee drinkers are more likely to smoke, then the smoking could be responsible for the increased cancer risk, not the coffee. Smoking in this example would be a confounding factor.

The primary disadvantage of randomized controlled clinical trials is that they are time-consuming and expensive. There is still some potential for bias in a randomized trial, although the randomization itself removes many potential sources of bias. Procedure bias may occur when the treatment group receives more attention than the control group. Recall bias may occur when subjects in one group are more likely to remember events than subjects in another group. This would only cause bias if the outcome were self-reported. Compliance bias may occur when patients comply with one treatment more than another. When reading the medical or public health literature, pay careful attention to the description of the study design. It should provide enough information to enable the reader to answer these key questions of interest from a statistical perspective: 

Was the study observational or experimental? How were the data collected? 

For an observational study: What study design was used? Was random sampling used? If so, how was it implemented? What are some potential sources of bias?

For an experimental study: What study design was used? Was the study controlled? Was it blinded? What are the characteristics of the study sample? Was random assignment used? If so, how was it implemented? What are some potential sources of bias?

<!--chapter:end:7a-StudyDesign.Rmd-->

How many participants do I need for my study? This is one of the most common questions asked of statisticians. The purpose of this lecture is to help you understand what goes into estimating a sample size for a study. This understanding will be invaluable, regardless of whether you are going to do your own sample size calculations or consult a statistician for assistance. Whether you are designing your study independently or requesting assistance from your friendly neighborhood statistician, you likely will have one key question: How many participants do I need for my study? This is a key question because it strongly affects both the budget and the time required for the study and therefore its feasibility. Before getting to the actual sample size calculations, it is necessary to take a step back and address some key background questions about your study. This will ensure that the sample size calculations are appropriate to the situation. These questions include: 

What is the scientific or research question that you are trying to address with your study? It is important to clearly define this question, since the study design, conduct, and analysis all depend upon it. 

2) What population or group of people are you interested in studying?

3) What is the outcome of primary interest? You may be interested in a number of outcomes, but one of them needs to be designated as primary so that it can be used to estimate the needed sample size. It is also helpful to consider how the outcome will be measured and the variability of it.

4) What is the intervention or treatment or exposure of interest? For experimental studies, how exactly will the treatment be administered? For observational studies, how exactly is the exposure defined? Will there be a comparison group or groups? 

5) What study design are you thinking about using? How will the participants for the study be recruited? Will random sampling be involved? Will random assignment be used? How long will the participants be followed? How many of them are likely to drop out or be lost to follow-up?

Each of these questions may lead to additional questions, but this list is a good place to start. If you have requested assistance from your friendly neighborhood statistician in designing your study, here are a couple of things to keep in mind.

First, your statistician will be able to help you best if you involve them from the very beginning. The earlier, the better! Do NOT wait until after you have collected all of the study data to contact your statistician. If you do that, it may be too late to remedy flaws in the study design (such as being underpowered) or the study conduct.

Second, the first meeting will go much more smoothly if you provide your statistician with as much information about your study as possible in advance. If you have a draft grant application, a draft study protocol, or a draft manuscript related to this study, send it to your statistician in advance of the meeting to help them prepare.

Third, even though your primary question may be what sample size is needed for my study?, your friendly neighborhood statistician is likely to ask you a lot of probing background questions [as discussed on the previous slide] before they get to answering your questions. They are not doing this to be difficult nor to question your competence as a scientist/physician/professional. They are doing this to be able to give you the best possible advice about the study design and sample size.

References:
Clock: https://www.storyblocks.com/stock-image/alarm-clock-rwaf4on7u-j6gmgvcf
Laptop: https://pixabay.com/vectors/laptop-knowledge-information-1723059/
Brainstorming: https://www.storyblocks.com/stock-image/brainstorming-infographic-metaphor-with-line-icons-project-brainstorming-concept-for-website-and-infographics-vector-line-art-icon-isolated-on-white-background-hxv01inplbj5m958o1Once you (or your friendly neighborhood statistician) are satisfied that your study goals are clear and that your study plan is adequate to achieve those goals, it is time to focus in on the sample size needed for the study. This is where the typical four questions come in. 

How much variability is there in the primary outcome measurement? If the measurement is very precise, then it will be easier to see a difference between the groups. If the measurement has a lot of variability and is imprecise, then it will be difficult to see a difference between groups (just as it is difficult to distinguish nearby stars using a fuzzy low-resolution telescope). Other things being equal, the higher the outcome variability is, the larger the necessary sample size will be.

What is the smallest difference between the groups (i.e., the treatment effect size) that would be clinically or scientifically relevant? Other things being equal, the smaller the effect you are looking to find, the larger the necessary sample size will be.

What significance level (alpha) will you use for your study? The standard level is alpha=0.05. Recall, however, that there is a direct trade-off between Type I error (alpha), which is finding an effect or a difference that isnt real, and Type II error (beta), which is missing a real effect or difference. Reducing alpha from 0.05 to 0.01 essentially raises the bar for the study; that is, it requires stronger evidence of an effect or a difference. This makes it less likely that you will falsely conclude that there is a difference between the groups, but at the price of making it more likely that you will miss a real difference. For a given area of research, the optimal tradeoff between alpha and beta may vary from the standard levels. Other things being equal, the lower the significance level is, the larger the necessary sample size will be.

What power do you need for your study? (This question is tied to the previous one, since the power of a study is equal to 1  beta.) Power is the ability to see a real effect or difference and can be thought of as the sensitivity of the study. The standard power level for research studies used to be 0.80 or 80% power, but it is becoming increasingly common to require 0.90 or 90% power. Other things being equal, the higher the power is, the larger the necessary sample size will be.

All of this information will be used in the calculation to estimate a sample size for your study. 

In reality, the sample size for a study also depends on several additional factors, including the available budget, the available time (particularly if the participants need to be followed over time), and the ability to recruit participants (particularly for rare conditions). If the initial sample size estimate turns out to be completely unrealistic, given the practical constraints on the study, then the above series of questions will need to be revisited. In some cases, it may not be possible to address the scientific question of interest within the existing budget and time constraints. In such cases, it may be better to cancel the planned study, particularly if it involves human subjects. It is unethical to expose people to the risks of a study that does not have enough power to answer the scientific question of interest. So now youve answered the four questions either independently or in conversation with your statistician collaborator. In the latter case, your statistician will go off and develop a sample size estimate for your study. What do you do if you are working independently? We will discuss in detail only one simple situation, comparing means between two groups, in order to help you understand what factors affect sample size. In reality, sample size calculations are always done using statistical software.

Above is a standard sample size equation for comparing the means of a continuous outcome between two independent groups, such as a treatment group and a control group. This equation assumes the two groups have the same sample size. The sample size, n, that would be needed for the study would depend on four things:

The standard deviations, s1 and s2, of the outcome measurement in each group. This is a measure of the variability of the outcome measurement. The standard deviation goes in the numerator of the equation, which means that an outcome with more variability will require a larger sample size.

The minimum practically (or clinically) important difference, delta (?), between the two group means that we would like to be able to detect. This is sometimes abbreviated as the MCID. It is also sometimes called the minimum effect size. This is the difference between the null value of our hypothesis (0) and the value wed like to be able to declare as different. The delta value goes in the denominator of the formula, which means that a smaller effect will require a larger sample size to detect.

The alpha level we choose for our two-sided test, which comes in through the z*? (z alpha) term. This is the z-value from the Standard Normal distribution corresponding to an area of ?/2 in each tail. Common values for z*? are 1.645 for alpha=0.10, 1.96 for alpha=0.05, and 2.58 for alpha = 0.01.

The power we require for the study, which comes in through the z*? (z beta) term. If we would like our study to have a power of 0.90 or 90%, then we will set ? at 1  0.90 or 0.10. We then use the z-value from the Standard Normal distribution corresponding to an area of ? in the upper-tail. Common values for z*? are 0.84 for 80% power, 1.28 for 90% power, and 1.645 for 95% power. Note that z*? becomes larger as the power increases. This term is in the numerator of the formula, so a study with higher power (and larger z*? ) will require a larger sample size.Lets see how this formula would be used for a study to determine whether the mean mercury level in fish from Lake Superior differs from the mean level in Lake Michigan.

We want to know how many fish, n, we would need to sample (randomly, of course!) from EACH of the lakes to determine if the mean mercury level in the fish differs between the lakes. We will assume, for simplicity, that we will sample the same number of fish from each lake.

The outcome we are interested in is the mercury level in the fish. Lets suppose that weve done some pilot studies and we know that the standard deviation of mercury levels in fish in Lake Superior is roughly 0.4 ppm and in Lake Michigan is roughly 0.5 ppm.

The minimum effect size in this case is how much *difference* between the mean mercury levels in the two lakes we want to be able to detect. Lets say that we want to be able to tell if the mean levels differ by 0.1 ppm.

Lets assume as before that we want our study to have 90% power (so z*? = 1.28) and that we will use alpha = 0.05 in our two-sided test (so z*? = 1.96).

Putting all of these values into the equation (and remembering to round UP!) gives a minimum sample size of 431 fish in each group. We will need 862 fish in our study to have a 90% chance of detecting a mean difference as small as 0.1 ppm.

References: 
Fish: https://www.storyblocks.com/stock-image/trout-fish-jumping-spbzenvm_zj6gmkthtIn practice, sample size calculations are almost never done by hand. Some online sample-size calculators are listed on the course site. These are useful for doing quick ballpark calculations, or for double-checking other software. 

In your future career, you may encounter study design situations that are more complex than those discussed in this unit. Be sure to contact your friendly neighborhood statistician for assistance! Examples of situations that are beyond the scope of this course include: survival outcomes, correlated (non-independent) observations, unequal sample sizes in the groups, non-inferiority or equivalence studies, adaptive clinical trials, etc. Sample size and power calculations for some of these other situations will be covered in the second semester of this course, PubH 6451. For others, more advanced training in clinical trials and/or methods for correlated data would be needed.

<!--chapter:end:7b-SampleSize.Rmd-->

Many studies involve comparisons among more than two groups of participants. Examples of this include: 
Comparing types of women runners (nonrunners, recreational runners, or elite runners) on hormonal changes1,
Comparing how posture (dominant pose, neutral pose, or submissive pose) affects the perception of pain2, and
Comparing how smoking during pregnancy (nonsmokers, former smokers, smokers) relates to birth weights of infants3. 
If the outcome of interest is numerical, as described in these examples, ANOVA can be used to compare the mean outcomes among three or more groups. 

ANOVA is the abbreviated name for the Analysis of Variance method. This method was developed in the 1920s by R.A. Fisher, one of the founders of modern statistics. In fact, the ANOVA F-test is named after Fisher. 

Resources:
1Hetland, M. L., Haarbo, J., Christiansen, C., & Larsen, T. (1993). Running induces menstrual disturbances but bone mass is unaffected, except in amenorrheic women.The American journal of medicine,95(1), 53-60.
2Bohns, V. K., & Wiltermuth, S. S. (2012). It hurts when I do this (or you do that): Posture and pain tolerance.Journal of Experimental Social Psychology,48(1), 341-345.
3Brooke, O. G., Anderson, H. R., Bland, J. M., Peacock, J. L., & Stewart, C. M. (1989). Effects on birth weight of smoking, alcohol, caffeine, socioeconomic factors, and psychosocial stress.Bmj,298(6676), 795-801.If you were tasked with evaluating which mean outcomes are different among several groups, your first thought might be to use a two-sample t-test, multiple times, each time comparing one groups mean with another groups mean (pairwise comparisons). For example, if there were three groups, you would compare group 1 with group 2, group 1 with group 3, and group 2 with group 3. This strategy, however, comes with several problems. 

One problem with this approach is it is time consuming. An increasing number of tests are needed as the number of groups increases. There are: 

3 pairwise comparisons between 3 groups,
6 pairwise comparisons between 4 groups,
10 pairwise comparisons between 5 groups, and so on. 

The other problem with multiple t-tests is that the probability of making a Type I error increases as the number of tests increases. Generally speaking, if you calculate many p-values, some are likely to be small just by random chance, when in fact, there is nothing going on. So, if the probability of a Type I error, alpha, is set at 0.05 for each test and 10 t-tests are performed, the overall probability of a Type I error for the set of tests is 1  (0.95)^10 = 0.40 instead of 0.05. That is, if we perform 10 tests (with the null hypothesis being true in all cases), the chance that one or more of these tests will result in an incorrect conclusion of statistically significant is 40%. [Eeek!] 

We want to avoid this multiple comparisons issue of an increased probability of rejecting a true null hypothesis (that is, of mistakenly deciding that there is an effect when there isnt). The solution? ANOVA! Why? Because ANOVA compares all of the groups at once and reports only one p-value. It determines whether any of the group means are significantly different from the other group means. If the significance level is set at 0.05, the probability of a Type I error for the ANOVA F-test is capped at 0.05, regardless of the number of groups being compared. Researchers published a study in 2014 on the impact of concussions from playing collegiate football on hippocampus volume and cognitive outcomes1. The three different groups of interest, 25 students per group, were:

College football players with a history of diagnosed concussions. 
College football players with no history of diagnosed concussions.
Non-football-playing college students of similar age and education level (controls).

The researchers measured the hippocampus volume in microliters (L) one time via MRI for all groups. A boxplot and summary statistics for hippocampus volume in each group are presented above.

Based on visual evaluation of the plots, there is some overlap in the distributions of hippocampus volume in the three groups. The highest average hippocampus volume appears to be in the control group and the smallest appears to be in the football players who have had a concussion. All of the distributions appear to be approximately symmetric.

Is there evidence that at least one of the population means is different than the others? Lets use statistical inference methods to answer this question. 

References: 
1Singh, R., Meier, T. B., Kuplicki, R., Savitz, J., Mukai, I., Cavanagh, L., Allen, T., Teague, T.K., Nerio, C., Polanski, D. & Bellgowan, P. S. (2014). Relationship of collegiate football experience and concussion with hippocampal volume and cognitive outcomes. JAMA, 311(18), 1883-1888.
Dataset: Lock, R.H., Lock, P.F., Lock Morgan, K., Lock, E.F., & Lock, D.F. (2013). Statistics: Unlocking the power of data (1st ed.). Hoboken, NJ: John Wiley & Sons, Inc. The research question we will answer for this example is: Is there a difference in average hippocampus volume among football players with a history of concussion, football players without a history of concussion, and non-football-playing healthy controls? Lets evaluate the assumptions necessary for carrying out ANOVA. Notice that the first three assumptions are similar to those weve seen before. 

The samples should be random (or representative) samples from the respective populations, to allow us to generalize the results to those populations; 

The observations within each group should be independent of one another and the observations in one group should be independent of the observations in the other groups. 

The sampling distribution of the sample means for each group should be approximately Normal. Recall that we check this assumption in one of two ways. If the underlying population distributions for each group are approximately Normal (which we can check by plotting the sample distributions, one for each group), then the sampling distribution of the sample means will be approximately Normal. OR, if the underlying population distributions are not Normal but the sample sizes are large enough (and what is large enough depends on how heavily skewed the population distributions are), then the sampling distribution of the sample means will still be approximately Normal. If this assumption is not met, then other methods for carrying out inference for comparing three or more means (such as re-randomization tests) should be used instead. 

The variances of the outcomes are approximately equal across all of the groups. This is known as homoscedasticity. A rough rule is to check the standard deviation of the outcome measurement in each of the groups. If the largest standard deviation is no more than two times the smallest standard deviation, than this assumption is considered to be met. 

If these assumptions are not met, then the ANOVA results will not be valid. Lets check the assumptions for the Hippocampus Volume and Football example. 

Are the participants a random or representative sample of all college football players and non-football-playing college males? Based on the brief description of the dataset, it doesnt seem that the participants were randomly selected. Could they be considered representative of their respective groups? It seems reasonable to argue that the researchers would likely have chosen the participants to be representative of the groups of interest. 

Are the observations independent of one another? Again, without more information, it is reasonable to assume that the participants in each group are independent of each other and that the participants from one group are independent from participants in the other group. 

Are the conditions met for the sampling distribution of the sample means in each group to be approximately Normally distributed? Because the shapes of the sample distributions for each group do not appear to be severely skewed, the sample sizes of 25 may be large enough to assume the sampling distribution of the sample means in each group is approximately Normal. 

Lastly, are the variances across the groups approximately equal? Looking back at the table, we see that the largest standard deviation (scontrol = 1074) is not more than twice the smallest standard deviation (sFBConcuss = 593.4), so this assumption appears to be met.

References:
Image: https://www.pinclipart.com/pindetail/owRJbh_file-mw-icon-checkmark-svg-creative-commons-check/Now we define the hypotheses for the ANOVA test. 

The null hypothesis for ANOVA builds on the null hypothesis for a two-sample t-test: The population means for all groups are equal. In notation form, we write this as: the population mean for group 1, 1, is equal to the population mean for group 2, 2, is equal to the population mean for group 3, 3, and so on, all the way up to the population mean for group k, k, where k is the number of groups being compared.

The alternative hypothesis is that at least one group has a different population mean than the other groups. This doesnt mean that all of the groups population means are different from each other, just that the population mean for at least one group is different from the others. In notation form, we write this as: at least one population mean for group i, i, is not equal to the population mean for a different group j, j. 

Notice that the alternative hypothesis does not specify a direction for the difference, and so it only has one form  the two-tailed test. In addition, the alternative hypothesis does not state which two groups have different population means. ANOVA is only designed to test whether there is enough evidence that a difference exists somewhere. The question of which groups are different? gets answered after carrying out ANOVA (spoiler alert: by using post-hoc tests).For the Hippocampus Volume and Football example, the null hypothesis is that the population mean hippocampus volume is the same in all three groups. In notation, we can write FBConcuss equals FBNoConcuss equals Control.

The alternative hypothesis for this example is that at least one population mean hippocampus volume is different among the three groups.We evaluate the claims in a similar fashion as presented in previous lectures: 
We collect evidence (data), 
We summarize the data using exploratory data analysis (summary statistics, tables, graphs), and 
We calculate a test statistic to measure the compatibility between the result from the data and the null hypothesis. 

The test statistic when using ANOVA to compare means in three or more groups (given that the assumptions are met) is the F-statistic. The F-statistic is calculated as the ratio of the average variability between groups to the average variability within groups. You might be thinking at this point: ANOVA is used to compare means between three or more groups, so why are we comparing measures of variability? It comes down to wanting a single measure that reflects how far apart the means are for all groups, while taking into account the variability observed within the groups. Lets examine some plots to better understand the idea behind the F-statistic.The figure above shows boxplots for hypothetical data comparing three groups in three different situations, given by Datasets A, B, and C.

In Dataset A, the group means are fairly close together, but the within-group variability is quite high. In Dataset B, the group means are similar to those in Dataset A, but the within-group variability is much smaller. In Dataset C, the group means are much further apart than in Datasets A and B, but the within-group variability is again quite high, similar to Dataset A.

Now pause and think: in which dataset is there the strongest visual evidence for a difference between the group means, and in which dataset is there the weakest evidence?

Datasets B and C show the strongest evidence for a difference in means between the three groups, whereas Dataset A shows the weakest evidence. For Dataset A, there is quite a bit of overlap between the three sample distributions (as indicated by the boxplots), so the three samples could easily have been taken from the same population. For Dataset B, even though the means are close together (relative to Dataset C), there is very little overlap between the three sample distributions. And finally, for Dataset C, as for Dataset A, there is quite a bit of overlap between the three sample distributions (as indicated by the boxplots), even though the means are much further apart, so the three samples could (again) easily have been taken from the same population. 

As we were assessing the plots, notice what was assessed to determine if the groups means were different. The two pieces of variability that were evaluated were:
the variability between groups: how far the group means were from each other, AND 
the variability within groups: how spread out the sample distributions were. 
This tells us that just knowing the groups means isnt enough. We can detect small differences between the means if the values within each group are close together (as in Dataset B). In contrast, we CANNOT detect even large differences between the means if the values within each group are widely spread out (as in Dataset C). This is the basic idea of ANOVA. When carrying out ANOVA, a table is calculated that summarizes the different pieces of the total variability. The terminology and notation will differ from software to software and between resources, but the general set up for the rows of the ANOVA table will be the same. The first row summarizes the between group variability, the second row summarizes the within group variability, and the third row summarizes the total variability (if one is provided). The variability is calculated similarly to the sample standard deviation, by adding up squared deviations. That is, we have some sort of deviation, we square that deviation value, and then add up all those squared terms. The term for this type of calculation is sum of squares (abbreviated SS). The sum of squares for between group and within group add up to the sum of squares for the total.

The equations are presented on this slide, but are tedious and error-prone to carry out by hand. We typically rely on software to produce the ANOVA table. However, each row will be described so you have a general understanding of what each cell means.For the between group row, the sum of squares between groups (denoted as SSG) is a measure of how far apart the group means are from each other. The steps for calculating SSG are: for each group, calculate the deviation of that group mean from the overall grand mean (the mean for all data combined), square that deviation, and weight that squared deviation by the sample size in that group. You then add up those weighted squared deviations for all of the groups. If the group means are close to each other, the variability between the groups, SSG, will be small. If the groups means are farther apart from one another, SSG will be larger.

For the within groups row, the sum of squares within groups (denoted as SSE, where E stands for error) is a measure of how much variability there is within each group. This is the variability we cant explain by the differences between groups and therefore is referred to as error or residual variability. The steps for calculating SSE are: for each observation, calculate the deviation of that observation from its group mean and square that value. You then add up those squared deviations for all of the observations in all of the groups. If there is a lot of variability (or noise) within groups, SSE will be large.

 Now, we want to compare the two pieces of variability: SSG and SSE. These two measures are not directly comparable since SSG measures variability between k means and SSE measures variability using all n observations. Thus, we need to put the two on comparable scales by dividing by their degrees of freedom. The degrees of freedom associated with SSG is k 1. For SSE, it loses one degree of freedom for each group mean, so if there are k means, we are left with n k degrees of freedom. [Note that the degrees of freedom for the between group and within group add up to the total degrees of freedom.]

The result of calculating SS/df is called a mean square, which is just a fancy word for variance. We have a mean square for groups (MSG) and a mean square for error (MSE). Comparing these two measures of variability, the between group variability (MSG) to the within group variability (MSE), gives us our F-statistic. This test statistic has a F-distribution with two parameters for the degrees of freedom: k  1 degrees of freedom for the numerator (MSG) and n  k degrees of freedom for the denominator (MSE). 

Similar to previous test statistics weve discussed, this F-statistic measures how far the sample result from the study is from what we would expect (given sampling variability) IF the null hypothesis were really true. We would expect the two mean squares, MSG and MSE, to be roughly similar if the null hypothesis (no difference) were really true. Alternatively, if the population means really differed, we would expect MSG to be larger relative to MSE. What does an F-distribution with parameters df1 and df2 look like? 

Since the F-statistic is a ratio of variances, the F-distribution can only take on positive values. (Recall that variances cant be negative.) Three different F-distributions are shown in the figure above. Notice that the F-distribution is not symmetric, but rather is positively skewed. The shape of the F-distribution varies depending on both the numerator and the denominator degrees of freedom. As the number of groups being compared increases, the F-distribution shifts to the right and becomes more Normal looking.The data from the Hippocampus Volume and Football example were summarized using plots and summary statistics earlier in this presentation. We noticed that the sample means for each group were different and that the control group had a larger average hippocampus volume than the other two groups. But do these results provide evidence that at least one population mean hippocampus volume is different among the groups?

To answer this question, we produce an ANOVA table (via software!). The F-statistic for this example is 31.47, which means that the variation between the three group means is about 31.5 times bigger than we would expect under the null hypothesis, based on the variability within the groups. This test statistic has a F-distribution, with 2 and 72 degrees of freedom.If our null hypothesis were really true and there was no difference between the groups population means, then the sampling distribution of the F-statistic would follow a F-distribution with 2 and 72 degrees of freedom, as shown in the plot above. Our F-statistic value of 31.47 lies in the upper (right) tail of this distribution. This value seems fairly unlikely to occur, but how likely or unlikely is it? Lets quantify this probability. The evaluation piece of the hypothesis framework is the same as that from previous lectures. 

Recall that we quantify how unusual the evidence is compared to what is assumed to be true by computing a p-value. The p-value is the probability that you would obtain a result this unusual if the null hypothesis were really true and any observed difference was simply due to sampling variability. To put it another way, its the probability of getting our sample result (or one even more extreme) if the null hypothesis were true. In ANOVA, the p-value is how much area is in the upper tail of the F-distribution. The smaller the p-value, the less consistent or compatible the data are with the null hypothesis. 

Once we have the p-value, we make a conclusion about the strength of evidence we have against the claim by comparing the p-value to the significance level, alpha. If the p-value is less than alpha, we reject the null hypothesis and say we have evidence against the null in favor of the alternative hypothesis. Another way of saying this is that the result is statistically significant. Alternatively, if the p-value is greater than alpha, then we do not reject the null hypothesis and we say we lack evidence against the null. This result would not be considered statistically significant. In either case, remember to always state the conclusion in the context of the problem. Lets evaluate the evidence for the Hippocampus Volume and Football example. 

We are interested in the probability of seeing a F-statistic of 31.47 or more extreme if the null hypothesis were really true. When doing ANOVA, the p-value is the area in upper tail of the F-distribution. The p-value for this example is 1.5 x 10^-10 (in scientific notation), which is a really, really, really small value. For a significance level set to 0.05, we reject the null hypothesis because the p-value is less than 0.05. Therefore we conclude that we have evidence that at least one of the groups population means differs from the rest. 

You may be wondering at this point, which groups are different? We will explore this topic, called post-hoc tests, in the next lecture. Here are a few additional notes about ANOVA. 

First, if we carried out an ANOVA test with only two groups, this would be exactly equivalent to carrying out a two-sample t-test with equal variance assumption. The F-statistic for comparing two means is equal to the square of the two-sample t-test statistic. 

Second, ANOVA can be used both for balanced designs, when the number of observations is the same in each group, and for unbalanced designs, when the number of observations is different in each group. With a balanced design, the test will have somewhat more power and the test statistic will be somewhat more robust to departures from the equal variance assumption, but the test is equally applicable to unbalanced designs.  

Lastly, what should you do if some of the assumptions for ANOVA are not met? 

If the normality assumption is in question, ANOVA has been shown to be quite robust to deviations from the normality assumption and the results will still be valid in most cases, provided that the sample sizes in the groups arent too small. Another option might be to transform the outcome variable (for example, using a log transformation) and see if the transformed outcome is closer to Normal.

If the equal variances assumption is in question, ANOVA has also been shown to be pretty robust to deviations from this assumption as well, provided that the sample sizes are roughly equal between the groups. 

Other options include nonparametric techniques, such as simulation-based techniques (such as bootstrapping or re-randomization tests) or rank-based tests (such as the Kruskal-Wallis test), which require fewer assumptions.

<!--chapter:end:9a-ANOVA.Rmd-->

This lecture (finally!) discusses how to answer the question of which groups are different? following a significant ANOVA result. We could simply carry out a number of pairwise t-tests to compare the mean outcome between each possible pair of groups, in order to find out which groups differ. However, as we pointed out earlier, this can involve a large number of tests, especially as the number of groups gets larger. If each pairwise test is conducted using a Type I error rate (alpha) of 0.05, and there are many tests, then the overall chance of making a Type I error becomes inflated to a much larger value than 0.05. The multiple comparisons that we carry out will lead to an inflated Type I error rate, which is undesirable. We want the probability of a Type I error to stay at the specified alpha level, regardless of the number of comparisons we make. That is, we want the familywise error rate (the probability of a Type I error among the entire family of comparisons) to remain at the specified alpha level. 

This is where so-called post-hoc tests come into play. Many approaches have been proposed to avoid Type I error inflation due to multiple comparisons after ANOVA. Each one deals with the Type I error rate in a different way. Two of these methods, Bonferroni correction and Tukeys HSD, will be discussed in more detail in this lecture; some (though certainly not all) of the other tests are briefly described here. 

The Bonferroni correction is the simplest method to understand and apply. However, it is more conservative than other approaches. This method is recommended when you want to compare only a few preselected pairs of means. 

Tukeys HSD procedure is recommended when you want to test all possible pairwise differences of means. 

Dunnetts test compares the mean from each group to the mean of a control group rather than carrying out all of the possible pairwise comparisons between the groups. Because it makes fewer comparisons, it generates narrower confidence intervals and has more power to detect differences between groups. 

Holms test is a powerful and versatile approach. It is a modification and upgrade to the Bonferroni correction, meaning it is designed to be more powerful than the original. However, it only provides conclusions about which comparisons are and are not statistically significant and doesnt provide confidence intervals for the differences. 

Scheffs test is more flexible in the types of comparisons made. This is the preferred method when more elaborate comparisons are of interest, such as comparing the combined mean of all intervention groups with the mean of the control group or comparing the mean of Groups A and B with the mean of Groups C, D, and E. However, this flexibility comes at a price. It will have less statistical power than other methods. To apply the Bonferroni correction to control the familywise error rate, one adjusts by the total number of comparisons (K) that are being made. In effect, we are saying that we have K comparisons to make and we dont want the overall Type I error rate to exceed alpha, so we will allocate 1/Kth of alpha to each test. For example, if we had 3 comparisons to make, we would allocate 1/3 of alpha to each test.

This adjustment can be carried out in one of two ways: adjusting the alpha level and comparing the p-values from each of the pairwise t-tests to that adjusted alpha, OR adjusting the p-values from each of the pairwise t-tests and comparing them to the original alpha. [Note: Because ANOVA assumes equal variance, these pairwise t-tests also use the equal variance assumption.] These two approaches are equivalent and lead to the same conclusions, but different software packages may use one or the other approach.

For example, suppose that a study has four groups (labeled A through D) and the ANOVA test result was significant. Now we want to carry out all possible pairwise comparisons to see where the difference lies. In this case, there would be six possible comparisons between the four groups (A to B, A to C, A to D, B to C, B to D and C to D). 

If we use the adjusted alpha approach (assuming alpha = 0.05), then we calculate an adjusted alpha by dividing alpha by the 6 comparisons, giving an adjusted alpha value of 0.05/6 = 0.0083. Then, when we carry out the pairwise t-tests, a test result is only declared statistically significant when its p-value is less than the adjusted alpha value of 0.0083. Essentially, the 5% significance level applies to the entire family of comparisons rather than to each of the 6 individual comparisons. 

On the other hand, if we use the adjusted p-value approach, then we calculate an adjusted p-value for each of the pairwise t-tests by multiplying the original p-value by 6. Each adjusted p-value is then (assuming alpha = 0.05) compared to 0.05. 

A confidence interval can also be calculated using the Bonferroni method. The calculation looks identical to the CI for a difference in means (with the equal variance assumption) but uses the Bonferroni adjusted alpha to find the t* value. Lets apply the Bonferroni correction to the Hippocampus Volume and Football example and figure out which groups are different from another. 

First, we carry out two-sample t-tests (with the equal variance assumption) to test for differences in means for each possible pair of groups. These p-values are presented in the top table on the slide.

If we are using the adjusted alpha approach, each of the p-values from the three tests are compared to the adjusted alpha level of 0.05/3 = 0.0167. All three p-values are smaller than the adjusted alpha level.

If we are using the adjusted p-value approach, each of the three p-values are multiplied by 3 and then compared to the alpha level of 0.05. These adjusted p-values are presented in the bottom table on the slide. All three adjusted p-values are smaller than alpha=0.05.

Regardless of which approach we use, based on the results presented in the tables, there is strong evidence of a difference in means between all three groups. Another common multiple comparisons approach is the Tukeys HSD method. If you are curious, the HSD stands for Honestly Significant Difference. To control for the familywise error rate, this method utilizes a distribution called the studentized range distribution (q). The equations presented for Tukeys HSD method look similar to the usual t-test and confidence interval equations, but just adjusted to account for multiple comparisons. 

The Tukeys HSD confidence intervals have the familiar form: point estimate +/- margin of error, where margin of error is some degree of confidence times the standard error for the estimate. 

For the Tukeys HSD confidence interval, since we are comparing two means, the point estimate is the difference between the two sample means. 

The estimated standard error for the Tukeys HSD CI is computed from the pooled SD of all the groups (because of the equal variance assumption in ANOVA) and the sample sizes for the two groups of interest, as shown on the slide. The pooled SD value is actually the square root of the pooled variance, which is labeled mean square error (MSE) in the ANOVA table.  

Lastly, the degree of confidence for the Tukeys HSD CI is the q* value, which has a studentized range distribution with two degrees of freedom parameters, df1 = k (total number of groups) and df2 = n  k (total number of observations  total number of groups). 

The Tukeys HSD q-test statistic also has the familiar form: a point estimate divided by the standard error of the estimate. Again, as above, since we are comparing two means, the point estimate is the difference between the two sample means, and the estimated SE is computed from the pooled SD of all the groups and the sample sizes for the two groups of interest.For the Hippocampus Volume and Football example, the table above shows the difference in mean hippocampus volume between each pair of groups, 95% confidence intervals for the differences, calculated using Tukeys HSD method, and p-values for the differences, also calculated using Tukeys HSD method. Both the CIs and the p-values lead us to conclude that all three groups differ in hippocampus volume.

We strongly encourage you to look beyond just the inferential results (CIs and p-values) when making your conclusions about a study. Look back at the summary statistics to help tell your story. Even if the results are statistically significant, are the differences observed meaningful or relevant? Do the direction and magnitudes of the differences make sense?

In this example, the controls (the healthy non-football-playing college students) have the highest average hippocampus volume, followed by the college football players with no history of diagnosed concussions, and then by the college football players with a history of diagnosed concussions. 
Based on this observational study, there is evidence of a relationship between playing football and hippocampus volume, especially when concussions are involved.Multiple testing or multiple comparisons issues arise in many other contexts beyond ANOVA. Multiple testing adjustments are mandatory in genomics (where a study may test for differences in thousands of genes or SNPs) or imaging (where a study may be looking for differences among thousands of locations in the brain). Adjustments should be considered when comparing multiple subgroups, for example in secondary analyses in a clinical trial, or when developing regression models potentially involving hundreds or thousands of potential combinations of dozens of predictor variables of interest.

It isnt always required or even reasonable to adjust for multiple comparisons, depending on the purpose of the study or the field of research, but it is always necessary to describe how the testing process was carried out, so the reader can be aware of the possible Type I error inflation. Carrying out large numbers of tests in a search for a significant result is often termed data torture or p-hacking and is considered unethical.

<!--chapter:end:9b-PostHocTests.Rmd-->


This lecture (finally!) discusses how to answer the question of “which groups are different?” following a significant ANOVA result. We could simply carry out a number of pairwise t-tests to compare the mean outcome between each possible pair of groups, in order to find out which groups differ. However, as we pointed out earlier, this can involve a large number of tests, especially as the number of groups gets larger. If each pairwise test is conducted using a Type I error rate (alpha) of 0.05, and there are many tests, then the overall chance of making a Type I error becomes inflated to a much larger value than 0.05. The multiple comparisons that we carry out will lead to an inflated Type I error rate, which is undesirable. We want the probability of a Type I error to stay at the specified alpha level, regardless of the number of comparisons we make. That is, we want the familywise error rate (the probability of a Type I error among the entire family of comparisons) to remain at the specified alpha level. 

This is where so-called post-hoc tests come into play. Many approaches have been proposed to avoid Type I error inflation due to multiple comparisons after ANOVA. Each one deals with the Type I error rate in a different way. Two of these methods, Bonferroni correction and Tukey’s HSD, will be discussed in more detail in this lecture; some (though certainly not all) of the other tests are briefly described here. 

The Bonferroni correction is the simplest method to understand and apply. However, it is more conservative than other approaches. This method is recommended when you want to compare only a few preselected pairs of means. 

Tukey’s HSD procedure is recommended when you want to test all possible pairwise differences of means. 

Dunnett’s test compares the mean from each group to the mean of a control group rather than carrying out all of the possible pairwise comparisons between the groups. Because it makes fewer comparisons, it generates narrower confidence intervals and has more power to detect differences between groups. 

Holm’s test is a powerful and versatile approach. It is a modification and “upgrade” to the Bonferroni correction, meaning it is designed to be more powerful than the original. However, it only provides conclusions about which comparisons are and are not statistically significant and doesn’t provide confidence intervals for the differences. 

Scheffé’s test is more flexible in the types of comparisons made. This is the preferred method when more elaborate comparisons are of interest, such as comparing the combined mean of all intervention groups with the mean of the control group or comparing the mean of Groups A and B with the mean of Groups C, D, and E. However, this flexibility comes at a price. It will have less statistical power than other methods. To apply the Bonferroni correction to control the familywise error rate, one adjusts by the total number of comparisons (K) that are being made. In effect, we are saying that we have K comparisons to make and we don’t want the overall Type I error rate to exceed alpha, so we will allocate 1/Kth of alpha to each test. For example, if we had 3 comparisons to make, we would allocate 1/3 of alpha to each test.

This adjustment can be carried out in one of two ways: adjusting the alpha level and comparing the p-values from each of the pairwise t-tests to that adjusted alpha, OR adjusting the p-values from each of the pairwise t-tests and comparing them to the original alpha. [Note: Because ANOVA assumes equal variance, these pairwise t-tests also use the equal variance assumption.] These two approaches are equivalent and lead to the same conclusions, but different software packages may use one or the other approach.

For example, suppose that a study has four groups (labeled A through D) and the ANOVA test result was significant. Now we want to carry out all possible pairwise comparisons to see where the difference lies. In this case, there would be six possible comparisons between the four groups (A to B, A to C, A to D, B to C, B to D and C to D). 

If we use the adjusted alpha approach (assuming alpha = 0.05), then we calculate an adjusted alpha by dividing alpha by the 6 comparisons, giving an adjusted alpha value of 0.05/6 = 0.0083. Then, when we carry out the pairwise t-tests, a test result is only declared ”statistically significant” when its p-value is less than the adjusted alpha value of 0.0083. Essentially, the 5% significance level applies to the entire family of comparisons rather than to each of the 6 individual comparisons. 

On the other hand, if we use the adjusted p-value approach, then we calculate an adjusted p-value for each of the pairwise t-tests by multiplying the original p-value by 6. Each adjusted p-value is then (assuming alpha = 0.05) compared to 0.05. 

A confidence interval can also be calculated using the Bonferroni method. The calculation looks identical to the CI for a difference in means (with the equal variance assumption) but uses the Bonferroni adjusted alpha to find the t* value. Let’s apply the Bonferroni correction to the Hippocampus Volume and Football example and figure out which groups are different from another. 

First, we carry out two-sample t-tests (with the equal variance assumption) to test for differences in means for each possible pair of groups. These p-values are presented in the top table on the slide.

If we are using the adjusted alpha approach, each of the p-values from the three tests are compared to the adjusted alpha level of 0.05/3 = 0.0167. All three p-values are smaller than the adjusted alpha level.

If we are using the adjusted p-value approach, each of the three p-values are multiplied by 3 and then compared to the alpha level of 0.05. These adjusted p-values are presented in the bottom table on the slide. All three adjusted p-values are smaller than alpha=0.05.

Regardless of which approach we use, based on the results presented in the tables, there is strong evidence of a difference in means between all three groups. Another common multiple comparisons approach is the Tukey’s HSD method. If you are curious, the HSD stands for Honestly Significant Difference. To control for the familywise error rate, this method utilizes a distribution called the studentized range distribution (q). The equations presented for Tukey’s HSD method look similar to the usual t-test and confidence interval equations, but just adjusted to account for multiple comparisons. 

The Tukey’s HSD confidence intervals have the familiar form: point estimate +/- margin of error, where margin of error is “some degree of confidence” times the standard error for the estimate. 

For the Tukey’s HSD confidence interval, since we are comparing two means, the point estimate is the difference between the two sample means. 

The estimated standard error for the Tukey’s HSD CI is computed from the pooled SD of all the groups (because of the equal variance assumption in ANOVA) and the sample sizes for the two groups of interest, as shown on the slide. The pooled SD value is actually the square root of the pooled variance, which is labeled “mean square error” (MSE) in the ANOVA table.  

Lastly, the degree of confidence for the Tukey’s HSD CI is the q* value, which has a studentized range distribution with two degrees of freedom parameters, df1 = k (total number of groups) and df2 = n – k (total number of observations – total number of groups). 

The Tukey’s HSD q-test statistic also has the familiar form: a point estimate divided by the standard error of the estimate. Again, as above, since we are comparing two means, the point estimate is the difference between the two sample means, and the estimated SE is computed from the pooled SD of all the groups and the sample sizes for the two groups of interest.For the Hippocampus Volume and Football example, the table above shows the difference in mean hippocampus volume between each pair of groups, 95% confidence intervals for the differences, calculated using Tukey’s HSD method, and p-values for the differences, also calculated using Tukey’s HSD method. Both the CIs and the p-values lead us to conclude that all three groups differ in hippocampus volume.

We strongly encourage you to look beyond just the inferential results (CIs and p-values) when making your conclusions about a study. Look back at the summary statistics to help tell your story. Even if the results are statistically significant, are the differences observed meaningful or relevant? Do the direction and magnitudes of the differences make sense?

In this example, the controls (the “healthy” non-football-playing college students) have the highest average hippocampus volume, followed by the college football players with no history of diagnosed concussions, and then by the college football players with a history of diagnosed concussions. 
Based on this observational study, there is evidence of a relationship between playing football and hippocampus volume, especially when concussions are involved.Multiple testing or multiple comparisons issues arise in many other contexts beyond ANOVA. Multiple testing adjustments are mandatory in genomics (where a study may test for differences in thousands of genes or SNPs) or imaging (where a study may be looking for differences among thousands of locations in the brain). Adjustments should be considered when comparing multiple subgroups, for example in secondary analyses in a clinical trial, or when developing regression models potentially involving hundreds or thousands of potential combinations of dozens of predictor variables of interest.

It isn’t always required or even reasonable to adjust for multiple comparisons, depending on the purpose of the study or the field of research, but it is always necessary to describe how the testing process was carried out, so the reader can be aware of the possible Type I error inflation. Carrying out large numbers of tests in a search for a significant result is often termed ‘data torture’ or ‘p-hacking’ and is considered unethical.
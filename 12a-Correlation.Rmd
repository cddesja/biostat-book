Thus far in this course, we have discussed statistical methods for comparing means of a numerical variable between two or more groups (using t-tests, confidence intervals, or ANOVA) and for comparing two categorical variables (using confidence intervals for relative risks or odds ratios, Chi-square tests, or FisherÕs exact tests). This week, we move on to examining methods for comparing two numerical variables. This lecture will focus on how to graphically depict (via scatterplots) and numerically summarize (via correlation coefficients) the relationship between two numerical variables.Recall the Mercury Content in Fish example from a previous lecture. The average mercury content in fish in a lake is actually rather difficult to measure, in practice. It would be great if researchers could use some other value that is more easily obtained from a water sample, such as acidity, to give information about mercury content. The researchers from that study also collected data on water acidity and alkalinity from the 53 Florida lakes. Is there a relationship between mercury level in fish and acidity? Because the two variables are quantitative, we can display this relationship by creating a scatterplot (as shown above). 

Before presenting the formal approach for evaluating a scatterplot, what do we notice from the plot itself? It appears that as acidity level increases, mercury level in fish tends to decrease for lakes. What other things should we examine when we look at a scatterplot? 

References:
Study: Lange, T. R., Royals, H. E., & Connor, L. L. (1994). Mercury accumulation in largemouth bass (Micropterus salmoides) in a Florida lake. Archives of Environmental Contamination and Toxicology, 27(4), 466-471.
Dataset: Lock, R.H., Lock, P.F., Lock Morgan, K., Lock, E.F., & Lock, D.F. (2013). Statistics: Unlocking the power of data (1st ed.). Hoboken, NJ: John Wiley & Sons, Inc. When evaluating a scatterplot, we assess four features of the plot:

Form. Do the points form a trend? If so, do they appear to follow a straight line (which we call linear association), or some other curve or pattern? 

Direction. Do the points generally have an upward direction or downward direction when we look from left to right? Or, do the points just appear to be randomly scattered (imagine a splat of paint on a canvas)? An upward direction is called a positive association and a downward direction is called a negative association. 

Strength. How closely do the points adhere to some imaginary trend line? If they appear to have a lot of scatter, we say there is a weak association. If they appear to be really close to the trend line, we say there is a strong association. This can be difficult to judge based on visual clues alone, but with practice and experience, you can start to develop a sense for ÒweakÓ vs. ÒstrongÓ relationships in plots. 

Outliers. Are there any points that are striking deviations from the overall pattern? LetÕs apply the four features to the Mercury Content in Fish and Water Acidity scatterplot.

It appears that the relationship between average mercury and acidity for lakes is a moderate, negative, linear association with no obvious outliers. 

After visual inspection of the relationship, if the relationship appears to be roughly linear, we can calculate a correlation coefficient to numerically describe the linear relationship between two numerical variables.
The correlation coefficient quantifies the strength and direction of a linear association between two quantitative variables. The equation for computing the correlation coefficient, r, is presented on the slide. LetÕs dissect the formula to better understand how it works. 

LetÕs start with a particular point or observation in the dataset. For that observation, we calculate its z-score for the x-variable and its z-score for the y-variable. Recall that a z-score will be positive if the value is above the average (for X or Y) and will be negative if it is below the average (for X or Y). We then multiply the two z-scores together to see how they co-vary. Three things could happen: 
If the x- and y-values for the observation are both above their averages, both z-scores will be positive and the product of the z-scores will be positive; or, 
If the x- and y-values are both below their averages, both z-scores will be negative and the product of the z-scores will still be positive; or, 
If the x- and y-values are in opposite directions (e.g., higher than average x-value and lower than average y-value), one of the z-scores will be positive and the other will be negative, resulting in a negative product. 

We then (1) calculate the z-scores for X and Y for all observations in the dataset, (2) multiply the z-scores for each observation, (3) add up all of the products, and (4) divide by n-1. Essentially, the correlation coefficient is the average of the product of the z-scores (for X and Y). [Recall that average is just Òadding up a bunch of somethingsÓ, and Òdividing by how many somethings we haveÓ.] To put it another way, on average, how do the z-scores for X and Y co-vary or relate? 

If the two variables, X and Y, relate in the same way for every single observation, then the correlation coefficient will be +1 or -1, with positive indicating that they co-vary in the same direction and negative indicating that they co-vary in the opposite direction. That is, r will equal +1 or -1 when there is an exact linear relationship between x and y. If there is no apparent linear relationship between the variables (i.e., no linear pattern at all), then the correlation will be near zero. The plots on the slide provide a visual reference for how the scatterplot would look for a particular correlation coefficient value. Here are some things to notice as you are examining the plots:

The correlation coefficient, r, can take on values from -1 to +1. 

The sign of r (positive or negative) indicates the direction of the association. 

As previously mentioned, looking at the magnitude of the value (and not the sign) indicates the strength of the association; that is, how tightly the points are clustered around the imaginary linear trend line. Although there is no consensus on what constitutes a ÒstrongÓ or ÒweakÓ association, here are some rough guidelines:
If the absolute value of r is greater than or equal to 0.7 (i.e., |r| > 0.7), that indicates a strong association.
If the absolute value of r is between 0.3 and 0.7 (i.e., 0.3 <|r| < 0.7), that indicates a moderate association.
If the absolute value of r is less than 0.3 (i.e., |r| < 0.3), that indicates a weak association.

The correlation coefficient is unit-less. 

The correlation coefficient is symmetric. That is, for correlation, it doesnÕt make any difference which variable you label as X and which as Y. The correlation between variables X and Y is the same as the correlation between Y and X. (However, this is not the case for regression, as you will see in a future lecture). 

Reference:
Images: https://www.mathsisfun.com/data/correlation.htmlWhat is the correlation coefficient for the Mercury Content in Fish and Water Acidity example? The value turns out to be -0.575. 

The negative sign indicates a negative association between the two variables. A negative association means that as one variable increases, the other one decreases. In this example, as lake acidity increases, the average mercury content in a lakeÕs fish decreases. 

The magnitude of the value tells us that there is a moderate association between the two variables. 

Both of these aspects were also gleaned from our visual inspection of the plot, but the correlation coefficient can be helpful to provide an objective assessment of the relationship, as well as to compare with relationships from other studies. Here are a few additional notes about the correlation coefficient.

The first thing you should do when you have to carry out any type of analysis is plot the data! This is especially true when it comes to correlation. The correlation coefficient only provides information about the linear association between two numerical variables (if any). So even if the relationship between two variables is curved, it is still possible to calculate a correlation coefficientÉ but it will be meaningless. For example, a correlation near 0 does not necessarily mean that the two variables are not associated Ð they may have a strong nonlinear or curved relationship. Lesson to the wise: Look before you leap when you interpret the r value. 

To emphasize this point, letÕs look at a famous example, called AnscombeÕs quartet1. All four datasets have nearly identical correlation coefficients (r = 0.816), but their scatterplots, shown on the slide above, look very different. You can see in the upper-right plot that there is a perfect curved relationship between the two variables, yet merely looking at the correlation coefficient, r, gives you no indication of this. The correlation coefficient provides meaningless information regarding this nonlinear relationship. 

For the bottom two plots, we see that the correlation coefficient is being influenced by outliers. In the bottom left plot, there is a nearly perfect linear relationship, but the one outlier is influencing r and lowering the value from 1 to 0.816. In the bottom right plot, the majority of the points indicate there isnÕt any relationship between the x and y variables, but one high outlier is enough to result in a high correlation coefficient. This brings us to the next note about correlationÉ

Reference: 
Anscombe, F.J. (1973). Graphs in statistical analysis. American Statistician, 27,  17-21. 
Plot: https://www.flickr.com/photos/113716728@N08/11806491624/, courtesy of Eric-Jan Wagenmakers.The correlation coefficient is influenced by outliers. Changing or excluding points that are potential outliers can change the results. Therefore, r is not robust to outliers, especially outliers that lie to the far right or far left relative to the other points. This is another reason why you should look before you leap when interpreting the r value. However, you should not instantly assume that potential outliers are Òbad pointsÓ that are just Òmessing upÓ the analysis. Investigate to see if they are actual mistakes (such as typos) in the data. If not, they may be real observations that indicate that the relationship between X and Y is not as simple as you had hoped. Real observations should never be excluded from an analysis without a really good reason. It is possible that the potential outliers are the most interesting and information-filled observations in the study. ?
The correlation coefficient that was discussed in this lecture is called the PearsonÕs correlation coefficient. When people refer to the correlation (or more appropriately, the correlation coefficient), they are most often referring to this one. However, there are other kinds of correlation coefficients that quantify the relationship between other kinds of variables. For example, the SpearmanÕs rank correlation coefficient measures the relationship between two variables that are ordinal. 

Lastly, a correlation coefficient that indicates a strong association between two variables does not necessarily imply a cause-and-effect relationship. Possible explanations for why two variables strongly co-vary include: 
X causes Y
Y causes X
There is a third variable that causes both X and Y.
X, Y, and all other factors are all part of a complex system and the observed correlation is just a ÒpeekÓ at this much more complicated set of relationships. 
X and Y are not related at all, and the observed correlation is just a coincidence. (We would need inference to address this question). 
Most people immediately think of the first two possibilities but ignore the rest. ItÕs worth also considering the other possibilities when interpreting correlation coefficients. 
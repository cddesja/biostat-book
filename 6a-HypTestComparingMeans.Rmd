In previous lectures, we discussed using confidence intervals and hypothesis testing to make inferences about a single group’s mean. 

This lecture focuses on using hypothesis testing to compare means from two independent groups (for example to compare treatment versus control groups, or males versus females). As before, we recognize that in a given study, the difference in means between two groups, such as treatment and control groups, is unlikely to be exactly zero even if there is no treatment effect in the population. The difference will vary a little bit around zero from one study’s sample to the next study’s sample simply due to sampling variability. The question is, is the difference we observe in this study sufficiently large that it is unlikely to be due merely to sampling variability when there is actually no difference between the two groups? Here’s an interesting two group scenario. The health benefits of drinking tea have been known for centuries. However, research on the immunologic effect of tea was missing. In 2003, researchers published an article1 to try to fill that gap in scientific knowledge. They studied the effect of drinking tea on the production of interferon gamma, which is a molecule that helps the immune system fight bacteria, viruses, and tumors. 

The researchers recruited 21 healthy individuals who did not normally drink tea or coffee and randomly assigned them to either the tea group (n=11) or the coffee group (n=10). The tea group was asked to drink five to six cups of black Lipton tea per day for two weeks (equivalent to roughly 600 ml/day) and the coffee group was asked to drink five to six cups of instant Nescafe coffee per day for two weeks. Coffee was chosen as the comparison treatment because it also has caffeine but does not have the L-theanine that is in tea. Volunteers were allowed to add sugar, lemon, or cream to suit their taste. After two weeks, blood samples were collected and the interferon gamma production was measured. Plots of this data are presented above. 

The summary statistics for the production in the coffee group are: 
Mean = 17.7, SD = 16.7, Median = 15.5, Q1 = 5.0 , Q3 = 21.0

The summary statistics for the production in the tea group are: 
Mean = 34.8, SD = 21.1, Median = 47, Q1 = 15.5, Q3 = 53.5

Based on this, we can see that the mean interferon gamma production is higher in the tea group than the coffee group. But could this large of a difference be the result of sampling variability or is this evidence of a difference? Let’s use statistical inference methods to answer this question.

References: 
1Kamath, A. B., Wang, L., Das, H., Li, L., Reinhold, V. N., & Bukowski, J. F. (2003). Antigens in tea-beverage prime human V?2V?2 T cells in vitro and in vivo for memory and nonmemory antibacterial cytokine responses. Proceedings of the National Academy of Sciences, 100(10), 6009-6014.
Dataset: Lock, R.H., Lock, P.F., Lock Morgan, K., Lock, E.F., & Lock, D.F. (2013). Statistics: Unlocking the power of data (1st ed.). Hoboken, NJ: John Wiley & Sons, Inc. 
As with any study, we begin with a research question. Since the research question drives the entire research process, it is essential to define it clearly at the outset of a study. 

The research question for the Tea, Coffee, and the Immune System study is: is there an effect of drinking tea (compared to coffee) on the production of interferon gamma?  Before we can carry out any inferential method, we first need to evaluate the assumptions of the method. The assumptions behind hypothesis tests and confidence intervals for comparing means in two groups are the same as those we’ve seen before, except now we need to evaluate them for each group. That is: 

The samples should be random (or representative) samples from the respective populations, to allow us to generalize the results to those populations; 

The observations within each group should be independent of one another (as discussed in earlier lectures). IN ADDITION, the observations in one group should be independent of the observations in the other group. This assumption would be violated if the observations in one group were related in some way to those in the other group: for example, if the participants in the two groups were matched or paired. If the observations are not independent, then methods for correlated data should be used instead.

The sampling distribution of the difference in sample means between the two groups should be approximately Normal. Statistical theory (which is beyond the scope of this course) tells us that if two sample means are normally distributed, then the difference of the two means will also be normally distributed. So this assumption simply requires us to be sure that the sampling distributions of the sample means in each group are approximately Normal. How can we check this assumption? We can check to see if the conditions for the Central Limit Theorem (CLT) hold. If the underlying population distributions for each group are approximately Normal (which we can check by plotting two sample distributions, one for each group), then the sampling distribution of the sample means will be approximately Normal. OR, if the underlying population distributions are not Normal but the sample sizes are ”large enough” (and what is “large enough” depends on how heavily skewed the population distributions are), then the sampling distribution of the sample means will still be approximately Normal. If this assumption is not met, then other methods for carrying out inference for comparing two means (such as re-randomization tests) should be used instead. 

If these assumptions are not met, then the results of the inferential methods (CIs and hypothesis tests) will not be valid. 
Let’s check the assumptions for the Tea, Coffee, and the Immune System example. 

Are the healthy participants a random or representative sample of all healthy adults? Based on the brief description of the dataset, it doesn’t seem that the participants were randomly selected. Could they be considered representative of all healthy adults? It seems reasonable to argue that the researchers would likely have chosen the participants to be representative of all healthy adults. 

Are the observations independent of one another? Again, without more information, it is reasonable to assume that the participants in each group are independent of each other and that the participants from one group are independent from participants in the other group. 

Lastly, are the conditions met for the sampling distribution of the difference in sample means to be approximately Normally distributed? The sample sizes are small so it’s hard to tell from the sample statistics and plots whether the data for each group are bell-shaped or not. Because the shapes of the sample distributions for each group do not appear to be severely skewed, the sample sizes of 10 and 11 may be ”large enough” to assume the sampling distribution of the difference in means is approximately Normal. 




References:
Image: https://www.pinclipart.com/pindetail/owRJbh_file-mw-icon-checkmark-svg-creative-commons-check/The next step is to define the hypotheses for the test. We write them in terms of the population parameters of interest – the population mean for group 1, µ1, and the population mean for group 2, µ2.

Recall that the null hypothesis defines the skeptical perspective or the “no difference” situation. When comparing two groups on a continuous variable, it is written as: the population mean for group 1, mu1, is equal to the population mean for group 2, mu2, or with a little rearranging, the difference in the two population means (mu1 – mu2) equals 0. 

The alternative hypothesis, on the other hand, defines the competing claim, the thing we are interested in finding evidence for. 

Just as we saw before, there are three different forms the alternative hypothesis can take. 
If we have written our null hypothesis as µ1 = µ2, then our alternative hypothesis can be written as either:
µ1 is not equal to µ2 (two-tailed test), or
µ1 is greater than µ2 (one-tailed test), or
µ1 is less than µ2 (also a one-tailed test). 

If we have written our null hypothesis as µ1 - µ2 = 0, then our alternative hypothesis can be written as either:
µ1 - µ2 is not equal to zero (two-tailed test), or
µ1 - µ2 is greater than zero (one-tailed test), or
µ1 - µ2 is less than zero (also a one-tailed test). 

As always, the alternative hypothesis is defined by the research question.Let’s define the hypotheses for the Tea, Coffee, and the Immune System example. 

The null hypothesis is that the true difference in mean interferon gamma production between the tea and coffee groups is 0, or, to put it another way, the true mean production is the same for the tea and coffee groups. Using notation, we can write µT (subscript T for the tea group) equals µC (subscript C for the coffee group), or µT minus µC equals 0. 

Because the researchers were interested in whether there was a difference, with no indication of the direction of the difference, the appropriate alternative hypothesis is that the true difference in mean interferon gamma production between the tea and coffee groups is not 0, or, the true mean production is not the same for the tea and coffee groups. Using notation, we can write µT does not equal µC, or µT minus µC does not equal 0. This is a two-sided alternative hypothesis.We evaluate the claims in a similar fashion as presented in a previous lecture: 
We collect evidence (data), 
We summarize the data using exploratory data analysis (summary statistics, tables, graphs), and 
We calculate a test statistic to measure the compatibility between the result from the data and the null hypothesis. 

The test statistic when comparing two means (given that the assumptions are met) is similar to the one used to carry out a test for a single mean: it is a t-test statistic. This test is often referred to as a two-sample t-test or unpaired t-test. The formula for the two-sample t-test statistic looks similar to the one for the one-sample t-test statistic we used before: we use the sample estimate (this time, it’s the difference in the two sample means) minus the null value, all divided by the standard error for that estimate. Written out, it is the difference between the two sample means (x-bar in group 1 minus x-bar in group 2) minus 0 (the null value), divided by the square root of the standard deviation in group one (s1), squared, over the sample size in group 1 (n1) , plus the standard deviation in group 2 (s2), squared, over the sample size in group 2 (n2). Again, this value measures how far the sample result from the study is from what we would “expect” (given sampling variability) IF the null hypothesis was really true. Large values of the t-statistic represent large (relative) differences between the sample result and the null value and small values represent small (relative) differences. 

The formula for the degrees of freedom for this two-sample t-test statistic, shown on the slide above, is slightly more complex than that for a one-sample t-test. If we were carrying out this analysis in software, we would just let the software calculate the more precise degrees of freedom. Without software, we can calculate the approximate degrees of freedom by using the smaller of the two values: n1 – 1 or n2 – 1.For the Tea, Coffee, and the Immune System example, data were collected and summarized using plots and summary statistics earlier in this presentation. The sample mean, sample standard deviation, and sample size for the tea and coffee groups are presented again here for reference. The difference in mean interferon gamma production between the tea group and the coffee group was 17.1. Would this difference in means be unusual if the true difference between the groups were really zero?

To answer this question, we compute a t-test statistic using the data and the null value. The t-test statistic for this example is 34.8 minus 17.7, all divided by the square root of 21.1 squared over 11 plus 16.7 squared over 10. Solving this out results in a t-test statistic of 2.07. This test statistic has a t-distribution, where the degrees of freedom is the smaller of nT – 1 = 11 – 1 = 10 or nC – 1 = 10 – 1 = 9 degrees of freedom: df = 9. If our null hypothesis were really true and there was no difference between the population means of the two groups, then the sampling distribution of the t-test statistic would follow a t-distribution with 9 degrees of freedom, as shown in the plot above. Our t-test statistic value (or t-value) of 2.07 lies in the upper (right) tail of this distribution. This value seems fairly unlikely to occur, but how likely or unlikely is it? Let’s quantify this probability. Recall that we quantify how unusual the evidence is compared to what is assumed to be true by computing a p-value. The p-value is the probability that you would obtain a sample difference this “unusual” if the null hypothesis were really true and any observed difference was simply due to sampling variability. To put it another way, it’s the probability of getting our sample result (or one even more extreme) if the null hypothesis were true. As before, what counts as “extreme” depends on the alternative hypothesis. The smaller the p-value, the less consistent or compatible the data are with the null hypothesis. 

Once we have the p-value, we make a conclusion about the strength of evidence we have against the claim by comparing the p-value to the significance level, alpha. If the p-value is less than alpha, we reject the null hypothesis and say we have evidence against the null in favor of the alternative hypothesis. Another way of saying this is that the result is “statistically significant”. Alternatively, if the p-value is greater than alpha, then we do not reject the null hypothesis and we say we lack evidence against the null. This result would not be considered statistically significant. In either case, remember to always state the conclusion in the context of the problem. Let’s evaluate the evidence for the Tea, Coffee, and the Immune System example. 

With a t-value of 2.07 and an alternative hypothesis that is two-sided, we are interested in the probability of seeing a t-value of 2.07 or more “extreme” if the null hypothesis were really true. The t-value could be more “extreme”, which means further from zero, in either direction: either further above +2.07 or further below -2.07. Therefore, the probability we are looking for is the area in both tails of the t-distribution, highlighted green in the plot above. To find this, we calculate the area under the t-distribution curve to the right of +2.07 and add to it the area to the left of -2.07, which is the mirror image of the upper portion of the distribution. Alternatively, because the t-distribution is symmetric, we can just calculate the area under the curve to the right of +2.07 and multiply this value by 2. The p-value for this example is 0.068. For a significance level (alpha) set to 0.05, we fail to reject the null hypothesis because the p-value of 0.068 is greater than 0.05. Therefore we conclude that we did not find evidence of a significant difference in mean interferon gamma production between the tea and coffee groups. The data from this sample showed some evidence that tea drinkers might have higher mean interferon gamma production than coffee drinkers, but the evidence is not very strong. The information presented on two-sample t-tests thus far has assumed that the variability in the two groups is different. However, there may be situations when one might reasonably assume that the two populations have the same, or nearly the same, variances (or standard deviations), even if their means are distinct. If there is a reasonable argument for this strong assumption of equal variances, then a pooled variance can be calculated as shown above. {The term “pooled” refers to the fact that we are using the data from both groups “pooled” together to estimate the variance.} If the assumption is valid, then the pooled variance provides a better estimate of the true variance because it uses a larger sample of data (from both groups combined).

This pooled variance would be substituted into the standard error formula shown earlier in this presentation, by replacing both (s1)2 and (s2)2 with (spooled)2 and then simplifying to give the SEpooled formula shown above. The associated degrees of freedom for the updated t-test statistic is n1 + n2 – 2. 
As we wrap up the topic of t-tests, here are some additional notes about these tests. 

To pool or not to pool the standard deviations? This topic is controversial in statistics. There are hypothesis tests to test whether the population variances are equal, but they are not very powerful; in addition, if the assumptions of those tests are not met, then the results will not be valid. There are also rules of thumb to gauge whether the two variances are approximately equal, but these are based on experience and not on theory. One textbook1 claims that pooling offers almost no advantage over the unequal variance case. Therefore, we recommend using the more conservative approach of not pooling the standard deviations.

Recall that one of the assumptions of the t-test is that the data come from a population which is approximately Normal. Furthermore, one way to check this assumption is to plot the data and judge by eye whether the distribution looks approximately bell-shaped. (Note that this check doesn’t work very well if the sample size is small.) What should we do if we find that the data don’t look very normal? Good news! The t-test procedure is robust to even fairly large deviations from normality and the results will still be valid in most cases, provided the sample sizes aren’t too small. 

Lastly, here’s a fun historical fact2 about the t-test procedure. It was devised by William Gosset in the early 1900’s to monitor the quality of Guinness beer. Because Guinness didn’t allow their employees to publish their findings, he published under the pseudonym “Student” and so the procedure was known for many years as “Student’s t-test”.

References: 
1Lock, R.H., Lock, P.F., Lock Morgan, K., Lock, E.F., & Lock, D.F. (2013). Statistics: Unlocking the power of data (1st ed.). Hoboken, NJ: John Wiley & Sons, Inc. 
2Wikipedia contributors. (2019, July 29). Student's t-test. In Wikipedia, The Free Encyclopedia. Retrieved 16:52, July 30, 2019, from https://en.wikipedia.org/w/index.php?title=Student%27s_t-test&oldid=908375594
Simple linear regression, which was covered in a previous lecture, allows us to describe and test the linear relationship between a numerical outcome and one numerical predictor. It also allows us to predict a value of the outcome based on a single numerical predictor. However, we know that the world is a complex place and multiple variables could serve as potential predictors. Enter multiple linear regression!

Multiple linear regression (MLR) is an extension of simple linear regression (SLR), and many of the ideas from SLR carry over into MLR. Most often, the goal of multiple linear regression is to understand how combinations of several variables are simultaneously related to the outcome. The underlying idea behind adding predictors to the model is to try to explain the part of Y that has not already been explained by the single numerical predictor. This lecture is to serve as an introduction to multiple linear regression. The second course in this sequence, PubH 6451: Biostatistics II, will provide a much thorough presentation of this topic. 

We are aware that the example used in this lecture, the BetterBirth study on perinatal morbidity and mortality, may be sensitive to some. This particular example was chosen because it is part of an important, and needed, area of research in public health and medicine. We, as biostatisticians, may also have emotional moments when we deal with data on a difficult topic, but we feel called to help people by using our gifts of being analytical and precise and careful in our work to further research that will improve their lives.Recall the conceptual notation that was presented in a previous lecture: 

Response = Trend + Random. 

In simple linear regression, we wrote this as: the value of Y for a particular subject from the population is given by an intercept (beta0) plus a slope (beta1) times the X value for that subject, plus some random normally-distributed error or random noise, denoted by epsilon (?). Remember that the error term is added to describe the random scatter of the points about the true line: it tells us that the Y values for individuals in the population deviate from the regression line by the distance ?. Because the error is random, we made assumptions about the behavior of the randomness.

For multiple linear regression, the model notation is similar, but now there are several predictors. To properly denote this, the regression coefficients and the predictors (X’s) are given subscripts. The first predictor is denoted X1, the second predictor is denoted X2, and so on, up to k predictors. [Note: Order of predictors doesn’t matter.] Up until this point, we have only considered numerical predictors. Fortunately, regression models can accommodate both numerical and categorical predictors. Categorical predictors are typically incorporated into the model by recoding the categories as 0 or 1. These kind of variables are called indicator or ‘dummy’ variables. In this lecture, we will only deal with binary predictors. 

As before, we say that the value of Y for a particular subject in the population is given by an intercept (beta0) plus a coefficient, beta1, times the X1 value for that subject, plus a coefficient, beta2, times the X2 value for that subject, and so forth for all of the predictors (k of them), plus some random normally-distributed error or random noise, denoted by epsilon (?). The error term is interpreted in the same manner as in SLR, that is, as the random scatter of the points about the true line. We again make assumptions about the behavior of the randomness of the error term. Because these population coefficients are unknown to us, we use the coefficients determined from the sample (b0, b1,…, bk) to make an inference about the population coefficients (beta0, beta1,…, betak). 

Just as in SLR, least-squares regression is used in MLR to find values for all of the coefficients that result in the best fit for the observed data, that is, that minimize the sum of the squared residuals. The math behind the multiple regression model involves solving systems of equations in matrix algebra. In this course, you do not need to worry about how this works, but only about how to interpret the results obtained from statistical software.For simple linear regression, we can easily plot and visualize the (X, Y) data and the regression line in a two-dimensional (2D) scatterplot. It becomes more difficult to plot the data for multiple linear regression, since there are more than two dimensions. 

In the case of two predictors, where one predictor is numerical and the other is binary, we could plot a 2D scatterplot, and denote the value of the categorical variable by changing the color or shape or size of the points (see the plot on the left-hand-side of the slide). In this situation, the regression model would allow us to obtain two separate lines, one for each group. 

Or, in the case of two predictors that are both numerical, we could conceivably plot the data in three dimensions (X1, X2, and Y), and the regression model would be a plane (instead of a line) in three-dimensional space (see the plot on the right-hand-side of the slide). The least-squares estimation approach in this case involves minimizing the distance of the observed data points from the best-fit plane. 

If there are more than two predictors, we would need to plot the data in k + 1 dimensional space, which is beyond human capacity. So we often don’t create scatterplots in multiple linear regression, beyond the numerical + binary predictor situation. The example we will use in this lecture is the BetterBirth Study. 

An important research area in public health is improving maternal and child health locally and globally. One approach is to study ways to improve the quality of care provided at health-care facilities. One study investigated using a checklist-based strategy in an area that was largely rural and had one of the highest rates of maternal and newborn mortality in India and globally. 

A matched-pair, cluster-randomized controlled study was carried out to try to reduce severe maternal, fetal, and newborn harm in Uttar Pradesh, India. Sixty primary and community health-care facilities were assigned to the intervention arm, which involved coaching-based implementation of a checklist-based program, and 60 matched control sites were also enrolled at the same time. The results of the study indicated that the intervention did not reduce perinatal mortality, maternal morbidity, or maternal mortality. A post-hoc analysis1 was carried out to further investigate any relationships of the outcomes (perinatal mortality, maternal morbidity, or maternal mortality) with facility-level attributes. [Note: We are going to be examining a very small subset of the predictors that were actually used in the post-hoc analysis, for educational purposes.]

Suppose we were interested in examining the relationship between female literacy, at the district-level, and perinatal mortality, after accounting for study arm. A scatterplot of the relationship, with the points colored by study arm (coaching-based intervention is orange and control is blue), is shown on the slide. What do we notice from the plot? It appears that as female literacy at the district-level increases, the perinatal mortality tends to decrease for facilities. Also, the study arm dots appear to be mixed together in the plot, suggesting that there isn’t a difference between the study arms when it comes to the relationship of perinatal mortality and female literacy. 

Let’s now fit a model to understand the impact of female literacy at the district-level on perinatal mortality, after accounting for study arm. 

References: 
1Delaney, M. M., Miller, K. A., Bobanski, L., Singh, S., Kumar, V., Karlage, A., Tuller, D.E., Gawande, A.A., & Semrau, K. E. (2019). Unpacking the null: a post-hoc analysis of a cluster-randomised controlled trial of the WHO Safe Childbirth Checklist in Uttar Pradesh, India (BetterBirth). The Lancet Global Health, 7(8), e1088-e1096.
Dataset: https://doi.org/10.7910/DVN/GNJBAO The MLR model can be defined as shown in the equation on the slide. For the BetterBirth Study, the outcome Y is the perinatal mortality for a facility (which will be denoted as PM), with predictors X1 for proportion of female literacy (at the district-level) (which will be denoted as FL) and X2 for study arm (X2 = 1 for coaching-based intervention site and 0 for control site). Let’s figure out what the parameters of this model tell us. 
?Beta0 is the intercept; that is, it is the average value of Y when all of the X’s equal 0. For the BetterBirth Study, the intercept can be interpreted as the model-predicted average value of perinatal mortality when female literacy is 0 and when study arm is 0 (control group). This is not very useful because a female literacy value of 0 was not observed in our data (and it’s not a very realistic number). 
?Beta1 is how much higher (or lower) the average of Y is for each one-unit increase in X1, keeping X2 fixed or constant. For the BetterBirth Study, this would be how much higher (or lower) the average of perinatal mortality is for each one-unit increase in female literacy, holding study arm constant (or after accounting for study arm). 

Notice that this is a similar slope interpretation as in SLR, but this time, we have to add an extra part to denote that we are “accounting for” or “adjusting for” the other variables in the model. We have to do this with all of the regression coefficients and can do so using a variety of phrases. We could say “after adjusting for…” or “after accounting for…” or “holding all other variables constant.”
?Beta2 has the same interpretation as beta1; that is, how much higher (or lower) the average of Y is for each one-unit increase in X2, keeping X1 fixed or constant. However, since X2 is a categorical variable coded as 0 or 1, a ”one-unit increase” represents switching from one category to the other. So it would be how much higher (or lower) the average of Y is when comparing X2 = 1 to X2 = 0. For the BetterBirth Study, this would be how much higher (or lower) the average of perinatal mortality is for intervention facilities compared to control facilities, after accounting for female literacy . 

Because there is a binary predictor in the model, we can obtain the regression line for each of the binary categories, one for control arm and one for the intervention arm. We do this by substituting X2 = 1 in the regression equation for the intervention arm and X2 = 0 for the control arm. When we plug X2 = 0 into the equation, we obtain a regression line for the control facilities with an intercept of beta0 and a slope of beta1 times female literacy (see box on the left-hand-side of the slide). When we plug X2 = 1 into the equation, we obtain a regression line for the intervention facilities with an intercept of beta0 plus beta2  and a slope of beta1 times female literacy (see box on the right-hand-side of the slide). Notice that the regression lines for the two arms have different intercepts, but the same slope for female literacy. 

It should be noted that this model assumes that the association of female literacy with perinatal mortality (that is, the slope for female literacy) is the same for both study arms, and that the association of study arms with perinatal mortality (that is, the offset due to study arm) is the same for all levels of female literacy. In other words, this model assumes that there is no interaction between female literacy and study arms. We will come back to this point later in this lecture. The results from fitting the MLR model to our data are shown on the slide. The estimated regression coefficients are in the column labeled “Estimate”. Let’s interpret the values of the regression coefficients. 

For facilities in the same study arm, for every 1-unit increase in female literacy, perinatal mortality is expected to decrease by 0.13. Similarly, for facilities with the same female literacy value, perinatal mortality is expected to be 0.0007 less in the intervention arm compared to the control arm. 

For the BetterBirth Study post-hoc analyses, the interpretation of the study arm coefficient was not of primary importance. The study arm predictor was added to the model to take that variable into account or adjust for it when examining other predictors. Focusing on one or a few predictors in regression models is very common in research, unless you are doing an exploratory analysis. Some predictors are placed in the model so the analysis can properly adjust for the effects of them, while focusing on the relationship between a subset of the predictors and the response variable.A plot of this model is shown above. Now there are two regression lines: the orange line is for the intervention arm and the blue line is for the control arm. 

The two lines have the same slope, -0.1331, which tells us that average perinatal mortality declines by about 0.13 for every additional unit of female literacy. 

The two lines are offset from each other: the line for the intervention arm is 0.0007 below the line for the control arm at any female literacy value, which tells us that for any given female literacy value, an intervention facility will have about 0.0007 lower perinatal mortality than a control facility, on average. Just as with SLR, a question we might have once we obtain the MLR regression results is, “How well does the model fit the data?” This can again be assessed using the coefficient of determination, but rather than using the multiple R2 value (which is analogous to the R2 value reported in SLR), we use the adjusted R2 value. Why? Well, the multiple R2 value will always increase (get closer to 1.0) as each new predictor is added to the model, even if the new predictor isn’t related to the outcome at all. (This is called “overfitting” or “fitting noise”.) In contrast, the adjusted R2 value takes the number of predictors into account and only increases if the newly added predictor is expected to make the model fit future data better than the model without it. The adjusted R2 value is always less than the multiple R2 value.

In the BetterBirth Study example, how much of the variability in perinatal mortality is explained by the predictors after adjusting for the number of predictors in the model? The adjusted R2 value is 15.77%. Let’s add another predictor to the model to see if we can explain some of that remaining variation. The results from fitting an MLR model with three predictors is shown on the slide. The other predictor that was added to the model was the average years of experience by birth attendants at the facilities (BA Experience). Let’s interpret the regression coefficient for this new predictor. 

For facilities in the same study arm and with the same female literacy value, for every 1-year increase in average years of experience of the birth-attendants, average perinatal mortality is expected to decrease by 0.0005. 

We can see that this model fits the data slightly better than when BA Experience wasn’t in the model, as the adjusted R2 increased from roughly 16% to 17%, but the increase probably isn’t enough to say that it fits MUCH better. 

Just as before, we know that summary statistics will vary from sample to sample, so we are interested in knowing: Could the relationships we observe be the result of sampling variability or is this evidence of real relationships? Let’s use statistical inference methods to answer this question. 

For the remainder of the analysis, we will just be focusing on the coefficients for female literacy and BA experience, as those were of primary interest to the researchers. The study arm predictor was placed in the model to properly adjust for the effects of it but was not of primary interest.The hypotheses for multiple linear regression are similar to simple linear regression, except we have one for each regression coefficient. There are k+1 coefficients we could test (one for the intercept and k coefficients, one for each predictor), but, as previously mentioned, we are often really interested in one or a few of them (depending on our research question). The test(s) of interest are most likely about the coefficient(s) for the predictors and not about the intercept. Also, the difference between MLR and SLR is that the individual tests for each coefficient assess the importance/effectiveness of each predictor after the other predictors are in the model. 

Recall that the null hypothesis defines the skeptical perspective or the “nothing is going on” situation. When we are looking to see if a predictor is associated with the outcome (conditional on the other variables in the model), the skeptical perspective is that they aren’t associated at all or they are independent of one another (given the other predictors in the model). For multiple linear regression, this would occur if the regression coefficient for that predictor equals 0. 

The alternative hypothesis, on the other hand, defines the competing claim, the thing we are interested in finding evidence for. In this case, the alternative hypothesis is that the predictor and the outcome are indeed associated with or related to each other (given the other predictors in the model) and therefore, are not independent of one another. We would write the alternative hypothesis for multiple linear regression as the regression coefficient for that predictor does not equal 0. We evaluate the claims in the usual way: 
We collect evidence (data), 
We summarize the data using exploratory data analysis (multiple linear regression line, adjusted R2),
We calculate a test statistic as a measure of the compatibility between the result from the data and the null hypothesis, and
We can also calculate a confidence interval that provides a plausible range of values the population parameter can take.

We calculate the test statistic and confidence interval for a regression coefficient in the same way as in SLR. Recall that the general t-statistic formula is the sample estimate minus the null value, all divided by the standard error for that estimate. Written out for multiple linear regression, it is the sample coefficient (betaj) minus 0 (the null value), divided by the standard error of that coefficient. We can compute a test statistic for each coefficient in the model. The equation for the standard error of the regression coefficient is a little ugly (and time-consuming to carry out by hand), so we will rely on software to compute the standard error for us and leave the explanation for a future statistics course. This test statistic has a t-distribution with n – k – 1 degrees of freedom. [Note: The df = n – k – 1 because we are estimating k + 1 parameters (coefficients) in this model: the k coefficients and the intercept.] 

Confidence intervals are an alternative method for summarizing the evidence provided by the data. Recall that the general formula for a confidence interval is the point estimate plus or minus the margin of error. Filling in these details for estimating a population regression coefficient:

--The point estimate is the coefficient estimated from the sample. 

--The margin of error is the degree of confidence, which is calculated from the appropriate t-value (which is NOT the same as the t-test statistic) times the standard error for the coefficient. The appropriate degrees of freedom for the t-value is n – k – 1 and the standard error for this situation is the SE for the coefficient (obtained via software). For the BetterBirth Study example, data were collected and summarized using an MLR model and adjusted R2 (shown on a previous slide). Let’s answer the question of: Would the individual regression coefficient that we observed be unusual if the true coefficient was really zero in this model?

To answer this question, we compute a t-test statistic for each coefficient using the model-estimated coefficient, its standard error, and the null value. The t-test statistic for female literacy in this model is -0.1275 minus 0, all divided by 0.0270. Solving this out results in a t-test statistic of -4.73 (which we can see in the output from our software). The t-test statistic for BA experience in this model is -0.0005 divided by 0.0003, which equals -1.81. These test statistics each have a t-distribution, where the degrees of freedom are 120 – 3 – 1 = 116.

We can also compute a confidence interval for the coefficients in this model using information in the output. The sample coefficient for female literacy in this model is -0.1275 and the standard error for the slope is 0.0270. The t-value for a 95% confidence interval is the value in the t-distribution with 116 degrees of freedom and 0.975 area lying below that value, so the t-value is 1.981. Putting all of those values together–the point estimate, the t-value, and the standard error–a 95% confidence interval for the true coefficient for female literacy in this model is -0.181 to -0.074. We could carry out a similar calculation for the the coefficient for BA experience. The last step in our inferential framework is to make a decision about how usual or unusual the evidence is compared to the claim about the population parameter. Recall that we do this by quantifying how unusual the evidence is compared to what is assumed to be true by computing a p-value. Once we have the p-value, we make a conclusion about the strength of evidence we have against the null hypothesis by comparing the p-value to the significance level, alpha. If the p-value is less than alpha, we reject the null hypothesis and say we have evidence against the null in favor of the alternative hypothesis. For a significance level (alpha) set to 0.05, we reject the null hypothesis for testing the coefficient for female literacy because the p-value is less than 0.05, but fail to reject the null hypothesis for the coefficient for BA experience because the p-value is greater than 0.05. Therefore we conclude that female literacy is useful in this model to predict perinatal mortality, but BA experience is not an effective predictor in this model. What are the assumptions when carrying out inference for multiple linear regression? They are the same as in simple linear regression, and even use the same diagnostic plots to assess them. As review, they are:

The sample should be a random (or representative) from the population of interest, to allow us to generalize the results to that population; 

The data should show a linear trend. We assess this by looking at a residuals vs. fitted values plot and determining if there is random scatter of the points (or a big ”blob”). If the points are randomly dispersed around the horizontal axis at 0, a linear regression model is appropriate for the data. If the points appear to trend in some way, a different model may be more appropriate.

The observations should be independent of one another. That is, whether one point is above or below the line does not influence whether another point is above or below the line. We assess this through how the data were sampled and collected. This assumption would be violated if the observations were related in some way to other observations: for example, if the participants were matched or paired, or if they were related in some way to each other (e.g., members of the same family). If the observations are not independent, then methods for correlated data should be used instead.

The residuals are nearly Normal in their distribution. We can check this assumption by creating a histogram of the residuals and evaluating the shape of the distribution (Normal? Skewed?). We can also used a QQ plot of the residuals and examine how close the points follow a linear pattern. The more closely the points follow a linear pattern, the closer the distribution is to a Normal distribution. A diagonal line is often included in the plot as a visual aid to assess how well the points follow a linear trend. If this assumption is not met, then other procedures for carrying out inference for multiple linear regression (such as transformation or nonparametric methods) should be used instead. 

The variability of the points around the best-fit line is roughly constant. We assess this assumption by using a residual vs. fitted values plot. We look to see if the residuals have consistent variability above and below the horizontal line at 0 as we move across the x-axis, and there doesn’t appear to be any “fanning” or “funneling” trend. If this assumption is not met, then other procedures for carrying out inference for multiple linear regression (such as transformation of X and/or Y data values or weighted least squares linear regression) should be used instead. 

If these assumptions are not met, then the MLR results will not be valid. Let’s use diagnostic plots and the description of the data to check the assumptions for the BetterBirth Study example. The slide displays the residual plot and the QQ plot. 

Are the health care facilities a random or representative sample of all facilities in that area in India? Based on the brief description of the dataset, the facilities were not a random sample of all facilities in that area. Could they be considered representative of all health care facilities in that area? It seems reasonable to argue that the researchers would likely have chosen the facilities to be representative of all facilities in that area.

Examining the residual plot, the points seem to be a random scatter above and below the horizontal axis, or a big “blob”, with no obvious fanning or funneling trend. Based on this, it appears that the linearity and equal variance assumptions are met for this example. 

Based on the brief description of the dataset, it is reasonable to assume that the facilities are independent of each other.

Lastly, examining the QQ plot, the residual values appear to generally follow a linear trend, which suggests that the residuals are approximately normally distributed. 

Putting it all together, for the BetterBirth Study example, all of the assumptions for linear regression appear to be met, so using an MLR model to determine the relationship between perinatal mortality and female literacy, BA experience, and study arm, simultaneously, is appropriate. This lecture on multiple linear regression was just a taste of the topic. For a more in-depth understanding of using multiple linear regression, take the second course in this sequence: PubH 6451: Biostatistics II. For those who are only taking this course, here are some notes to be aware of when using MLR.

As previously mentioned, the implication of the model that was presented (the additive model) is that the impact of each predictor on the outcome is the SAME for every value of all other predictors. But, what if the intervention has a bigger impact on perinatal function in districts with higher female literacy than in districts with lower female literacy? If this were the case, there would be an interaction between female literacy and study arm. It is possible to include an interaction term in the model (and the researchers did just that in their article) but this topic is left for the second course in this sequence. 

Multiple linear regression can incorporate any number of predictors, and their interactions, within limits dictated by the sample size of patients available for the analysis. Larger studies have more participants and can therefore test more scientific questions (more model parameters) and will have more power to detect differences. There are many statistical methods for helping us determining what is the “best model”, and it may not always be the most complicated one. These methods are under the umbrella of model selection. Choosing a final regression model is somewhat of an art that often requires a good deal of experience. This topic will be covered in the second course in this sequence. 

Avoid adding predictors to the model that are highly associated with other predictors, a phenomenon known as multicollinearity. When this is done, the correlated predictors convey essentially the same information, so one of the correlated terms becomes unnecessary and redundant and the model results can often become unstable. For example, if we were trying to predict blood pressure and entered weight and height as predictors, we would have encountered multicollinearity, because people who are taller also tend to be heavier. Once one is known (e.g., height), also knowing the other (e.g., weight) does not contribute that much additional information. This is why we emphasized that the interpretations and significance of the coefficients in MLR are dependent on the other predictors in the model. 

Lastly, we can add categorical predictors to the model that have more than two categories. Essentially, a categorical variable with c categories will be converted to c-1 binary variables. For example, suppose there were three study groups: two interventions (A and B) and a control group (C). If this predictor was added to a model, it would typically be recoded into two binary (or ‘dummy’) variables: one comparing A (1) to C (0) and another one comparing B (1) to C (0). This topic will be covered in the second course in this sequence. 

And thus concludes the learning material for PubH 6450: Biostatistics I. Enjoy answering the hard questions that permeate our complex world through the lens of biostatistics/data analysis! 